<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>站在巨人的肩膀上</title>
  
  <subtitle>技术文章精选</subtitle>
  <link href="http://posts.hufeifei.cn/atom.xml" rel="self"/>
  
  <link href="http://posts.hufeifei.cn/"/>
  <updated>2024-08-14T08:34:47.985Z</updated>
  <id>http://posts.hufeifei.cn/</id>
  
  <author>
    <name>Holmofy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OLAP入门问答-进阶篇</title>
    <link href="http://posts.hufeifei.cn/db/olap/"/>
    <id>http://posts.hufeifei.cn/db/olap/</id>
    <published>2023-01-23T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="续言"><a href="#续言" class="headerlink" title="续言"></a>续言</h3><p>前一篇文章从OLTP出发，通过对比引出OLAP，进一步介绍了数仓的基本概念，包括多维数据模型、数据立方体及其典型操作等。本篇再进一步，将介绍OLAP的类型及其代表产品，并分析主流开源OLAP产品的核心技术点。</p><p>未看过前一篇文章的读者，欢迎点击链接（<a href="https://zhuanlan.zhihu.com/p/144926830">温正湖：OLAP数仓入门问答-基础篇</a>）做进一步了解。</p><h3 id="有哪些类型的OLAP数仓？"><a href="#有哪些类型的OLAP数仓？" class="headerlink" title="有哪些类型的OLAP数仓？"></a>有哪些类型的OLAP数仓？</h3><h3 id="按数据量划分"><a href="#按数据量划分" class="headerlink" title="按数据量划分"></a>按数据量划分</h3><p>对一件事物或一个东西基于不同角度，可以进行多种分类方式。对数仓产品也一样。比如我们可以基于数据量来选择不同类型的数量，如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-cde95a6639efa514bea331ac821bf387_1440w.webp"></p><p>本系列文章主要关注的是数据量处于百万到百亿级别的偏实时的分析型数仓，Cloudera的Impala、Facebook的Presto和Pivotal的GreenPlum均属于这类系统；如果超过百亿级别数据量，那么一般选择离线数仓，如使用Hive或Spark等（SparkSQL3.0看起来性能提升很明显）；对于数据量很小的情况，虽然是分析类应用，也可以直接选择普通的关系型数据库，比如MySQL等，“杀鸡焉用牛刀”。</p><p>这些系统均属于网易杭研大数据和数据库团队的研究范畴，对各系统均有深入研究和优化，对外提供网易猛犸、网易有数和网易云RDS等服务。</p><h3 id="按建模类型划分"><a href="#按建模类型划分" class="headerlink" title="按建模类型划分"></a>按建模类型划分</h3><p>下面我们主要关注数据量中等的分析型数仓，聚焦OLAP系统。 根据维基百科对<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Online_analytical_processing%23Types">OLAP</a>的介绍，一般来说OLAP根据建模方式可分为MOLAP、ROLAP和HOLAP 3种类型，下面分别进行介绍并分析优缺点。</p><h3 id="MOLAP"><a href="#MOLAP" class="headerlink" title="MOLAP"></a>MOLAP</h3><p>这应该算是最传统的数仓了，1993年Edgar F. Codd提出OLAP概念时，指的就是MOLAP数仓，M即表示多维（Multidimensional）。大多数MOLAP产品均对原始数据进行预计算得到用户可能需要的所有结果，将其存储到优化过的多维数组存储中，可以认为这就是上一篇所提到的“数据立方体”。</p><p>由于所有可能结果均已计算出来并持久化存储，查询时无需进行复杂计算，且以数组形式可以进行高效的免索引数据访问，因此用户发起的查询均能够稳定地快速响应。这些结果集是高度结构化的，可以进行压缩/编码来减少存储占用空间。</p><p>但高性能并不是没有代价的。首先，MOLAP需要进行预计算，这会花去很多时间。如果每次写入增量数据后均要进行全量预计算，显然是低效率的，因此支持仅对增量数据进行迭代计算非常重要。其次，如果业务发生需求变更，需要进行预定模型之外新的查询操作，现有的MOLAP实例就无能为力了，只能重新进行建模和预计算。</p><p>因此，<strong>MOLAP适合业务需求比较固定，数据量较大的场景</strong>。在开源软件中，由eBay开发并贡献给Apache基金会的Kylin即属于这类OLAP引擎，支持在百亿规模的数据集上进行亚秒级查询。</p><p><img src="https://pic1.zhimg.com/80/v2-dcdbc7bafdd6f75bb32e7ebc7d06c7e4_1440w.webp"></p><p>其架构图较直观得反映了基于cube的预计算模型（build），如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-48153557a5055d0e6ec3161d1fa2a99e_1440w.webp"></p><h3 id="ROLAP"><a href="#ROLAP" class="headerlink" title="ROLAP"></a>ROLAP</h3><p>与MOLAP相反，ROLAP无需预计算，直接在构成多维数据模型的事实表和维度表上进行计算。R即表示关系型（Relational）。显然，这种方式相比MOLAP更具可扩展性，增量数据导入后，无需进行重新计算，用户有新的查询需求时只需写好正确的SQL语句既能完成获取所需的结果。</p><p>但ROLAP的不足也很明显，尤其是在数据体量巨大的场景下，用户提交SQL后，获取查询结果所需的时间无法准确预知，可能秒回，也可能需要花费数十分钟甚至数小时。本质上，ROLAP是把MOLAP预计算所需的时间分摊到了用户的每次查询上，肯定会影响用户的查询体验。</p><p>当然ROLAP的性能是否能够接受，取决于用户查询的SQL类型，数据规模以及用户对性能的预期。对于相对简单的SQL，比如TPCH中的Query响应时间较快。但如果是复杂SQL，比如TPC-DS中的数据分析和挖掘类的Query，可能需要数分钟。</p><p>相比MOLAP，ROLAP的使用门槛更低，在完成星型或雪花型模型的构建，创建对应schema的事实表和维度表并导入数据后，用户只需会写出符合需求的SQL，就可以得到想要的结果。相比创建“数据立方体”，显然更加方便。</p><p>有分析表明，虽然<strong>ROLAP的性能比如MOLAP，但由于其灵活性、扩展性，ROLAP的使用者是MOLAP的数倍</strong>。</p><blockquote><p>The survey shows that ROLAP tools have 7 times more users than MOLAP tools within each company  </p></blockquote><h3 id="HOLAP"><a href="#HOLAP" class="headerlink" title="HOLAP"></a>HOLAP</h3><p>MOLAP和ROLAP各有优缺点，而且是互斥的。如果能够将两者的优点进行互补，那么是个更好的选择。而HOLAP的出现就是这个目的，H表示混合型（Hybrid），这个想法很朴素直接。对于查询频繁而稳定但又耗时的那些SQL，通过预计算来提速；对于较快的查询、发生次数较少或新的查询需求，像ROLAP一样直接通过SQL操作事实表和维度表。</p><p>目前似乎没有开源的OLAP系统属于这个类型，一些大数据服务公司或互联网厂商，比如HULU有类似的产品。相信未来HOLAP可能会得到进一步发展，并获得更大规模的使用。</p><h3 id="HTAP"><a href="#HTAP" class="headerlink" title="HTAP"></a>HTAP</h3><p>从另一个维度看，HTAP也算是一种OLAP类型的系统，是ROLAP的一个扩展，具备了OLAP的能力。最新发展显示，有云厂商在HTAP的基础上做了某种妥协，<a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/oIlntVO9ZXimqphlO5ERrA">将T（transaction）弱化为S（Serving），朝HSAP方向演进</a>。关于HTAP/HSAP，本文不做进一步展开，可自主查询其他资料。</p><p>主流的OLAP数仓系统很多，包含上面所述的各种类型，下图是Gartner 2019 年发布的数据分析市场排名（<a href="https://link.zhihu.com/?target=https://www.infoq.cn/article/8kwSM0CaUVYW1EzeO7GT">数据来源</a>）</p><p><img src="https://pic2.zhimg.com/80/v2-ef0e0b493d7f8c7a6ec232d4ec6fa50d_1440w.webp"></p><p>可以发现，传统的商业厂商和闭源的云服务厂商占据了绝大部分市场。大部分系统笔者只听过而没有研究过。作为屁股在互联网公司的数据库/数据仓库开发者，本文后续主要聚焦在基于Hadoop生态发展的开源OLAP系统（SQL on Hadoop）。</p><h3 id="有哪些常用的开源ROLAP产品？"><a href="#有哪些常用的开源ROLAP产品？" class="headerlink" title="有哪些常用的开源ROLAP产品？"></a>有哪些常用的开源ROLAP产品？</h3><p>目前生产环境使用较多的开源ROLAP主要可以分为2大类，一个是宽表模型，另一个是多表组合模型（就是前述的星型或雪花型）。</p><p><img src="https://pic1.zhimg.com/80/v2-36a8ca216e06f1012d530ae0e190d740_1440w.webp"></p><h3 id="宽表模型"><a href="#宽表模型" class="headerlink" title="宽表模型"></a>宽表模型</h3><p>宽表模型能够提供比多表组合模型更好的查询性能，不足的是支持的SQL操作类型比较有限，比如对Join等复杂操作支持较弱或不支持。</p><p>目前该类OLAP系统包括<strong>Druid和ClickHouse</strong>等，两者各有优势，Druid支持更大的数据规模，具备一定的预聚合能力，通过倒排索引和位图索引进一步优化查询性能，在广告分析场景、监控报警等时序类应用均有广泛使用；ClickHouse部署架构简单，易用，保存明细数据，依托其向量化查询、减枝等优化能力，具备强劲的查询性能。两者均具备较高的数据实时性，在互联网企业均有广泛使用。</p><p>除了上面介绍的Druid和ClickHouse外，<strong>ElasticSearch和Solar</strong>也可以归为宽表模型。但其系统设计架构有较大不同，这两个一般称为搜索引擎，通过倒排索引，应用Scatter-Gather计算模型提高查询性能。对于搜索类的查询效果较好，但当数据量较大或进行扫描聚合类查询时，查询性能会有较大影响。</p><h3 id="多表组合模型"><a href="#多表组合模型" class="headerlink" title="多表组合模型"></a>多表组合模型</h3><p>采用星型或雪花型建模是最通用的一种ROLAP系统，常见的包括GreenPlum、Presto和Impala等，他们均基于MPP架构，采用该模型和架构的系统具有支持的数据量大、扩展性较好、灵活易用和支持的SQL类型多样等优点。</p><p>相比其他类型ROLAP和MOLAP，该类系统性能不具有优势，实时性较一般。通用系统往往比专用系统更难实现和进行优化，这是因为通用系统需要考虑的场景更多，支持的查询类型更丰富。而专用系统只需要针对所服务的某个特定场景进行优化即可，相对复杂度会有所降低。</p><p>对于ROLAP系统，尤其是星型或雪花型的系统，如果能够尽可能得缩短响应时间非常重要，这将是该系统的核心竞争力。这块内容，我们放在下一节着重进行介绍。</p><h3 id="有哪些黑科技用于优化ROLAP系统性能？"><a href="#有哪些黑科技用于优化ROLAP系统性能？" class="headerlink" title="有哪些黑科技用于优化ROLAP系统性能？"></a>有哪些黑科技用于优化ROLAP系统性能？</h3><p>目前生产环境使用的ROLAP系统，均实现了大部分的该领域性能优化技术，包括采用MPP架构、支持基于代价的查询优化（CBO）、向量化执行引擎、动态代码生成机制、存储空间和访问效率优化、其他cpu和内存相关的计算层优化等。下面逐一进行介绍。</p><h3 id="什么是MPP架构？"><a href="#什么是MPP架构？" class="headerlink" title="什么是MPP架构？"></a>什么是MPP架构？</h3><p>首先来聊聊系统架构，这是设计OLAP系统的第一次分野，目前生产环境中系统采用的架构包括基于传统的MapReduce架构加上SQL层组装的系统；主流的基于MPP的系统；其他非MPP系统等。</p><h3 id="MR架构及其局限"><a href="#MR架构及其局限" class="headerlink" title="MR架构及其局限"></a>MR架构及其局限</h3><p>在Hadoop生态下，最早在Hive上提供了基于MapReduce框架的SQL查询服务。</p><p><img src="https://pic4.zhimg.com/80/v2-63552f91763fc0ccbb77ffd81c655fdf_1440w.webp"></p><p>但基于MR框架局限性明显，比如：</p><ul><li><p>每个MapReduce 操作都是相互独立的，Hadoop不知道接下来会有哪些MapReduce。</p></li><li><p>每一步的输出结果，都会持久化到硬盘或者HDFS 上。</p></li></ul><p>第一个问题导致无法进行跨MR操作间的优化，第二个问题导致MR间数据交互需要大量的IO操作。两个问题均对执行效率产生很大影响，性能较差。</p><h3 id="MPP优缺点分析"><a href="#MPP优缺点分析" class="headerlink" title="MPP优缺点分析"></a>MPP优缺点分析</h3><p>MPP是massively parallel processing的简称，即大规模并行计算框架。相比MR等架构，MPP查询速度快，通常在秒计甚至毫秒级以内就可以返回查询结果，这也是为何很多强调低延迟的系统，比如OLAP系统大多采用MPP架构的原因。</p><p>下面以Impala为例，简单介绍下MPP系统架构。</p><p><img src="https://pic2.zhimg.com/80/v2-3ee5e0d130f3b00031fd46741f4d6dd5_1440w.webp"></p><p>上图即为Impala架构图，展示了Impala各个组件及一个查询的执行流程。</p><ol><li>用户通过Impala提供的impala-shell或beeline等客户端/UI工具向Impala节点下发查询SQL；接收该SQL的Impala节点即为Coordinator节点，该节点负责进行SQL解析；</li><li>首先产生基于单节点的执行计划；再对执行计划进行分布式处理，比如将Join、聚合（aggregation）等并行化到各Impala Executor节点上。执行计划被切分为多个Plan Fragment（PF），每个PF又由一到多个Operator组成；</li><li>接着，下发经过优化后的执行计划的PF到对应的Executor节点，多个执行节点并行处理任务，缩短整个任务所需时间；</li><li>执行节点扫描HDFS/Hbase等存储上的数据，并逐层进行处理，比如进行跨节点的数据shuffe，Join等操作；</li><li>执行节点完成任务并将输出结果统一发送到Coordinator节点；</li><li>Coordinator节点汇总各个执行节点数据，做最后处理，最终返回给用户想要的结果集。</li></ol><p>MPP架构之所以性能比MR好，原因包括：</p><ul><li><p>PF之间的数据交互（即中间处理结果）驻留在内存Buffer中不落盘（假设内存够大）；</p></li><li><p>Operator和PF间基于流水线处理，不需要等上一个Operator/PF都完成后才进行下一个处理。上下游之间的关系和数据交互式预先明确的。</p></li></ul><p>这样可以充分利用CPU资源，减少IO资源消耗。但事情往往是两面的，MPP并不完美，主要问题包括：</p><ul><li><p>中间结果不落盘，在正常情况下是利好，但在异常情况下就是利空，这意味着出现节点宕机等场景下，需要重新计算产生中间结果，拖慢任务完成时间；</p></li><li><p>扩展性没有MR等架构好，或者说随着MPP系统节点增多到一定规模，性能无法线性提升。有个原因是“木桶效应”，系统性能瓶颈取决于性能最差的那个节点。另一个原因是规模越大，出现节点宕机、坏盘等异常情况就会越频繁，故障率提高会导致SQL重试概率提升；</p></li></ul><p>基于上述分析，MPP比较适合执行时间不会太久的业务场景，比如数小时。因为时间越久，故障概率越大。</p><h3 id="其他非MPP架构"><a href="#其他非MPP架构" class="headerlink" title="其他非MPP架构"></a>其他非MPP架构</h3><p>基于MR系统局限性考虑，除了采用MPP架构外，Hive和Spark均使用不同方式进行了优化，包括Hive的Tez，SparkSQL基于DAG（Directed Acyclic Graph）等。</p><p>不同架构有不同优缺点，重要的是找到其适用的场景，并进行靠谱地优化，充分发挥其优势。</p><h3 id="什么是基于代价的查询优化？"><a href="#什么是基于代价的查询优化？" class="headerlink" title="什么是基于代价的查询优化？"></a>什么是基于代价的查询优化？</h3><p>有了适合的系统架构并不一定能够带来正向收益，“好马配好鞍”，执行计划的好坏对最终系统的性能也有着决定性作用。执行计划及其优化，就笔者的理解来说，其来源于关系型数据库领域。这又是一门大学问，这里仅简单介绍。</p><p>分布式架构使得执行计划能够进行跨节点的并行优化，通过任务粒度拆分、串行变并行等方式大大缩短执行时间。除此之外，还有2个更重要的优化方式，就是传统的基于规则优化以及更高级的基于代价优化。</p><h3 id="基于规则优化"><a href="#基于规则优化" class="headerlink" title="基于规则优化"></a>基于规则优化</h3><p>通俗来说，基于规则的优化（rule based optimization，RBO）指的是不需要额外的信息，通过用户下发的SQL语句进行的优化，主要通过改下SQL，比如SQL子句的前后执行顺序等。比较常见的优化包括谓语下推、字段过滤下推、常量折叠、索引选择、Join优化等等。</p><p><strong>谓语下推，即PredicatePushDown</strong>，最常见的就是where条件等，举MySQL为例，MySQL Server层在获取InnoDB表数据时，将Where条件下推到InnoDB存储引擎，InnoDB过滤where条件，仅返回符合条件的数据。在有数据分区场景下，谓语下推更有效；</p><p><strong>字段过滤下推，即ProjectionPushDown</strong>，比如某个SQL仅需返回表记录中某个列的值，那么在列存模式下，只需读取对应列的数据，在行存模式下，可以选择某个索引进行索引覆盖查询，这也是索引选择优化的一种场景；</p><p><strong>常量或函数折叠</strong>也是一种常见的优化方式，将SQL语句中的某些常量计算（加减乘除、取整等）在执行计划优化阶段就做掉；</p><p><strong>Join优化</strong>有很多方法，这里说的基于规则优化，主要指的是Join的实现方式，比如最傻瓜式的Join实现就是老老实实得读取参与Join的2张表的每条记录进行Join条件比对。而最普遍的优化方式就是Hash Join，显然效率很高。不要认为这是想当然应该有的功能，其实MySQL直到8.0版本才具备。另外Join的顺序及合并，有部分也可以直接通过SQL来进行判断和选择。</p><h3 id="基于代价优化"><a href="#基于代价优化" class="headerlink" title="基于代价优化"></a>基于代价优化</h3><p>基于规则的优化器简单，易于实现，通过内置的一组规则来决定如何执行查询计划。与之相对的是基于代价优化（cost based optimization，CBO）。</p><p>CBO的实现依赖于详细可靠的统计信息，比如每个列的最大值、最小值、平均值、区分度、记录数、列总和，表大小分区信息，以及列的直方图等元数据信息。</p><p>CBO的一大用途是在Join场景，决定Join的执行方式和Join的顺序。这里所说的Join我们主要是讨论Hash Join。</p><p><img src="https://pic3.zhimg.com/80/v2-33872b6a316c3a77a8437a9daf09ba6a_1440w.webp"></p><p><strong>Join执行方式</strong></p><p>根据参与Join的<strong>驱动表（Build Table）和被驱动表（Probe Table）</strong>的大小，Hash Join一般可分为broadcast和partition两种。</p><p><img src="https://pic4.zhimg.com/80/v2-a686847a853612d438b66c4e26effc53_1440w.webp"></p><p><strong>广播方式</strong>适用于大表与小表进行Join，在并行Join时，将小表广播到大表分区数据所在的各个执行节点，分别与大表分区数据进行Join，最后返回Join结果并汇总。</p><p><img src="https://pic3.zhimg.com/80/v2-08f39b8a4725501c48c28428bf7e7eea_1440w.webp"></p><p>而<strong>分区方式</strong>是最为一般的模式，适用于大表间Join或表大小未知场景。分别将两表进行分区，每个分区分别进行Join。</p><p><img src="https://pic1.zhimg.com/80/v2-f2e4fe9d83f1d9376d9ab008463901ec_1440w.webp"></p><p>显然，判断大小表的关键就看是否能够通过某种方式获取表的记录数，如果存储层保存了记录数，那么可从元数据中直接获取。</p><p>如果Join的两表都是大表，但至少有个表是带Where过滤条件的，那么在决定走分区方式前还可进一步看满足条件的记录数，这时候，物理上进行分区的表存储方式可发挥作用，可以看每个分区的最大值和最小值及其记录数来估算过滤后的总记录数。当然，还有种更精确的方式是列直方图，能够直接而直观得获取总记录数。</p><p>如果上述的统计信息都没有，要使用CBO还有另一种方式就是进行记录的动态采样来决定走那种Join方式。</p><p><strong>Join顺序</strong></p><p>如果一个查询的SQL中存在多层Join操作，如何决定Join的顺序对性能有很大影响。这块也已是被数据库大佬们充分研究过的技术。</p><p><img src="https://pic2.zhimg.com/80/v2-659e39bea5507b6de68612defb2b0489_1440w.webp"></p><p>一个好的CBO应该能够根据SQL 语句的特点，来自动选择使用<strong>Left-deep tree</strong>（LDT，左图）还是 <strong>bushy tree</strong>（BYT，右图）执行join。</p><p>两种Join顺序没有好坏之分，关键看进行Join的表数据即Join的字段特点。</p><p>对于LDT，如果每次Join均能够过滤掉大量数据，那么从资源消耗来看，显然是更优的。对于给每个列都构建了索引的某些系统，使用LDT相比BYT更好。</p><p>一般来说，选择BYT是效率更高的模式，通过串行多层Join改为并行的更少层次Join，可以发挥MPP架构的优势，尽快得到结果，在多表模式ROLAP场景常采用。</p><h3 id="为什么需要向量化执行引擎？其与动态代码生成有何关系？"><a href="#为什么需要向量化执行引擎？其与动态代码生成有何关系？" class="headerlink" title="为什么需要向量化执行引擎？其与动态代码生成有何关系？"></a>为什么需要向量化执行引擎？其与动态代码生成有何关系？</h3><p><strong>查询执行引擎 (query execution engine)</strong> 是数据库中的一个核心组件，用于将查询计划转换为物理计划，并对其求值返回结果。查询执行引擎对系统性能影响很大，在一项针对Impala和Hive的对比时发现，Hive在某些简单查询上（TPC-H Query 1）也比Impala慢主要是因为Hive运行时完全处于CPU bound的状态中，磁盘IO只有20%，而Impala的IO至少在85%。</p><p>什么原因导致这么大的差别呢？首先得简单说下火山模型的执行引擎。</p><h3 id="火山模型及其缺点"><a href="#火山模型及其缺点" class="headerlink" title="火山模型及其缺点"></a>火山模型及其缺点</h3><p><strong>火山模型（Volcano-style execution）是最早的查询执行引擎</strong>，也叫做迭代模型 (iterator model)，或 one-tuple-at-a-time。在这种模型中，查询计划是一个由operator组成的DAG，其中每一个operator 包含三个函数：open，next，close。Open 用于申请资源，比如分配内存，打开文件，close 用于释放资源，next方法递归的调用子operator的 next方法生成一个元组（tuple，即行row在物理上的表示）。</p><p>下图描述了“select sum(C1) from T1 where C2 &gt; 15”的查询计划，该查询计划包含Project，HashAgg，Scan等operator，每个 operator的next方法递归调用子节点的 next，一直递归调用到叶子节点Scan operator，Scan operator的next 从文件中返回一个元组。</p><p><img src="https://pic1.zhimg.com/80/v2-6f96d3408831f1dd52fd9ca3e477e490_1440w.webp"></p><p>其缺点主要在于：</p><p>- <strong>大量虚函数调用</strong>：火山模型的next方法通常实现为一个虚函数，在编译器中，虚函数调用需要查找虚函数表, 并且虚函数调用是一个非直接跳转 (indirect jump), 会导致一次错误的CPU分支预测 (brance misprediction), 一次错误的分支预测需要十几个周期的开销。火山模型为了返回一个元组，需要调用多次next 方法，导致昂贵的函数调用开销</p><p>- <strong>类型装箱</strong>：对于a + 2 * b之类表达式，由于需要对不同数据类型的变量做解释，所以在Java中需要把这些本来是primitive（如int等类型）的变量包装成Object，但执行时又需要调用具体类型的实现函数，这本质上也是虚函数调用的效率问题；</p><p>- <strong>CPU Cache利用效率低</strong>：next方法一次只返回一个元组，元组通常采用行存储，如果仅需访问第一列而每次均将一整行填入CPU Cache，将导致Cache Miss；</p><p>- <strong>条件分支预测失败</strong>：现在的CPU都是有并行流水线的，但是如果出现条件判断会导致无法并行。比如判断数据的类型（是string还是int），或判断某一列是否因为其他字段的过滤条件导致本行不需要被读取等场景；</p><p>- <strong>CPU与IO性能不匹配</strong>：每次从磁盘读取一个行数据，经过多次调用交给CPU进行处理，显然，大部分时间都是CPU等待数据就绪，导致CPU空转。</p><p>通过上述描述，可以得出解决问题的基本方法。可以将问题分为2大类，分别用下述的向量化引擎和动态代码生成技术来解决。</p><h3 id="向量化执行引擎"><a href="#向量化执行引擎" class="headerlink" title="向量化执行引擎"></a>向量化执行引擎</h3><p>向量化执行以列存为前提，主要思想是每次从磁盘上读取一批列，这些列以数组形式组织。每次next都通过for循环处理列数组。这么做可以大幅减少next的调用次数。相应的CPU的利用率得到了提高，另外数据被组织在一起。可以进一步利用CPU硬件的特性，如SIMD，将所有数据加载到CPU的缓存当中去，提高缓存命中率，提升效率。在列存储与向量化执行引擎的双重优化下，查询执行的速度会有一个非常巨大的飞跃。</p><h3 id="动态代码生成"><a href="#动态代码生成" class="headerlink" title="动态代码生成"></a>动态代码生成</h3><p>向量化执行减少CPU等待时间，提高CPU Cache命中率，通过减少next调用次数来缓解虚函数调用效率问题。而动态代码生成，则是进一步解决了虚函数调用问题。</p><p>动态代码生成技术不使用解释性的统一代码，而是直接生成对应的执行语言的代码并直接用primitive type。对于判断数据类型造成的分支判断，动态代码的效果可以消除这些类型判断，使用硬件指令来进一步提高循环处理效率。</p><p>具体实现来说，JVM系如Spark SQL，Presto可以用反射，C++系的Impala则使用了llvm生成中间码。相对来说，C++的效率更高。</p><p>向量化和动态代码生成技术往往是一起工作达到更好的效果。</p><h3 id="都有哪些存储空间和访问效率优化方法？"><a href="#都有哪些存储空间和访问效率优化方法？" class="headerlink" title="都有哪些存储空间和访问效率优化方法？"></a>都有哪些存储空间和访问效率优化方法？</h3><p>存储和IO模块的优化方法很多，这里我们还是在Hadoop生态下来考虑，当然，很多优化方法不是Hadoop特有的，而是通用的。OLAP场景下，数据存储最基础而有效的优化是该行存储为列存储，下面讨论的优化措施均基于列存。</p><h3 id="数据压缩和编码"><a href="#数据压缩和编码" class="headerlink" title="数据压缩和编码"></a>数据压缩和编码</h3><p>数据压缩是存储领域常用的优化手段，以可控的CPU开销来大幅缩小数据在磁盘上的存储空间，一来可以节省成本，二来可以减小IO和数据在内存中跨线程和跨节点网络传输的开销。目前在用的主流压缩算法包括zlib、snappy和lz4等。压缩算法并不是压缩比越高越好，压缩率越高的算法压缩和解压缩速度往往就越慢，需要根据硬件配置和使用场景在cpu 和io之间进行权衡。</p><p>数据编码可以理解为轻量级压缩，包括<strong>RLE和数据字典编码</strong>等。</p><p><img src="https://pic1.zhimg.com/80/v2-bc965a8e4b0e1a336dd5cae28ea2b3ac_1440w.webp"></p><p>上图截至Presto论文，展示了RLE编码和数据字典编码的使用方式。RLE用在各列都是重复字符的情况，比如page0中6行记录的returnflag都是”F”。数据字典可高效使用在区分度较低的列上，比如列中只有几种字符串的场景。考虑到同个表的列的值相关性，数据字典可以跨page使用。</p><p>与数据压缩相比，数据编码方式在某些聚合类查询场景下，无需对数据进行解码，直接返回所需结果。比如假设T1表的C1列为某个字符，RLE算法将16个C1列的值“aaaaaabbccccaaaa”编码为6a2b4c4a，其中6a表示有连续6个字符a。当执行 select count(*) from T1 where C1=’a’时，不需要解压6a2b4c4a，就能够知道这16行记录对应列值为a有10行。</p><p>在列存模式下，数据压缩和编码的效率均远高于行存。</p><h3 id="数据精细化存储"><a href="#数据精细化存储" class="headerlink" title="数据精细化存储"></a>数据精细化存储</h3><p>所谓数据精细化存储，是通过尽可能多得提供元数据信息来减少不必要的数据扫描和计算，常用的方法包括但不限于如下几种：</p><p>- <strong>数据分区</strong>：数据分区可用于将表中数据基于hash或range打散到多个存储节点上，配合多副本存储。可以提高数据容灾和迁移效率。除此之外，在查询时可以快速过滤掉不符合where条件要求的数据分区，无需逐列读取数据进行判断。</p><p>- <strong>行组</strong>：与数据分区类似，Hadoop中常用的parquet和orcfile还将表数据分为多个行组（row group），每个行组内的记录按列存储。这样即达到列存提高OLAP查询效率，同时能够兼顾查询多行的需求；</p><p>- <strong>局部索引</strong>：在数据分区或行组上创建索引，可以提高查询效率。如下图所示，orcfile在每个行组的头部维护了Index Data来，保存最大值和最小值等元数据，基于这些信息可以快速决定是否需扫描该行组。某些OLAP系统进一步丰富了元数据信息，比如建立该行组记录的倒排索引或B+树索引，进一步提高扫描和查询效率。</p><p><img src="https://pic1.zhimg.com/80/v2-cfa177b7fedb7679534a0374cea9ee70_1440w.webp"></p><p>- <strong>富元数据</strong>：除了提供最大值和最小值信息外，还可进一步提供平均值、区分度、记录数、列总和，表大小分区信息，以及列的直方图等元数据信息。</p><h3 id="数据本地化访问"><a href="#数据本地化访问" class="headerlink" title="数据本地化访问"></a>数据本地化访问</h3><p>数据本地化读写是常见的优化方法，在Hadoop下也提供了相应的方式。</p><p>一般来说，读HDFS上的数据首先需要经过NameNode获取数据存放的DataNode信息，在去DataNode节点读取所需数据。</p><p>对于Impala等OLAP系统，可以通过HDFS本地访问模式进行优化，直接读取磁盘上的HDFS文件数据。HDFS这个特性称为”Short Circuit Local Reads”，其相关的配置项（在hdfs-site.xml中）如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>其中：dfs.client.read.shortcircuit是打开这个功能的开关，dfs.domain.socket.path是Datanode和DFSClient之间沟通的Socket的本地路径。</p><h3 id="运行时数据过滤"><a href="#运行时数据过滤" class="headerlink" title="运行时数据过滤"></a>运行时数据过滤</h3><p>这是少部分OLAP系统才具有的高级功能，比如<strong>Impala的RunTime Filter（RF）运行时过滤</strong>，和<strong>SparkSQL 3.0的 Dynamic Partition Pruning动态分区裁剪</strong>，可以将驱动表的bloomfilter（BF）或过滤条件作用在被驱动表的数据扫描阶段，从而极大减少需扫描/返回的数据量。下面分别用一个图进行简述，在后续分析具体OLAP系统时再详述。</p><p><img src="https://pic3.zhimg.com/80/v2-67eb935c66bb3aecd43e5139a4401d0a_1440w.webp"></p><p>上图直观得展示了Impala runtime filter的实现。流程如下：</p><ol><li><p>同时下发两个表的SCAN操作。左边是大表，右边是小表（相对而言，也有可能是同等级别的），但是左表会等待一段时间（默认是1s），因此右表的SCAN会先执行；</p></li><li><p>右表的扫描的结果根据join键哈希传递扫不同的Join节点，由Join节点执行哈希表的构建和RF的构建；</p></li><li><p>Join节点读取完全部的右表输入之后也完成了RF的构建，它会将RF交给Coordinator节点（如果是Broadcast Join则会直接交给左表的Scan节点）；</p></li><li><p>Coordinator节点将不同的RF进行merge，也就是把Bloom Filter进行merge，merge之后的Bloom Filter就是一个GLOBAL RF，它将这个RF分发给每一个左表Scan；</p></li><li><p>左表会等待一段时间（默认1s）再开启数据扫描，为了是尽可能的等待RF的到达，但是无论RF什么时候到达，RF都会在到达那一刻之后被应用；</p></li><li><p>左表使用RF完成扫描之后同样以Hash方式交给Join节点，由Join节点进行apply操作，以完成整个Join过程。</p></li></ol><p><img src="https://pic1.zhimg.com/80/v2-aa6e3230f2d40ae7cd2e6ebdec08887c_1440w.webp"></p><p>sparksql图1（官方这个图有误，右边应该是Scan Date）</p><p><img src="https://pic4.zhimg.com/80/v2-7a8de9e0e91df3f0c8d45e03c10d200f_1440w.webp"></p><p>sparksql图2</p><p>上面2幅图是SparkSQL 3.0的动态分区裁剪示意图。将右表的扫描结果（hashtable of table Date after filter）广播给左表的Join节点，在进行左表扫描时即使用右表的hashtable进行条件数据过滤。</p><h3 id="除了上面这些，还有其他优化方法吗？"><a href="#除了上面这些，还有其他优化方法吗？" class="headerlink" title="除了上面这些，还有其他优化方法吗？"></a>除了上面这些，还有其他优化方法吗？</h3><p>还有个极为重要的技术是集群资源管理和调度。Hadoop使用YARN进行资源调度，虽然带来了很大遍历，但对性能要求较高的OLAP系统却有些不适合。</p><p>如启动AppMaster和申请container会占用不少时间，尤其是前者，而且container的供应如果时断时续，会极大的影响时效性。</p><p>目前的优化方法主要包括让AppMaster启动后长期驻守，container复用等方式。让资源在需要用时已经就位，查询无需等待即可马上开始。</p><h3 id="做个总结"><a href="#做个总结" class="headerlink" title="做个总结"></a>做个总结</h3><p>本系列通过2篇文章，总结了下笔者最近看的一些OLAP相关文献材料。笔者通过这两篇文章主要是想说下自己对数仓和OLAP系统的理解，之所以采用问答形式，是因为笔者就是带着这些问题去google网上或公司内部的资料，或者直接请教在这个领域的大佬。</p><p>由于水平有限，难免有所错误，非常欢迎大家看后能够指出，让笔者有进步的机会。这两篇文章可以理解为是对他人文章的一次汇总加工。部分内容直接参考了其他文章，这也是在笔者先前其他文章中极少出现的情况，这些内容均在文末“引用”小结列出。</p><p><a href="https://www.infoq.cn/article/NTwo*yR2ujwLMP8WCXOE">最快开源OLAP引擎！ClickHouse在头条的技术演进_开源_陈星_InfoQ精选文章</a></p><p><a href="https://snappydata-cn.github.io/2018/04/04/SnappyData%E4%B8%8EPresto-Druid-Kylin-ES%E7%9A%84%E5%AF%B9%E6%AF%94-2/">SnappyData与Presto,Druid,Kylin,ES的对比-2 | SnappyData中文博客 (snappydata-cn.github.io)</a></p><p><a href="http://blog.daas.ai/2018/11/26/ClickHouse_vs._Druid_Pinot/">ClickHouse vs. Druid/Pinot | Xavier’s Lab (daas.ai)</a></p><p><a href="https://waltyou.github.io/Spark-DAG/">Spark 中 DAG 介绍 (waltyou.github.io)</a></p><p><a href="http://hbasefly.com/">有态度的HBase/Spark/BigData (hbasefly.com)</a></p><p><a href="https://www.infoq.cn/article/an-article-mastering-sql-on-hadoop-core-technology">一篇文章掌握Sql-On-Hadoop核心技术_语言 &amp; 开发_王宝生_InfoQ精选文章</a></p><p><a href="https://mp.weixin.qq.com/s/4O07cECjLbUQ4H-5K8f3gQ">盘点：SQL on Hadoop中用到的主要技术 (qq.com)</a></p><p><a href="https://www.infoq.cn/article/columnar-databases-and-vectorization">列式数据库和向量化_语言 &amp; 开发_Siddharth Teotia_InfoQ精选文章</a></p><p><a href="http://mysql.taobao.org/monthly/2017/01/06/">PgSQL · 引擎介绍 · 向量化执行引擎简介 (taobao.org)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h3 id=&quot;续言&quot;&gt;&lt;a href=&quot;#续言&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="数据库" scheme="http://posts.hufeifei.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="OLAP" scheme="http://posts.hufeifei.cn/tags/OLAP/"/>
    
    <category term="ROLAP" scheme="http://posts.hufeifei.cn/tags/ROLAP/"/>
    
    <category term="MOLAP" scheme="http://posts.hufeifei.cn/tags/MOLAP/"/>
    
  </entry>
  
  <entry>
    <title>2023，可观测性需求将迎来“爆发之年”？ | 解读可观测技术的 2022</title>
    <link href="http://posts.hufeifei.cn/backend/observability/"/>
    <id>http://posts.hufeifei.cn/backend/observability/</id>
    <published>2022-12-28T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>本文是 “<a href="https://www.infoq.cn/theme/168" title="xxx">2022 InfoQ 年度技术盘点与展望</a>” 系列文章之一，由 InfoQ 编辑部制作呈现，重点聚焦可观测技术领域在 2022 年的重要进展、动态，希望能帮助你准确把握 2022 年可观测技术领域的核心发展脉络，在行业内始终保持足够的技术敏锐度。</p><p>“InfoQ 年度技术盘点与展望”是 InfoQ 全年最重要的内容选题之一，将涵盖操作系统、数据库、AI、大数据、云原生、架构、大前端、编程语言、开源安全、数字化十大方向，后续将聚合延展成专题、迷你书、直播周、合集页面，在 InfoQ 媒体矩阵陆续放出，欢迎大家持续关注。</p></blockquote><p>随着 5G、云计算、微服务等技术的广泛应用，企业所面临的 IT 运维环境越来越复杂，需要运维的系统不仅数量多，而且网络架构复杂、基础设施多样。在信息化建设日益普及的当下，快速提升企业 IT 资产管理能力开始成为企业迫切解决的问题。</p><p>过去一年，可观测技术得到了极大关注，并被 Gartner 列为 2023 年十大战略技术趋势之一。然而，企业在实际落地可观测的过程中依旧面临很多问题，比如如何选择合适的方案、平衡新旧系统之间的关系等等。</p><p>其实，一个企业 IT 系统的可观测性建的好不好，在新员工入职时就能看出来。如果新员工去看文档时，看的是公司某个架构师画的系统架构图，显然这个公司技术系统是缺乏可观测性的。因为企业系统一直在迭代，而系统的架构图却很难及时更新。如果新员工看到的是完整的可观测性平台数据看板，那这个公司的“可观测性”构建的应该算是比较完善的。当企业的可观测性构建趋于成熟后，开发者可以快速定位系统问题，精准的“观测性数据”可以帮助开发者完成包括自动化巡检在内的各种各样的 Control 或者是金丝雀发布的控制等等。</p><p>所以，可观测技术目前发展到哪个阶段了？开发者们对可观测性平台的需求到底是怎样的？带着这些问题我们对观测云 CEO 蒋烁淼进行了专访，我们一起对可观测技术过去一年的发展做了一个盘点。</p><h2 id="一、国内外“可观测性”的认知尚存在较大差异"><a href="#一、国内外“可观测性”的认知尚存在较大差异" class="headerlink" title="一、国内外“可观测性”的认知尚存在较大差异"></a>一、国内外“可观测性”的认知尚存在较大差异</h2><p>当我们想要盘点一项技术过去一年的发展状况时，首先要看它过去一年的应用率是否有所提升。去年，领域里有相关数据表明，大部分公司都在进行可观测实践，但国外也仅有 27% 的公司实现了全栈的可观测，国内可能更少，可能 10% 的比例都不到。而今年，这个比例似乎并没有提高。</p><p>正如蒋总所说的那样，“在中国，真正地构建了完全可观测性的企业非常的少，做的好的企业少之又少。”</p><p>放眼过去，目前国内确实很缺乏好的可观测性的服务商和技术提供商，这让中国的可观测性技术发展与国外产生了不小的距离。包括字节、阿里云等大厂在内的国内大部分厂商在可观测性方面都还是处于“零散拼装”的状态，他们大多把开源的 Prometheus、Loki、OpenTelemetry Collector 组装在一起形成“可观测性套件”，然后提供一个开源的托管版本给开发者使用。很少有人敢称自己为“可观测性平台”，因为没有一个完整的可观测性的概念。</p><p>然而，过去一年里，无论是在国外还是国内，企业 CTO 们对于可观测性的关注度有了明显大幅提升。从全球视角来看，不管是 Grafana 的 ObservabilityCON，还是今年谷歌云举办的 Google Cloud Next’22 Recap、北美的 KubeCon 2022 大会，亦或者是前不久亚马逊云科技举办的 re:Invent 全球大会、NGINX 举办的 NGINX Sprint China2022，可观测性话题的探讨比例都有了明显提升，像 Google Cloud Next’22 Recap 大会谈可观测性几乎占到了大会整体内容的三分之一。</p><p>切回国内视角，当我们和国内的企业 CTO 们去聊天的时候会发现，他们今年明显会更关注可观测性这件事，无论是因为业务层面譬如 B 站 713 事件带来的启发，还是受行业大会演讲内容的影响，国内在可观测性方面没有明显的技术发展，大多是因为疫情下的市场经济不景气、企业预算不足下的“按兵不动”之举。</p><p>聚焦到具体的业务需求上，目前企业技术决策者正处于一个认知迭代的过程中——“将监控升级为可观测性”。原来的“监控”大多都是分裂的，可能是每一个项目都有一套监控工具，可能是是一个项目中的日志平台、APM 平台单独运行。所以现在不少开发者会认为可观测性就是新瓶装旧酒，他们认为把传统的监控工具整合在一起，就是可观测性了，甚至不少创业公司的 CTO 会将一些 K8S 监控工具或者 APM 混同于可观测性。</p><p>事实上，可观测性相关的工具如果被称为“下一代监控”的话，它的投资占 IT 的比重并不小，可观测性的应用场景远远要超过目前大部分开发者的认知。“可观测性”这件事在大部分 CTO 的意识中，无论是横向扩展还是垂直扩展，大家的认知都还需要有一个“求同”的过程。</p><p>就像现在许多人会把“可观测性”定义为是一个降本增效的好工具，很多相关产品厂商会将“降本增效”作为产品的推广利益点，这让许多开发者认为安装了可观测性平台，就可以降低成本、为业务创造价值。事实上，可观测性工具从来不是一个“便宜”的东西，可观测性平台除了基本的工具投入，还需要有一套完整的数据存储方案，企业自研一套优秀的可观测性解决方案投入成本并不低。以美国企业的可观测性相关投入为例，其会占企业整体 IT 支出的 5%-10%。而且，它的“降本增效”不是短时间内就可以显现出来的，而是通过长时间的应用，为业务提供大量的数据支撑进行技术性优化，从而产生价值。</p><p>总体来说，真正的可观测性平台应该是，能够将各种各样对于系统的形态、实时的状态进行有结构性的收集并提供一系列的观察、测量手段的平台。简单来说，就是用各种各样的技术，像传感器一样，能够让开发者们的开发、测试、运维过程变的更容易，能够即时了解系统的运行状态，而并不是简简单单的“监控”。</p><p>可观测性平台一定是数据组织统一化的，否则分开的日志系统、链路系统、指标系统很难实现数据与数据的关联。比如日志里的字段，主机系统中叫 host，在指标系统里叫 host name，然后有的系统用 IP 地址，有的系统用主机名，那在做关联分析和连接查询时会发现根本无从下手。有的公司也会将不同系统的数据分散开进行监控，利用“数据中台”的一个构建方案将所有数据汇聚到同一个平台中去做分析，但在实践过程中会发现，数据缺少实时性的结构化，无法做实时预警预测，只能做事后故障定位。</p><h2 id="二、全球可观测性技术演进主要聚焦在-6-个方面"><a href="#二、全球可观测性技术演进主要聚焦在-6-个方面" class="headerlink" title="二、全球可观测性技术演进主要聚焦在 6 个方面"></a>二、全球可观测性技术演进主要聚焦在 6 个方面</h2><h4 id="1、OpenTelemetry-逐渐成为行业标准"><a href="#1、OpenTelemetry-逐渐成为行业标准" class="headerlink" title="1、OpenTelemetry 逐渐成为行业标准"></a>1、OpenTelemetry 逐渐成为行业标准</h4><p>OpenTelemetry 作为一套由 CNCF 主导的云原生可观测性的标准协议，目前已经是海外企业在该领域的实践标准。从数据收集的角度来说，基于 OpenTelemetry 或者兼容 OpenTelemetry 的方案，从今年到明年都应该会逐步成为主流。OpenTelemetry 已迭代到了 1.0 版本，作为一个客户端，日志和网络等部分也都正在得到逐步补强，严格意义上讲，OpenTelemetry 1.0 已经算是完成了整体的标准化，OpenTelemetry 标准协议已称得上是一个行业标准。</p><p>反观国内，虽未形成类似的标准，却也涌现了诸如 CAT 和 SkyWalking 等一系列国产开源的 APM 系统，通过实时监控企业的应用系统，系统化地提供应用性能管理和故障定位的解决方案，在运维中被广泛使用。得益于对业务代码无侵入，性能表现优秀，社区活跃，中文文档齐全等众多优秀特性，SkyWalking 在国内异常火爆。</p><h4 id="2、Grafana-8-0-修改了-AGPL-协议"><a href="#2、Grafana-8-0-修改了-AGPL-协议" class="headerlink" title="2、Grafana 8.0 修改了 AGPL 协议"></a>2、Grafana 8.0 修改了 AGPL 协议</h4><p>开源项目背后的商业公司只有在商业上取得成功，才能更好地为开源项目提供新特性的开发、支持与维护，形成良性循环，这样开源项目的维护才会更加长久。Grafana 在云原生技术领域的监控工具中几乎是标准一般的存在，而在可观测性的投资方面，Grafana 也写下了浓墨重彩的一笔，推出了 Loki 日志存储、时序引擎 Mimir 和调用堆栈的存储引擎等一系列产品，力图通过提供一个复刻 DataDog 所有功能的开源方案，打包售卖 Grafana Cloud 服务。</p><p>因此在 Grafana 8.0 后修改了 AGPL，允许甲方有技术实力的公司基于它的开源方案做二次开发，而不允许乙方在没有商业授权的时候拿着 Grafana 的产品包装出售，此举对国内不少的信创公司都造成了巨大的冲击。</p><h4 id="3、eBPF-技术日趋成熟"><a href="#3、eBPF-技术日趋成熟" class="headerlink" title="3、eBPF 技术日趋成熟"></a>3、eBPF 技术日趋成熟</h4><p>内核通过文件系统或系统调用暴露出的信息有限，单纯从用户态切入无法抓取这些信息，只有从内核本身调用的过程中获取调用信息才可以补足相关能力。eBPF 是一项革命性的技术，它可以在 Linux 内核中运行沙盒程序，而无需更改内核源代码或加载内核模块。通过使 Linux 内核可编程，基础架构软件可以利用现有的层，使它们更加智能和功能丰富，无需继续为系统增加额外的复杂性层。</p><p>eBPF 促进了网络，安全性，应用程序配置 / 跟踪和性能故障排除等领域的新一代工具的开发，这些工具不再依赖现有的内核功能，而是在不影响执行效率或安全性的情况下主动重新运行。在对内核无侵入的前提下，它通过动态地向内核中插入一段自己的代码，实现定义监控及跟踪能力。</p><p>随着内核的发展，eBPF 逐步从最初的数据包过滤拓展到了网络、内核、安全、跟踪等领域，并且它的功能特性还在快速发展中，通过内核虚拟机，所有的 IO 包括程序运行、进程的网络通信等各种细部数据尽收眼底，大大加强了系统的可观测能力。</p><p>然而，目前大多数的只通过 eBPF 获取数据并把自己标榜为可观测平台的公司，也仅仅是做了一个 eBPF 的支持而已。从第三方视角看，eBPF 是对传统观测能力一个非常好的补充，可以通过它拿到更多的数据，用户获取内核丰富观测指标的门槛被极大降低。</p><h4 id="4、可观测性开始更“注意安全”"><a href="#4、可观测性开始更“注意安全”" class="headerlink" title="4、可观测性开始更“注意安全”"></a>4、可观测性开始更“注意安全”</h4><p>云原生数据库等新兴概念的出现，大大消除了大家对于存储成本方面的顾虑，使得企业可以更聚焦于提升观测能力本身，而不必在成本和性能方面分神，这方面的发展也有迹可循。比如今年 DataDog 推出了一个名为哈士奇的存储架构，它是一个完全云原生面向可观测性的数据库，得益于云原生的存储能力，可以在同等价格下多存储 5-10 倍数据，以往即使收集到了海量的数据，但面对高昂的存储成本，这些数据也只能被丢弃或者针对这些数据进行采样，本质上看，这对客户需要的结果也是有浪费的。但这也并不意味着使用这种新技术之后并不需要进行采样，只是新的更低成本更高性能的数据库技术，可以大大提升可观测性能够覆盖的范围，降低存储成本，这也是一个很重要的发展。</p><p>安全和可观测性的合并，已在全球范围内形成一种趋势。摩根士丹利在《安全分析和可观测性》文章中也提到，在国外，以 DataDog 为代表的公司在上市之后发布的新增功能中有 70% 都是安全相关的，这其中的道理非常简单，可观测性是通过检查其输出来衡量系统内部状态的能力，它收集了系统的方方面面，通过这些数据可以分析出系统的故障，自然也就能够分析出系统有没有被入侵。比如 DataDog 就提供了通过分析当前访问请求，区分哪些可能是黑客在嗅探，或者准备未来做 DDoS 攻击的接口的功能。</p><p>也就是说，采集的数据在安全方面也能够发挥作用，而不像传统安全工具那样，需要针对安全场景再进行一次数据采集。所以，安全和可观测性的合并在全球范围内已经成为一种趋势，当然抗 DDoS 、挖防火墙这些并不会合并，针对攻击现场的追踪，比如国内的态势感知、国外的 SIEM 这些安全产品都选择了和可观测性进行融合。</p><h4 id="5、建立“业务的可观测性”越来越重要"><a href="#5、建立“业务的可观测性”越来越重要" class="headerlink" title="5、建立“业务的可观测性”越来越重要"></a>5、建立“业务的可观测性”越来越重要</h4><p>可观测性的概念起源于工业领域，在该领域中，可观测性被定义为从系统外部输出推断系统内部健康状态的能力；在软件产品和服务领域，可观测性就是从应用系统中收集尽可能多的遥测数据，以便可以调查和解决新出现的复杂问题，确保企业能够主动观察系统，在影响客户体验之前解决故障及问题，安全地进行测试并实施优化，它可以更好地管理和控制业务风险，有助于我们了解“正在发生什么”以及“为什么会这样”。可观测性使团队能够更有效地监控现代系统，帮助他们找到并连接复杂链中的影响，并将其追溯到原因。此外，它还使系统管理员、IT 运营分析师和开发⼈员能够了解他们的整个架构。</p><p>如今的 IT 系统，迭代发布更迅速，业务系统更庞大，网络链路更复杂，运行环境更动态。在“业务至上”的互联网时代，技术工程师们保障的核心其实并不是这套 IT 系统或软件，他们保障的核心其实是业务，一笔业务可能会涉及到多个微服务系统，技术工程师们不再追踪一个 API 的全链路调用关系，而是要追踪到整个 API 关联的订单、用户甚至具体到哪一笔交易，这也是可观测性和业务结合的一个重要发展趋势。Gartner 也提到，“未来一切业务皆需可观测性”，简单地讲就是把运营人员、运维人员、IT 人员看到的数据做统一，而不是互相甩锅。</p><h4 id="6、多个“新玩家”入局"><a href="#6、多个“新玩家”入局" class="headerlink" title="6、多个“新玩家”入局"></a>6、多个“新玩家”入局</h4><p>随着微服务架构的流行，一些微服务架构下的问题也日渐突出，比如一个请求会涉及到多个服务，而服务本身可能也会依赖其他业务，整个请求路径就构成了一个网状的调用链，而在这个调用链中，一旦某个节点发生异常，整个调用链的稳定性就会受到影响。</p><p>据此，我们需要一些可以帮助理解系统行为、用于分析性能问题的工具，以便发生故障的时候，能够快速定位和解决问题。近年来，各种调用链监控产品层出不穷，呈现百花齐放的态势，各家你方唱罢我登场，比如 New Relic 收购的那家叫 PIXIE 的公司可以称得上是一个新玩家，又如最近出现了一个面向 K8S 的叫做 Kubeshark 的公司，它专门研发用于网络分析的开源组件。</p><p>此外，随着 eBPF 的出现，未来会涌现越来越多的解决方案，尤其是面向整个 K8s 生态环境，这也是全球可观测性技术的一个演进方向。</p><h4 id="三、具有“4-个统一性”的可观测性平台是企业刚需"><a href="#三、具有“4-个统一性”的可观测性平台是企业刚需" class="headerlink" title="三、具有“4 个统一性”的可观测性平台是企业刚需"></a>三、具有“4 个统一性”的可观测性平台是企业刚需</h4><p>当我们盘点完可观测性的行业趋势，所以到底什么样的平台才是目前企业所需要的呢？国内首批获得中国信通院颁发的「可观测性平台技术能力」先进级认证，新一代云原生全链路数据可观测平台——观测云给出了答案。</p><p>观测云能全环境高基数采集数据，支持多维度信息智能检索分析，及提供强大的自定义可编程能力，使系统运行状态尽在掌控，故障根因无所遁形，实现了统一采集、统一标签、统一存储和统一界面，带来全功能的一体化可观测体验。观测云平台从技术层面主要实现了 4 个统一 —— 统一采集、统一治理、统一分析和统一编程：</p><p>第一个统一，是统一的数据收集。数据采集是数据关联的基础能力，现今，国内外都有大量的数据采集器，但大多数采集能力单一，比如 Telegraf 仅支持指标，Filebeat 只服务日志，OpenTelemetry 的 Collector 对非云原生的组件并不友好，需要大量安装 Exporter 插件。</p><p>为了实现系统的可观测性，一台主机上可能需要安装无数个 Agent，这既引入了管理上的问题，又产生了成本的问题，一定程度上造成了资源的浪费。观测云的 DataKit 是目前唯一的真正一体化实现各种环境（如传统环境、云原生等）统一数据采集平台，它通过一个进程或 Daemonset Pod 就可以实现全方位的数据采集，配置体验良好，开源且拓展性强，涵盖了更为全面的数据采集类型，有海量的技术栈的指标收集能力，采集器的配置更简单，数据质量更好。</p><p>第二个统一，是数据采样格式实现统一。可观测性数据的一个重要特性就是实时性，Logstash 等传统的开源方案均在中心或边缘中心完成数据治理工作，而受限于中心处理的延迟和 delay，在日志收集方面通常有很大的时延，实时性也就无从得到保障。而在观测云的解决方案中，客户只需多拿出 5% 的 CPU 用于数据的处理，一方面可以发挥可观测数据的实时性。另一方面，还可以利用数据本身的现状还原。比如当把所有数据收集到中心的时候，实际上很多环境信息或多或少的都会存在遗漏或缺失的情况，如数据来源于哪个容器、哪个 pod、哪个部署单元或者哪个项目等，在边缘侧可以随时获取这些数据作为补充。</p><p>行内人都知道，观测云是一个 SaaS 方案，在边缘侧还可以实现采样功能。比如数据中存在的一些电话号码、金额、用户 ID 等信息，完全可以通过边缘脱敏或者删除掉一些不需要上传或者不易上传的数字。这样一来，数据在上传前就已经完成了脱敏、加密或者丢弃，在丢弃情况下，还能大幅降低传输的带宽；数据无需上传到中心再做治理，这也进一步提升了客户的信任。此外，由于减少了一个中间处理模块，系统的整体实时性也得到了很大的保障。</p><p>目前观测云也在考虑把端上的统一治理能力做成一个插件，贡献给 OpenTelemetry，使得 OpenTelemetry 的 Agent 也具备边缘计算的能力。据悉，目前观测云平台完全兼容 OpenTelemetry，单一采集器即可实现容器指标采集、应用链路追踪和日志采集。同时能和各种常用采集器实现串联，包括 Promethues、Skywalking、Fluentd 等十几种常见工具，可以替换更可以共存，轻松实现数据高密采集。</p><p>第三个统一，是统一的数据治理。用户的数据在被传输到中心后，将会被统一存储、统一压缩、统一部署，保存在观测云自己定义的逻辑数据存储架构中，相较于开源方案，观测云主要有两个优势：</p><ul><li>在存储的整合性角度，观测云的成本低于开源方案；</li><li>不论是指标数据、日志数据、链路数据、用户行为甚至未来开展到安全数据，区别于其他厂商平台，观测云通过自研的 DQL (Debug Query Language) 语言在数据分析平台进行整体分析，甚至针对不用用户使用不同时代的存储技术。</li></ul><p>第四个统一，是统一的拓展开发平台。当 DQL 或者软件本身的能力无法满足用户的需要时，用户可以通过 DataFlux Function 平台进行无限的拓展。比如出于中大型公司的安全合规要求，直接通过第三方平台发送 SaaS 平台的告警邮件到内部邮箱，会被直接屏蔽掉，而如果用户把自己的发件服务器注册到 DataFlux Function，就可以实现向内部邮箱发送告警邮件，类似的能力还有短信、电话等等。</p><p>除此之外，DataFlux Function 平台还可以实现对各种云平台甚至业务数据的载入，最近，观测云使用 DataFlux Function 帮客户实现了业务对账系统和 IT 代码调用间的关联。据悉，未来观测云会基于 DataFlux Function 平台推出一个抢占型实例量化交易的 Demo。</p><p>除了这四个“统一”，观测云还提供 Site Reliability Engineering (SRE) 和 Observability Engineering 的最佳实践，支持智能推测算法，可自动识别基础设施和应用程序的潜在问题。坚持每两周迭代升级，使工程师们可随时体验最前沿的技术和最尖端的能力。是目前市面上少见的，真正意义上做到了“观测性”实现的平台，真正地满足了企业系统及开发者们在观测性方面的需求。</p><p>百闻不如一试。当我们回溯平台选型的根本，发现当前市面上的可观测性平台质量良莠不齐，大多还处于“监控”状态，系统整体性较差，无论是对于企业来说还是对于开发者来说，试错成本高。而目前观测云 SaaS 版完全按量计费，没有初始成本，拒绝为闲置功能付费，可提供专属服务经理，让初创团队也能体验一流的全链路系统可观测能力，切实地解决了平台选型难的问题。</p><p>正如蒋烁淼所说的，“一个好的系统首先要做到采集、治理、分析、变成这四个方面的统一；其次，需要尽量兼容开源，将整个技术栈和开源进行一个双向的连接。观测云在未来将进一步下探存储成本，最终让利用户。”</p><h4 id="四、国产可观测性的未来需要更多厂商共同努力"><a href="#四、国产可观测性的未来需要更多厂商共同努力" class="headerlink" title="四、国产可观测性的未来需要更多厂商共同努力"></a>四、国产可观测性的未来需要更多厂商共同努力</h4><p>目前，可观测性还在发展初期，很多产品仍在探索阶段，也有很多问题亟待解决，未来人们对可观测性的需求只会越来越高。</p><p>谈及“可观测性”的未来，蒋烁淼认为，可观测性未来主要有四大趋势。观测性平台的开发厂商未来需要在以下几个方面做出改进和优化：</p><ul><li>提供更大的技术环境支持能力，支持标准的可观测协议；</li><li>引入更多的算法，以智能的方式做数据的巡检；</li><li>连接安全产品；</li><li>进一步降低存储成本。</li></ul><p>这与媒体第三方视角观察到的趋势保持相似。然而，纸上得来终觉浅，绝知此事要躬行。虽然国内像观测云等厂商紧跟行业需求及技术发展趋势，提供了自定义数据大盘、基础设施监控、日志分析、用户访问监测、应用性能检测、Profiling、云拨测、智能巡检等多种解决方案，力图打造可观测性最佳实践。但是，仅仅是一家或者几家的努力，是远远不够的。</p><p>行业的发展需要更多的人参与进来。观测性平台开发厂商需要挖掘更多用户需求，而普通开发者也应该将更多的注意力放到“可观测性”上，让可观测性工具成为业务优化的得力助手。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;blockquote&gt;
&lt;p&gt;本文是 “&lt;a href=&quot;https://www</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="OpenTelemetry" scheme="http://posts.hufeifei.cn/tags/OpenTelemetry/"/>
    
    <category term="Grafana" scheme="http://posts.hufeifei.cn/tags/Grafana/"/>
    
    <category term="eBPF" scheme="http://posts.hufeifei.cn/tags/eBPF/"/>
    
  </entry>
  
  <entry>
    <title>13 种高维向量检索算法全解析！</title>
    <link href="http://posts.hufeifei.cn/db/vector-search/"/>
    <id>http://posts.hufeifei.cn/db/vector-search/</id>
    <published>2022-11-02T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote><p>编者按：<br>以图搜图、商品推荐、社交推荐等社会场景中潜藏了大量非结构化数据，这些数据被工程师们表达为具有隐式语义的高维向量。为了更好应对高维向量检索这一关键问题，杭州电子科技大学计算机专业硕士王梦召等人探索并实现了「效率和精度最优权衡的近邻图索引」，并在数据库顶会 VLDB 2021 上发表成果。<br>作为连接生产和科研的桥梁，Zilliz 研究团队一直与学界保持交流、洞察领域前沿。此次，王梦召来到 Z 星，从研究动机、算法分析、实验观测和优化讨论等维度展开讲讲最新的科研成果。<br>以下是他的干货分享，<a href="https://arxiv.org/pdf/2101.12631.pdf">点击这里可获得论文全文</a></p></blockquote><h2 id="高维数据检索：基于近邻图的近似最近邻搜索算法实验综述"><a href="#高维数据检索：基于近邻图的近似最近邻搜索算法实验综述" class="headerlink" title="高维数据检索：基于近邻图的近似最近邻搜索算法实验综述"></a>高维数据检索：基于近邻图的近似最近邻搜索算法实验综述</h2><h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>向量检索是很多 AI 应用必不可少的基础模块。近年来，学术界和工业界提出了很多优秀的基于近邻图的ANNS 算法以及相关优化，以应对高维向量的检索问题。但是针对这些算法，目前缺乏统一的实验评估和比较分析。为了优化算法设计、进一步落地工业应用，我们完成了论文《A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search》。该论文聚焦实现了效率和精度最优权衡的近邻图索引，综述了 13 种具有代表性相关算法，实验在丰富的真实世界数据集上执行。我们的贡献可总结如下：</p><ol><li>根据图索引的特征，我们将基于近邻图的 ANNS 算法划分为四类，这给理解现存算法提供了一个新的视角。在此基础上，我们阐述了算法间的继承和发展关系，从而梳理出算法的发展脉络；</li><li>我们分解所有算法为 7 个统一的细粒度组件，它们构成一个完整的算法执行流程，从而实现了算法的原子级分析。我们可以公平评估当前工作在某一组件的优化通过控制其它组件一致；</li><li>我们采用多样的数据集（包括 8 个真实世界数据集，它们包括视频、语音、文本和图像生成的高维向量）和指标评估当前算法的全面性能；</li><li>我们提供了不同场景下最优算法推荐、算法优化指导、算法改进示例、研究趋势和挑战的分析讨论。</li></ol><h3 id="研究动机"><a href="#研究动机" class="headerlink" title="研究动机"></a>研究动机</h3><p>根据以下观测，我们对 13 种基于近邻图的 ANNS 算法进行了比较分析和实验综述：</p><ul><li>目前，学术界和工业界提出 10 余种常见的近邻图算法，但对于这些算法的合理分类和比较分析较为缺乏。根据我们的研究，这些算法的索引结构可归结为 4 种基础的图结构，但这些图存在着非常多的问题，如复杂度太高等。所以，在这 4 种图结构基础上有多种优化，如对相对邻域图（RNG）优化就包含 HNSW、DPG、NGT、NSG、SPTAG 等算法。</li><li>很多现有的论文从“分子”角度评估基于近邻图的 ANNS 算法（宏观角度）。然而，我们发现，这些算法有一个统一的“化学表达式”，它们还可再向下细分为“原子”（微观角度），从“原子”角度分析可以产生一些新发现，比如很多算法都会用到某一“原子”（HNSW，NSG，KGraph，DPG的路由是相同的）。</li><li>除了搜索效率和精度的权衡之外，基于近邻图的 ANNS 算法的其它特征（包含在索引构建和搜索中）也间接影响了最终的搜索性能。在搜索性能逐渐达到瓶颈的情况下，我们对于索引构建效率、内存消耗等指标给予了重视。</li><li>一个算法的优越性并不是一成不变的，数据集的特征在其中起着至关重要的作用。比如，在 Msong（语音生成的向量数据集）上NSG 的加速是 HNSW 的 125 倍；然而，在 Crawl（文本生成的向量数据集）上 HNSW 的加速是 NSG 的 80 倍。我们在多样的数据集上执行实验评估，从而对算法形成更全面的认识。</li></ul><p><strong>近邻图算法“分子”级分析</strong></p><p>在分析基于近邻图的 ANNS 算法之前，首先给大家介绍下 4 个经典的图结构，即：德劳内图（DG）、相对领域图（RNG）、K 近邻图（KNNG）、最小生成树（MST）。如图1所示，这四种图结构的差异主要体现在选边过程，简单总结如下：DG 确保任意 3 个顶点 x, y, z 形成的三角形 xyz 的外接圆内部及圆上不能有除了 x, y, z 之外的其它顶点；RNG 要确保(b)中月牙形区域内不能有其它点，这里的月牙形区域是分别以x和y为圆心，x 与 y 之间的距离为半径的两个圆的交集区域；KNNG 每个顶点连接 K 个最近的邻居；MST 在保证联通性的情况下所有边的长度（对应两个顶点的距离）之和最小。</p><p><img src="https://pic4.zhimg.com/80/v2-5f757eeabfdd24f73d2c9af11c70c137_1440w.webp"></p><p>图1 四种图结构在相同的数据集上的构建结果</p><p>接下来，我将基于图1 中的 4 个图结构来梳理 13 个基于近邻图的ANNS算法。为了避免翻译造成了理解偏差，算法名使用英文简称，算法的原论文链接、部分高质量的中文介绍、部分代码请见参考资料。各算法之间更宏观的联系可参考论文中的表2 和图3。</p><p><strong>算法1：NSW</strong></p><p>NSW 是对 DG 的近似，而 DG 能确保从任意一个顶点出发通过贪婪路由获取精确的结果（即召回率为 1 ）。NSW 是一个类似于“交通枢纽”的无向图，这会导致某些顶点的出度激增，从论文的表11 可知，NSW 在某些数据集上的最大出度可达几十万。NSW 通过增量插入式的构建，这确保了全局连通性，论文表4 中可知，NSW的连通分量数均为1。NSW 具有小世界导航性质：在构建早期，形成的边距离较远，像是一条“高速公路”，这将提升搜索的效率；在构建后期，形成的边距离较近，这将确保搜索的精度。</p><p>原文：<a href="https://www.sciencedirect.com/science/article/abs/pii/S0306437913001300">https://www.sciencedirect.com/science/article/abs/pii/S0306437913001300</a></p><p>中文介绍：<a href="https://blog.csdn.net/u011233351/article/details/85116719">https://blog.csdn.net/u011233351/article/details/85116719</a></p><p>代码：<a href="https://github.com/kakao/n2">https://github.com/kakao/n2</a></p><p><strong>算法2：HNSW</strong></p><p>HNSW 在 NSW 的基础上有两个优化：“层次化”和“选边策略”。层次化的实现较为直观：不同距离范围的边通过层次呈现，这样可以在搜索时形成类似于跳表结构，效率更高。选边策略的优化原理是：如果要给某个顶点连接 K 个邻居的话，NSW 选择 K 个距离最近的，而 HNSW 从大于 K 个最近的顶点里面选出更离散分布的邻居（见参考资料1）。因此，从选边策略考虑，HNSW 是对 DG 和 RNG 的近似。</p><p>原文：<a href="https://ieeexplore.ieee.org/abstract/document/8594636">https://ieeexplore.ieee.org/abstract/document/8594636</a></p><p>中文介绍：<a href="https://blog.csdn.net/u011233351/article/details/85116719">https://blog.csdn.net/u011233351/article/details/85116719</a></p><p>代码：<a href="https://github.com/kakao/n2">https://github.com/kakao/n2</a></p><p><strong>算法3：FANNG</strong></p><p>FANNG 的选边策略与 HNSW 是一样的，都是对RNG近似。FANNG 比 HNSW 更早提出，不过当前 HNSW 得到更普遍的应用，可能的原因是：（1）FANNG 的选边策略是在暴力构建的近邻图的基础上实现的，构建效率很低；（2）HNSW 通过增量式构建且引入分层策略，构建和搜索效率都很高；（3）HNSW 开源了代码，FANNG 则没有。</p><p>原文：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Harwood_FANNG_Fast_Approximate_CVPR_2016_paper.html">https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Harwood_FANNG_Fast_Approximate_CVPR_2016_paper.html</a></p><p><strong>算法4：NGT</strong></p><p>NGT 是雅虎日本开源的向量检索库，核心算法基于近邻图索引。NGT 在构建近邻图时类似于 NSW，也是对 DG 的近似，后续有一些度调整优化，其中最有效的路径优化也是对 RNG 的近似（论文的附件B 也给出了证明）。</p><p>原文1：<a href="https://link.springer.com/chapter/10.1007/978-3-319-46759-7_2">https://link.springer.com/chapter/10.1007/978-3-319-46759-7_2</a></p><p>原文2：<a href="https://arxiv.org/abs/1810.07355">https://arxiv.org/abs/1810.07355</a></p><p>代码：<a href="https://github.com/yahoojapan/NGT">https://github.com/yahoojapan/NGT</a></p><p><strong>算法5：SPTAG</strong></p><p>SPTAG 是微软发布的向量检索库，它的构建过程基于分治策略，即迭代地划分数据集，然后在每个子集上构建近邻图，接着归并子图，最后通过邻域传播策略进一步优化近邻图。上述过程旨在构建一个尽可能精确的 KNNG。在搜索时，SPTAG 采用树索引和图索引交替执行的方案，即先从树上获取距查询较近的点作为在图上搜索的起始点执行路由，当陷入局部最优时继续从树索引上获取入口点，重复上述操作直至满足终止条件。</p><p>原文1：<a href="https://dl.acm.org/doi/abs/10.1145/2393347.2393378">https://dl.acm.org/doi/abs/10.1145/2393347.2393378</a></p><p>原文2：<a href="https://ieeexplore.ieee.org/abstract/document/6247790">https://ieeexplore.ieee.org/abstract/document/6247790</a></p><p>原文3：<a href="https://ieeexplore.ieee.org/abstract/document/6549106">https://ieeexplore.ieee.org/abstract/document/6549106</a></p><p>中文介绍1：<a href="https://blog.csdn.net/whenever5225/article/details/108013045">https://blog.csdn.net/whenever5225/article/details/108013045</a></p><p>中文介绍2：<a href="https://cloud.tencent.com/developer/article/1429751">https://cloud.tencent.com/developer/article/1429751</a></p><p>代码：<a href="https://github.com/microsoft/SPTAG">https://github.com/microsoft/SPTAG</a></p><p>代码使用：<a href="https://blog.csdn.net/qq_40250862/article/details/95000703">https://blog.csdn.net/qq_40250862/article/details/95000703</a></p><p><strong>算法6：KGraph</strong></p><p>KGraph 是对 KNNG 的近似，是一种面向一般度量空间的近邻图构建方案。基于邻居的邻居更可能是邻居的思想，KGraph 能够快速构建一个高精度的 KNNG。后续的很多算法（比如 EFANNA、DPG、NSG、NSSG）都是在该算法的基础上的进一步优化。</p><p>原文：<a href="https://dl.acm.org/doi/abs/10.1145/1963405.1963487">https://dl.acm.org/doi/abs/10.1145/1963405.1963487</a></p><p>中文介绍：<a href="https://blog.csdn.net/whenever5225/article/details/105598694">https://blog.csdn.net/whenever5225/article/details/105598694</a></p><p>代码：<a href="https://github.com/aaalgo/kgraph">https://github.com/aaalgo/kgraph</a></p><p><strong>算法7：EFANNA</strong></p><p>EFANNA 是基于 KGraph 的优化。两者的差别主要体现在近邻图的初始化：KGraph 是随机初始化一个近邻图，而 EFANNA 是通过 KD 树初始化一个更精确的近邻图。此外，在搜索时，EFANNA 通过 KD 树获取入口点，而 KGraph 随机获取入口点。</p><p>原文：<a href="https://arxiv.org/abs/1609.07228">https://arxiv.org/abs/1609.07228</a></p><p>中文介绍：<a href="https://blog.csdn.net/whenever5225/article/details/104527500">https://blog.csdn.net/whenever5225/article/details/104527500</a></p><p>代码：<a href="https://github.com/ZJULearning/ssg">https://github.com/ZJULearning/ssg</a></p><p><strong>算法8：IEH</strong></p><p>类比 EFANNA，IEH 暴力构建了一个精确的近邻图。在搜索时，它通过哈希表获取入口点。</p><p>原文：<a href="https://ieeexplore.ieee.org/abstract/document/6734715/">https://ieeexplore.ieee.org/abstract/document/6734715/</a></p><p><strong>算法9：DPG</strong></p><p>在 KGraph 的基础上，DPG 考虑顶点的邻居分布多样性，避免彼此之间非常接近的邻居重复与目标顶点连边，最大化邻居之间的夹角，论文的附件4 证明了 DPG 的选边策略也是对 RNG 的近似。此外，DPG 最终添加了反向边，是无向图，因此它的最大出度也是非常高的（见论文附件表11）。</p><p>原文：<a href="https://ieeexplore.ieee.org/abstract/document/8681160">https://ieeexplore.ieee.org/abstract/document/8681160</a></p><p>中文介绍：<a href="https://blog.csdn.net/whenever5225/article/details/106176175">https://blog.csdn.net/whenever5225/article/details/106176175</a></p><p>代码：<a href="https://github.com/DBW">https://github.com/DBW</a></p><p><strong>算法10：NSG</strong></p><p>NSG 的设计思路与 DPG 几乎是一样的。在 KGraph 的基础上，NSG 通过 MRNG 的选边策略考虑邻居分布的均匀性。NSG 的论文中将 MRNG 的选边策略与 HNSW 的选边策略做了对比，例证了 MRNG 的优越性。论文中的附件1 证明了NSG的这种选边策略与 HNSW 选边策略的等价性。NSG 的入口点是固定的，是与全局质心最近的顶点。此外，NSG 通过 DFS 的方式强制从入口点至其它所有点都是可达的。</p><p>原文：<a href="https://www.vldb.org/pvldb/vol12/p461-fu.pdf">http://www.vldb.org/pvldb/vol12/p461-fu.pdf</a></p><p>中文介绍：<a href="https://zhuanlan.zhihu.com/p/50143204">https://zhuanlan.zhihu.com/p/50143204</a></p><p>代码：<a href="https://github.com/ZJULearning/nsg">https://github.com/ZJULearning/nsg</a></p><p><strong>算法11：NSSG</strong></p><p>NSSG 的设计思路与 NSG、DPG 几乎是一样的。在 KGraph 的基础上，NSSG 通过 SSG 选边策略考虑邻居分布的多样性。NSSG 认为，NSG 过度裁边了（见论文表4），相比之下 SSG 的裁边要松弛一些。NSG 与 NSSG 另一个重要的差异是，NSG 通过贪婪路由获取候选邻居，而 NSSG 通过邻居的一阶扩展获取候选邻居，因此，NSSG 的构建效率更高。</p><p>原文：<a href="https://ieeexplore.ieee.org/abstract/document/9383170">https://ieeexplore.ieee.org/abstract/document/9383170</a></p><p>中文介绍：<a href="https://zhuanlan.zhihu.com/p/100716181">https://zhuanlan.zhihu.com/p/100716181</a></p><p>代码：<a href="https://github.com/ZJULearning/ssg">https://github.com/ZJULearning/ssg</a></p><p><strong>算法12：Vamana</strong></p><p>简单来说，Vamana 是 KGraph、HNSW 和 NSG 三者的结合优化。在选边策略上，Vamana 在 HNSW （或 NSG）的基础上增加了一个调节参数，选边策略为 HNSW 的启发式选边，取不同的值执行了两遍选边。</p><p>原文：<a href="https://harsha-simhadri.org/pubs/DiskANN19.pdf">http://harsha-simhadri.org/pubs/DiskANN19.pdf</a></p><p>中文介绍：<a href="https://blog.csdn.net/whenever5225/article/details/106863674">https://blog.csdn.net/whenever5225/article/details/106863674</a></p><p><strong>算法13：HCNNG</strong></p><p>HCNNG 是目前为止唯一一个以 MST 为基本构图策略的向量检索算法。类似SPTAG，HCNNG 的构建过程基于分治策略，在搜索时通过 KD 树获取入口点。</p><p>原文：<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319302730">https://www.sciencedirect.com/science/article/abs/pii/S0031320319302730</a></p><p>中文介绍：<a href="https://whenever5225.github.io/2019/08/17/HCNNG/">https://whenever5225.github.io/2019/08/17/HCNNG/</a></p><h3 id="近邻图算法“原子”级分析"><a href="#近邻图算法“原子”级分析" class="headerlink" title="近邻图算法“原子”级分析"></a><strong>近邻图算法“原子”级分析</strong></h3><p>我们发现所有的基于近邻图的 ANNS 算法都遵循统一的处理流程框架，该流程里面有多个公共模块（如图2 的 C1-&gt;C7）。当一个近邻图算法按照该流程被解构后，我们可以很容易了解该算法是如何设计组装的，这将给后续近邻图向量检索算法的设计带来很大的便利性。此外，我们也可固定其它模块，保持其他模块完全相同，从而更公平地评估不同算法在某一模块实现上的性能差异。</p><p><img src="https://pic3.zhimg.com/80/v2-c0bf2630a39dc9dbd98a8e83b994b7d6_1440w.webp"></p><p>图2 近邻图向量检索算法遵循的统一流程框架图</p><p>接下来，我们以 HNSW 和 NSG 为例，说明如何实现图2 的流程框架分析算法。在此之前，我们要先熟悉这两个算法的索引构建和搜索过程。首先是 HNSW 分解，HNSW 的构建策略是增量式的。因此，每插入一个数据点都要执行一遍 C1-&gt;C3 过程。</p><p><strong>HNSW 分解流程：</strong></p><table><thead><tr><th>模块</th><th>HNSW 具体实现</th></tr></thead><tbody><tr><td>C1</td><td>生成新插入点所处的最大层；获取搜索入口点</td></tr><tr><td>C2</td><td>新插入点作为查询点，从入口点开始，贪婪搜索，返回新插入点一定量最近邻作为邻居候选</td></tr><tr><td>C3</td><td>启发式选边策略</td></tr><tr><td>C4</td><td>无额外步骤，最高层中的顶点作为入口</td></tr><tr><td>C5</td><td>无额外步骤，增量式构建已隐式确保连通性（启发式选边又一定程度破坏连通性）</td></tr><tr><td>C6</td><td>最高层的顶点作为入口</td></tr><tr><td>C7</td><td>最佳优先搜索</td></tr></tbody></table><p><strong>NSG 分解流程：</strong></p><table><thead><tr><th>模块</th><th>NSG 具体实现</th></tr></thead><tbody><tr><td>C1</td><td>NN-Descent 初始化近邻图</td></tr><tr><td>C2</td><td>顶点作为查询，贪婪搜索获取邻居候选</td></tr><tr><td>C3</td><td>MRNG 选边策略</td></tr><tr><td>C4</td><td>全局质心作为查询，贪婪搜索获取最近顶点作为入口</td></tr><tr><td>C5</td><td>从入口开始，DFS 确保连通性</td></tr><tr><td>C6</td><td>C4 获取的入口</td></tr><tr><td>C7</td><td>最佳优先搜索</td></tr></tbody></table><p>由于 HNSW 的 C3 与 NSG 的 C3 是等价的，因此，从上面两个表格可知，HNSW 与 NSG 这两个算法差别并不大。其实，论文涉及到的 13 种算法中很多算法之间都是很相似的，详见论文第 4 章。</p><h3 id="实验观测和讨论"><a href="#实验观测和讨论" class="headerlink" title="实验观测和讨论"></a><strong>实验观测和讨论</strong></h3><p>具体的实验评估请参考论文第 5 章，接下来将概括介绍一下实验的观测结果和讨论：</p><p><strong>不同场景下的算法推荐</strong></p><p>NSG 和 NSSG 普遍有最小的索引构建时间和索引尺寸，因此，它们适用于有大量数据频繁更新的场景；如果我们想要快速构建一个精确的 KNNG（不仅用于向量检索），KGraph、EFANNA 和 DPG 更适合；DPG 和 HCNNG 有最小的平均搜索路径长度，因此，它们适合需要 I/O 的场景；对于 LID 值高的较难数据集，HNSW、NSG、HCNNG 比较适合；对于 LID 值低的简单数据集，DPG、NSG、HCNNG 和 NSSG 较为适合；NGT 有更小的候选集尺寸，因此适用于 GPU 加速（考虑到 GPU 的内存限制）；当对内存消耗要求较高时，NSG 和 NSSG 适合，因为它们内存占用更小。</p><p><strong>算法设计向导</strong></p><p>一个实用的近邻图向量检索算法一般满足以下四个方面：</p><ol><li>高构建效率</li><li>高路由效率</li><li>高搜索精度</li><li>低内存负载</li></ol><p>针对第一方面，我们不要在提升近邻图索引质量（即一个顶点的邻居中真实的最近邻居所占的比例）上花费太多的时间。因为最好的图质量（可通过图中与距它最近的顶点有边相连的顶点所占比例度量）不一定实现最佳搜索性能（结合论文中表4 和图7、8）。针对第二方面，我们应当控制适当的平均出度，多样化邻居的分布，减少获取入口点的花费，优化路由策略，从而减少收敛到查询点的邻域所需的距离计算次数。针对第三方面，我们应当合理设计邻居的分布，确保连通性，从而提升对陷入局部最优的”抵抗力”。针对第四方面，我们应当优化邻居选择策略和路由策略，从而减小出度和候选集尺寸。</p><p><strong>优化算法示例</strong></p><p>基于上面的向导，我们组装了一个新的近邻图算法（图3 中的 OA），该算法在图2 中的 7 个组件中每个组件选中现存算法的一个具体实现，即 C1 采用 KGraph 算法的实现；C2 采用 NSSG 算法的实现；C3 采用 NSG 算法的实现；C4 采用 DPG 算法的实现；C5 采用 NSSG 算法的实现；C6 采用 DPG 算法的实现；C7 采用 HCNNG 和 NSW 算法的实现。OA 算法实现了当前最优的综合性能，详见论文原文。因此，我们甚至不用优化当前算法，仅仅把现存算法的不同部分组装起来就可以形成一个新算法。</p><p><img src="https://pic3.zhimg.com/80/v2-01854acaec370d48268c141d3ca1ad7e_1440w.webp"></p><p>图3 OA 算法与当前最优的近邻图算法的搜索性能对比</p><p><strong>趋势与挑战</strong></p><p>基于 RNG 的选边策略设计在当前近邻图向量检索算法的效率提升中起到了关键作用。在论文的评估中，唯一一个基于 MST 的算法 HCNNG 也表现出来很好的综合性能。在上述纯近邻图算法基础上，后续发展有三个主要方向：</p><ol><li>硬件优化；</li><li>机器学习优化；</li><li>更高级的查询需求，比如结构化和非结构化混合查询。</li></ol><p>我们未来面对这三个挑战：</p><ol><li>数据编码技术与图索引如何有机结合以解决近邻图向量检索算法高内存消耗问题；</li><li>硬件加速近邻图索引构建减少近邻图索引构建时间；</li><li>根据不同场景的特征自适应选择最优的近邻图算法。  </li></ol><p><strong>参考资料</strong></p><p><a href="https://blog.csdn.net/whenever5225/article/details/106061653%3Fspm%3D1001.2014.3001.5501">https://blog.csdn.net/whenever5225/article/details/106061653?spm=1001.2014.3001.5501</a></p><p><strong>✏️ 关于作者：</strong></p><p>王梦召，杭州电子科技大学计算机专业硕士。主要关注基于近邻图的向量相似性检索、多模态检索等研究内容，并在相关方向申请发明专利三项，在数据库顶会 VLDB 和 SCI 一区 top 期刊 KBS 等发表论文两篇。</p><p>日常爱好弹吉他、打乒乓球、跑步、看书，他的个人网站是 <a href="https://mzwang.top/">http://mzwang.top</a>，Github主页是 <a href="http://github.com/whenever5225">http://github.com/whenever5225</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;blockquote&gt;
&lt;p&gt;编者按：&lt;br&gt;以图搜图、商品推荐、社交推荐等社会</summary>
      
    
    
    
    <category term="数据库" scheme="http://posts.hufeifei.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="VectorSearch" scheme="http://posts.hufeifei.cn/tags/VectorSearch/"/>
    
    <category term="SPTAG" scheme="http://posts.hufeifei.cn/tags/SPTAG/"/>
    
    <category term="KGraph" scheme="http://posts.hufeifei.cn/tags/KGraph/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出分析LSM树（日志结构合并树）</title>
    <link href="http://posts.hufeifei.cn/lsm-tree/"/>
    <id>http://posts.hufeifei.cn/lsm-tree/</id>
    <published>2022-08-02T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="零、前言"><a href="#零、前言" class="headerlink" title="零、前言"></a>零、前言</h2><p>最近在调研NoSQL数据库，发现RocksDB、LevelDB、HBase以及Prometheus等，其底层的存储引擎都是基于LSM树，于是决定花时间彻底吃透LSM树这一数据结构。</p><p>不幸的是，在查阅资料学习的过程中，发现网上各种文章汗牛充栋、抄来抄去，不是文不对题就是不知所云。</p><p>一气之下决定自己写一篇出来消消气，便有了这篇文章。。。</p><p>PS：学了这么多数据结构，LSM-Tree应该是最年轻的一个，它在1996年被设计出来(属老鼠的)，年纪比我还小~</p><p>相比于B/B+树或者倒排索引，LSM-Tree采用了“疯狂到不顾一切”的干啥都磁盘顺序写的方案，赋予了它无与伦比的写吞吐量。</p><h2 id="一、LSM树数据结构定义"><a href="#一、LSM树数据结构定义" class="headerlink" title="一、LSM树数据结构定义"></a>一、LSM树数据结构定义</h2><p>查阅了一些资料，LSM树并没有一种固定死的实现方式，更多的是一种将：</p><p>“磁盘顺序写” + “多个树(状数据结构)” + “冷热（新老）数据分级” + “定期归并” + “非原地更新”这几种特性统一在一起的思想。</p><p>为了方便后续的讲解分析，我们尝试先对LSM树做一个定义。</p><p>LSM树的定义：</p><ol><li>LSM树是一个横跨内存和磁盘的，包含多颗”子树”的一个森林。</li><li>LSM树分为Level 0，Level 1，Level 2 … Level n 多颗子树，其中只有Level 0在内存中，其余Level 1-n在磁盘中。</li><li>内存中的Level 0子树一般采用排序树（红黑树/AVL树）、跳表或者TreeMap等这类有序的数据结构，方便后续顺序写磁盘。</li><li>磁盘中的Level 1-n子树，本质是数据排好序后顺序写到磁盘上的文件，只是叫做树而已。</li><li>每一层的子树都有一个阈值大小，达到阈值后会进行合并，合并结果写入下一层。</li><li>只有内存中数据允许原地更新，磁盘上数据的变更只允许追加写，不做原地更新。<br>以上6条定义组成了LSM树，如图1所示。</li></ol><p><img src="https://pic4.zhimg.com/80/v2-9359003a0c88626b33979b784982e9ab_1440w.jpg" alt="LSM树的组成和定义"></p><ul><li>图1中分成了左侧绿色的内存部分和右侧蓝色的磁盘部分（定义1）。</li><li>图1左侧绿色的内存部分只包含Level 0树，右侧蓝色的磁盘部分则包含Level 1-n等多棵”树”（定义2）</li><li>图1左侧绿色的内存部分中Level 0是一颗二叉排序树（定义3）。注意这里的有序性，该性质决定了LSM树优异的读写性能。</li><li>图1右侧蓝色的磁盘部分所包含的Level 1到Level n多颗树，虽然叫做“树”，但本质是按数据key排好序后，顺序写在磁盘上的一个个文件（定义4） ，注意这里再次出现了有序性。</li><li>内存中的Level 0树在达到阈值后，会在内存中遍历排好序的Level 0树并顺序写入磁盘的Level 1。同样的，在磁盘中的Level n（n&gt;0）达到阈值时，则会将Level n层的多个文件进行归并，写入Level n+1层。（定义5）</li><li>除了内存中的Level 0层做原地更新外，对已写入磁盘上的数据，都采用Append形式的磁盘顺序写，即更新和删除操作并不去修改老数据，只是简单的追加新数据。图1中右侧蓝色的磁盘部分，Level 1和Level 2均包含key为2的数据，同时图1左侧绿色内存中的Level 0树也包含key为2的数据节点。（定义6）</li></ul><p>下面我们遵循LSM树的6条定义，通过动图对LSM树的增、删、改、查和归并进行详细分析。</p><h2 id="二、插入操作"><a href="#二、插入操作" class="headerlink" title="二、插入操作"></a>二、插入操作</h2><p>LSM树的插入较简单，数据无脑往内存中的Level 0排序树丢即可，并不关心该数据是否已经在内存或磁盘中存在。（已经存在该数据的话，则场景转换成更新操作，详见第四部分）</p><p>图2展示了，新数据直接插入Level 0树的过程。</p><p><img src="https://vdn6.vzuu.com/SD/b8494042-21d0-11ec-8331-6ad01c902347.mp4?pkey=AAVDwY2TcaW2d_7ijDc-8u5T-5xlKGNCqNA9jDGhxTMJ4WxbuLqwp88WkAH39SvTwjCKsbV01Iyi1Zjkckr1FcZl&c=avc.0.0&f=mp4&pu=078babd7&bu=078babd7&expiration=1661843826&v=ks6"></p><p>如上图2所示，我们依次插入了key=9、1、6的数据，这三个数据均按照key的大小，插入内存里的Level 0排序树中。该操作复杂度为树高log(n)，n是Level 0树的数据量，可见代价很低，能实现极高的写吞吐量。</p><h2 id="三、删除操作"><a href="#三、删除操作" class="headerlink" title="三、删除操作"></a>三、删除操作</h2><p>LSM树的删除操作并不是直接删除数据，而是通过一种叫“墓碑标记”的特殊数据来标识数据的删除。</p><p>删除操作分为：<code>待删除数据在内存中</code>、<code>待删除数据在磁盘中</code> 和 <code>该数据根本不存在</code>三种情况。</p><h3 id="3-1-待删除数据在内存中："><a href="#3-1-待删除数据在内存中：" class="headerlink" title="3.1 待删除数据在内存中："></a>3.1 待删除数据在内存中：</h3><p>如图3所示，展示了待删除数据在内存中的删除过程。我们不能简单地将Level 0树中的黄色节点2删除，而是应该采用墓碑标记将其覆盖（思考题：为什么不能直接删除而是要用墓碑标记覆盖呢）</p><p><img src="https://pic4.zhimg.com/v2-7b764ddb2404b3b3e2d3f19b917a7bf3_b.webp" alt="LSM树删除操作示例——待删除数据在内存中时"></p><h3 id="3-2-待删除数据在磁盘中："><a href="#3-2-待删除数据在磁盘中：" class="headerlink" title="3.2 待删除数据在磁盘中："></a>3.2 待删除数据在磁盘中：</h3><p>如图4所示，展示了待删除数据在磁盘上时的删除过程。我们并不去修改磁盘上的数据（理都不理它），而是直接向内存中的Level 0树中插入墓碑标记即可。</p><p><img src="https://pic4.zhimg.com/v2-6130424a556ba99a040bcd474371d347_b.webp" alt="LSM树删除操作示例——待删除数据在磁盘中时"></p><h3 id="3-3-待删除数据根本不存在："><a href="#3-3-待删除数据根本不存在：" class="headerlink" title="3.3 待删除数据根本不存在："></a>3.3 待删除数据根本不存在：</h3><p>这种情况等价于在内存的Level 0树中新增一条墓碑标记，场景转换为情况3.2的内存中插入墓碑标记操作。</p><p>综合看待上述三种情况，发现不论数据有没有、在哪里，删除操作都是等价于向Level 0树中写入墓碑标记。该操作复杂度为树高log(n)，代价很低。</p><h2 id="四、修改操作"><a href="#四、修改操作" class="headerlink" title="四、修改操作"></a>四、修改操作</h2><p>LSM树的修改操作和删除操作很像，也是分为三种情况：<code>待修改数据在内存中</code>、<code>在磁盘中</code>和 <code>该数据根本不存在</code>。</p><h3 id="4-1-待修改数据在内存中："><a href="#4-1-待修改数据在内存中：" class="headerlink" title="4.1 待修改数据在内存中："></a>4.1 待修改数据在内存中：</h3><p><img src="https://pic4.zhimg.com/v2-238724222503b9be0316c3554c5ae29f_b.webp" alt="LSM树修改操作示例——待修改数据在内存中时"></p><p>如图5所示，展示了待修改数据在内存中的操作过程。新的蓝色的key=7的数据，直接定位到内存中Level 0树上黄色的老的key=7的位置，将其覆盖即可。</p><h3 id="4-2-待修改数据在磁盘中："><a href="#4-2-待修改数据在磁盘中：" class="headerlink" title="4.2 待修改数据在磁盘中："></a>4.2 待修改数据在磁盘中：</h3><p><img src="https://pic3.zhimg.com/v2-482e280b9b27876ff5302b845c95353e_b.webp" alt="LSM树修改操作示例——待修改数据在磁盘中时"></p><p>如图6所示，展示了待修改数据在磁盘中的操作过程。LSM树并不会去磁盘中的Level 1树上原地更新老的key=7的数据，而是直接将新的蓝色的节点7插入内存中的Level 0树中。</p><h3 id="4-3-该数据根本不存在："><a href="#4-3-该数据根本不存在：" class="headerlink" title="4.3 该数据根本不存在："></a>4.3 该数据根本不存在：</h3><p>此场景等价于情况b，直接向内存中的Level 0树插入新的数据即可。</p><p>综上4.1、4.2、4.3三种情况可以看出，修改操作都是对内存中Level 0进行覆盖/新增操作。该操作复杂度为树高log(n)，代价很低。</p><p>我们会发现，LSM树的增加、删除、修改（这三个都属于写操作）都是在内存中倒腾，完全没涉及到磁盘操作，所以速度飞快，写吞吐量高的离谱。。。</p><h2 id="五、查询操作"><a href="#五、查询操作" class="headerlink" title="五、查询操作"></a>五、查询操作</h2><p>LSM树的查询操作会按顺序查找Level 0、Level 1、Level 2 … Level n 每一颗树，一旦匹配便返回目标数据，不再继续查询。该策略保证了查到的一定是目标key最新版本的数据（有点MVCC的感觉）。</p><p>我们来分场景分析：依然分为 <code>待查询数据在内存中</code> 和 <code>待查询数据在磁盘中</code> 两种情况。</p><h3 id="5-1-待查询数据在内存中："><a href="#5-1-待查询数据在内存中：" class="headerlink" title="5.1 待查询数据在内存中："></a>5.1 待查询数据在内存中：</h3><p>如图7所示，展示了待查询数据在内存中时的查询过程。</p><p><img src="https://pic4.zhimg.com/v2-49a19f8261209dfda34b0901fa6c2e77_b.webp" alt="LSM树查询操作示例——待查询数据在内存中时"></p><p>沿着内存中已排好序的Level 0树递归向下比较查询，返回目标节点即可。我们注意到磁盘上的Level 1树中同样包括一个key=6的较老的数据。但LSM树查询的时候会按照Level 0、1、2 … n的顺序查询，一旦查到第一个就返回，因此磁盘上老的key=6的数据没人理它，更不会作为结果被返回。</p><h3 id="5-2-待查询数据在磁盘中："><a href="#5-2-待查询数据在磁盘中：" class="headerlink" title="5.2 待查询数据在磁盘中："></a>5.2 待查询数据在磁盘中：</h3><p>如图8所示，展示了待查询数据在磁盘上时的查询过程。</p><p><img src="https://vdn.vzuu.com/SD/7b5d5564-21d1-11ec-b7c1-3a28dfb2838d.mp4?disable_local_cache=1&bu=078babd7&c=avc.0.0&f=mp4&expiration=1661761026&auth_key=1661761026-0-0-34096a369e904db9843921c686e773bb&v=ali&pu=078babd7" alt="LSM树查询操作示例——待查询数据在磁盘中时"></p><p>先查询内存中的Level 0树，没查到便查询磁盘中的Level 1树，还是没查到，于是查询磁盘中的Level 2树，匹配后返回key=6的数据。</p><p>综合上述两种情况，我们发现，LSM树的查询操作相对来说代价比较高，需要从Level 0到Level n一直顺序查下去。极端情况是LSM树中不存在该数据，则需要把整个库从Level 0到Level n给扫了一遍，然后返回查无此人（可以通过 布隆过滤器 + 建立稀疏索引 来优化查询操作）。代价大于以B/B+树为基本数据结构的传统RDB存储引擎。</p><h2 id="六、合并操作"><a href="#六、合并操作" class="headerlink" title="六、合并操作"></a>六、合并操作</h2><p>合并操作是LSM树的核心（毕竟LSM树的名字就叫: 日志结构合并树，直接点名了合并这一操作）</p><p>之所以在增、删、改、查这四个基本操作之外还需要合并操作：一是因为内存不是无限大，Level 0树达到阈值时，需要将数据从内存刷到磁盘中，这是合并操作的第一个场景；二是需要对磁盘上达到阈值的顺序文件进行归并，并将归并结果写入下一层，归并过程中会清理重复的数据和被删除的数据(墓碑标记)。我们分别对上述两个场景进行分析：</p><h3 id="6-1-内存数据写入磁盘的场景："><a href="#6-1-内存数据写入磁盘的场景：" class="headerlink" title="6.1 内存数据写入磁盘的场景："></a>6.1 内存数据写入磁盘的场景：</h3><p>如图9所示，展示了内存中Level 0树在达到阈值后，归并写入磁盘Level 1树的场景。</p><p><img src="https://pic4.zhimg.com/v2-52dc81ca11590af2c57a248b106f889f_b.webp" alt="LSM树合并操作示例——内存数据写入磁盘"></p><p>对内存中的Level 0树进行中序遍历，将数据顺序写入磁盘的Level 1层即可，我们可以看到因为Level 0树是已经排好序的，所以写入的Level 1中的新块也是有序的（有序性保证了查询和归并操作的高效）。此时磁盘的Level 1层有两个Block块。</p><h3 id="6-2-磁盘中多个块的归并："><a href="#6-2-磁盘中多个块的归并：" class="headerlink" title="6.2 磁盘中多个块的归并："></a>6.2 磁盘中多个块的归并：</h3><p>如图10所示，该图展示了磁盘中Level 1层达到阈值时，对其包含的两个Block块进行归并，并将归并结果写入Level 2层的过程。</p><p><img src="https://vdn6.vzuu.com/SD/47cada78-21d6-11ec-bf49-362c95d6552e.mp4?pkey=AAVt2c002M_K16jt9Hs0ANxxksTXkRgAO_c5neGOwgVGShthIeH7nWOcfewoHINd68d-iAR0Qdsv2uceM0I1efJ9&c=avc.0.0&f=mp4&pu=078babd7&bu=078babd7&expiration=1661843826&v=ks6" alt="LSM树合并操作示例——磁盘上的多个块合并"></p><p>我们注意到key=5和key=7的数据同时存在于较老的Block 1和较新的Block 2中。而归并的过程是保留较新的数据，于是我们看到结果中，key=5和7的数据都是红色的（来自于较新的Block2）。</p><p>综上我们可以看到，不论是场景6.1还是场景6.2，由于原始数据都是有序的，因此归并的过程只需要对数据集进行一次扫描即可，复杂度为O(n)。</p><h2 id="七、优缺点分析"><a href="#七、优缺点分析" class="headerlink" title="七、优缺点分析"></a>七、优缺点分析</h2><p>以上便是对LSM树的增、删、改、查和归并五种核心操作的详细分析。</p><p>可以看到LSM树将增、删、改这三种操作都转化为内存insert + 磁盘顺序写(当Level 0满的时候)，通过这种方式得到了无与伦比的写吞吐量。</p><p>LSM树的查询能力则相对被弱化，相比于B+树的最多3~4次磁盘IO，LSM树则要从Level 0一路查询Level n，极端情况下等于做了全表扫描。（即便做了稀疏索引，也是lg(N0)+lg(N1)+…+lg(Nn)的复杂度，大于B+树的lg(N0+N1+…+Nn)的时间复杂度）。</p><p>同时，LSM树只append追加不原地修改的特性引入了归并操作，归并操作涉及到大量的磁盘IO，比较消耗性能，需要合理设置触发该操作的参数。</p><p>综上我们可以给出LSM树的优缺点：</p><ul><li><p>优：增、删、改操作飞快，写吞吐量极大。</p></li><li><p>缺：读操作性能相对被弱化；不擅长区间范围的读操作；归并操作较耗费资源。</p></li></ul><p>LSMTree的增、删、改、查四种基本操作的时间复杂度分析如下所示：</p><table><thead><tr><th>操作</th><th>平均代价</th><th>最坏情况代价</th></tr></thead><tbody><tr><td>插入</td><td>1</td><td>1</td></tr><tr><td>删除</td><td>1</td><td>1</td></tr><tr><td>修改</td><td>1</td><td>1</td></tr><tr><td>查找</td><td>lgN</td><td>lgN</td></tr></tbody></table><h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>以上是对LSM树基本操作以及优缺点的分析，我们可以据此得出LSM树的设计原则：</p><p>先内存再磁盘</p><p>内存原地更新</p><p>磁盘追加更新</p><p>归并保留新值</p><p>如果说B/B+树的读写性能基本平衡的话，LSM树的设计原则通过舍弃部分读性能，换取了无与伦比的写性能。该数据结构适合用于写吞吐量远远大于读吞吐量的场景，得到了NoSQL届的喜爱和好评。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;零、前言&quot;&gt;&lt;a href=&quot;#零、前言&quot; class=&quot;head</summary>
      
    
    
    
    <category term="数据库" scheme="http://posts.hufeifei.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="LSM-Tree" scheme="http://posts.hufeifei.cn/tags/LSM-Tree/"/>
    
  </entry>
  
  <entry>
    <title>基于内容的图像搜索实现</title>
    <link href="http://posts.hufeifei.cn/algorithm/image-hash/"/>
    <id>http://posts.hufeifei.cn/algorithm/image-hash/</id>
    <published>2022-02-27T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="图像搜索引擎一般有三种实现方式"><a href="#图像搜索引擎一般有三种实现方式" class="headerlink" title="图像搜索引擎一般有三种实现方式"></a>图像搜索引擎一般有三种实现方式</h2><p>1、Search By Metadata，这种方式不会考虑图片本身内容（图片包含物体，以及图像像素分布等），纯粹根据图像标签来进行检索。如果某个网页中有一张赛马的图片，并且网页文本内容中包含“赛马”（或者相关词汇）的文字，当用户搜索“赛马”、“马”、“horse”等关键字时，搜索引擎就会把这张图当作检索结果返回给用户。换句话说，此时的图像搜索引擎干的事情跟普通搜索引擎差不多，匹配关键词，并将对应图片返回给用户。这种工作方式的优点是速度快，在普通搜索引擎的技术基础之上很容易改进去实现。缺点也很明显，它完全依赖于描述图片的文字（标签），如果描述图片的文字不对或者相关性不大时，搜索准确性可想而知，比如我这篇博客中如果插入一张“猫”的照片，但是整篇博客文章对“猫”只字不提，那么基于Search By Metadata的搜索引擎很难找到博客中猫的图片。有一类图片分享网站要求用户在上传图片时，人工用几个词汇描述图片中有什么（标签），便于后面基于Metadata的搜索。当然也不排除一些基于深度学习的图片分类自动打标签的方式。</p><p>2、Search By Example，这种方式考虑图片本身内容（图片包含物体，以及图片像素分布等等），用户输入图片，搜索引擎根据图片内容，返回与该图片相似的图片结果。这种方式相比Search By Metadata要复杂一些，当然如果实现恰当，准确性也更能得到保障。如果用户上传一张包含“马”的图片，那么搜索引擎会返回包含马的其他图片，或者，当用户上传一张“沙滩”的风景图片，搜索引擎会返回一堆海边沙滩的图片，这些图片包含类似的内容：沙滩、天空、大海、游客。Search By Example的方式就是我们熟知的“以图搜图”，通过一张图找出网上相似的其他图片。实现方式有很多，有基于深度学习的方式，也有传统的图像分析算法，后者也是本篇文章后面会详细介绍的部分。</p><p>3、第三种就是前两种的结合体，具体就不多说了，技术互补，到达最佳效果。</p><p>本篇文章主要介绍利用传统图像分析算法如何实现Search By Example的搜索引擎（以图搜索）。</p><blockquote><p>源码地址：<a href="https://github.com/sherlockchou86/cbir-image-search">https://github.com/sherlockchou86/cbir-image-search</a></p></blockquote><h2 id="图像指纹"><a href="#图像指纹" class="headerlink" title="图像指纹"></a>图像指纹</h2><p>人类有指纹，通过指纹对比可以判断两个人是否是同一个人，换句话说，指纹可以当作人类独一无二的标识符。图片也有类似的概念，成千上万的图片中，每张都有自己的特点，通过某种方式提取它的特征，可以作为图像对比时的关键依据。</p><p><img src="https://pic3.zhimg.com/80/v2-a76d2ccf73e824b217dbb38569e282c6_1440w.jpg"></p><p>图像指纹提取通常称为“特征提取”，在程序代码中，提取到的图像特征通常用一个多维向量或者一串64/126位二进制数字表示。特征提取的方式有很多种（后面会介绍三类），不管通过哪种方式提取图像特征，都应该保证：通过特征对比，我们可以反推原始图片的相似度。</p><h3 id="通过图像特征评价图片相似度"><a href="#通过图像特征评价图片相似度" class="headerlink" title="通过图像特征评价图片相似度"></a>通过图像特征评价图片相似度</h3><p>假设我们已经为两张图片提取到了合适的图像特征，如何操作我们可以通过特征来反应原始图片的相似度呢？一般我们计算两个特征之前的距离，通过距离反映原始图像的相似度，如果距离越近，两张原始图片相似度更高，反之亦然。</p><p>计算距离的方式有很多，需要根据特征提取的方式来选择不同的距离计算方法，通常有以下几个距离计算方法：</p><p>（1）欧氏距离。就是欧几里德距离，这个距离应该是我们最熟悉的，在小学就接触了。我们在计算直线上两点的距离就是指欧式距离，上初中时计算平面坐标系中两点的直线距离也是指欧式距离，上高中时计算三维坐标系中两点的直线距离同样指欧式距离，四维、五维甚至更高维都是这种计算方式。每个点的坐标用多维向量表示（a1, a2,…, an）和（b1,b2,…,bn），那么两个向量之间的欧式距离为：(a1-b1)^2 + (a2-b2)^2 + … + (an -bn)^2，然后开方。</p><p>（2）汉明距离。这个距离其实普通程序员应该也比较熟悉，它指两个字符串（或者两个位数相同的二进制数）对应位置不相同的字符个数，汉明距离越大，代表两个串不相同的字符越多，两个串相似度越低，反之亦然。如果是两个二进制串计算汉明距离（1001010110）和（1001010100），非常简单，直接使用XOR（异或运算符）即可，1001010110 xor 1001010100，然后计算结果中1的个数。</p><p>（3）余弦距离。余弦距离一般程序员可能用不到，它指两个向量之间夹角的余弦值。二维平面坐标系中，向量（a1,a2）和向量（b1,b2）之间的夹角余弦值很好计算，同理，三维、四维也一样。余弦距离代表两个向量中各个分量的占比程度，比如（3,3）和（5,5）两个向量的余弦距离为0，因为每个向量中各个分量占比都为50%（3/6，3/6 和 5/10，5/10）。同理三维向量（2,3,4）和（4,6,8）之间的余弦距离也是0。两个向量的余弦距离为零，不代表它们之间的欧式距离为零。但是反过来却成立。</p><p>（4）卡方距离。卡方距离主要用于衡量两个概率分布的相似性。可以假设两个多维向量中的每个分量都代表概率（分量的和为1），那么卡方距离就是计算这两个概率分布的相似度。具体内容请网络搜索。</p><h2 id="以图搜图实现原理"><a href="#以图搜图实现原理" class="headerlink" title="以图搜图实现原理"></a>以图搜图实现原理</h2><p>以图搜图的过程其实很简单，跟普通的检索流程差不多。关键有两个：</p><p>（1）一个是特征提取方法，这个是重点，如何合理提取图像特征是保证准确性的第一要素；</p><p>（2）二个就是如何提高特征对比的速度，成千上万张图片源，如何快速从中找出相同/相似的图片？</p><p><img src="https://pic1.zhimg.com/80/v2-cec83068a21385df57a710921244a5ec_1440w.jpg"></p><p>下面介绍三种传统图像特征提取的方式。</p><h3 id="图像特征点"><a href="#图像特征点" class="headerlink" title="图像特征点"></a>图像特征点</h3><p>任何一张数字图片，微观上看，像素之前总是能找到一些规律的，像素分布、像素值、像素密度等等。OpenCV内置很多特征点提取算法，比如ORB、SURF以及SIFT等等。以SIFT举例，输入图片，输出该图片中具有某些特征的点（可以存在多个），从一定程度上讲，这些点可以被当作图片的标识，每个特征点用一个128维的向量表示。如果要比较两张图片的相似度，先分别提取两张图片的SIFT特征点，然后进行特征点匹配，看有多少对特征点能够匹配上，如果能匹配的点超过一定数量，那么认为这两张图片相似。这个匹配的过程可能采用前面提到的各种距离比较。下面两张图通过特征点提取、特征点匹配后得到的结果，可以判断两张图片相似：</p><p><img src="https://pic1.zhimg.com/80/v2-06df9e3a20d1adc8f24949d0348639d0_1440w.jpg"></p><p>可以看到，虽然图片拍摄角度不一样，但是包含的物体一致、场景类似，通过特征点匹配，可以找到很多匹配到的特征点。</p><p>这里的图像特征用128维向量表示，可以包含多个，如果一张图找到了100个特征点，那么每张图有128*100个float数据需要存储。</p><h3 id="图像感知哈希"><a href="#图像感知哈希" class="headerlink" title="图像感知哈希"></a>图像感知哈希</h3><p>哈希算法、哈希函数我们经常听说，对于不同长度的输入，哈希算法可以产生固定长度的输出。那么我们能否直接通过计算图片文件的哈希值来判断图片是否相似/相同呢？答案是不能。这种方式计算出来的哈希值对评价原始图片相似度没有任何参考价值，因为即使是相似的图片通过这种方式计算出来的哈希值之间并不相似（可能相差十万八千里），换句话说，我们不能通过哈希值之间的距离来反推原始图片的相似度是多少。假如在一张图片上修改了一点点，最后生成的哈希值跟原来的哈希值相差很远。</p><p><img src="https://pic1.zhimg.com/80/v2-dbecfcf897a6d3ab874fd086c1def938_1440w.jpg"></p><p>因此我们需要用到图像感知哈希算法，这种方法将图片内容（更准确地讲，应该是像素）考虑进来了。通过这种方式计算得到的哈希值可以反推出原始图片之间的相似度。图像感知哈希算法常见分三类：ahash，phash，dhash。计算方式基本差不多，下面以ahash为例说明如何计算：</p><p>（1）缩放图片。直接将原始图片缩小到8*8的尺寸，不用考虑原始图片的长宽比；</p><p>（2）灰度化。将8*8的图片灰度化，这样处理后，这张图片一共包含64个像素；</p><p>（3）计算平均值。计算这64个像素值的平均值；</p><p>（4）构建哈希。将64个像素值依次与（3）中的平均值进行比较，大于平均值为1，否则为0，这样从左到右、从上到下就可以组合得到一个64位的二进制数（顺序无所谓，只要保证都按这个顺序即可）。这个二进制串就是原始图片计算出来的ahash值，格式为110010001…001001，一共64位，将其转换成16进制，得到的格式为8f373714acfcf4d0。</p><p>如何比较相似度？很简单，计算两张图ahash值的汉明距离，如果距离（不相同的位数）超过10则认为两张原始图片不相似，小于10表示相似，具体阈值需要调整。这种方式非常简单并且很好实现，而且很凑效，尤其是在一堆图片中找相同图片的时候（缩放无所谓），非常适合用缩略图查找原始图。</p><p>三种感知哈希计算方式：<a href="https://github.com/JohannesBuchner/imagehash">https://github.com/JohannesBuchner/imagehash</a></p><p>三种感知哈希原理说明：<a href="http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html">http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html</a></p><p><a href="http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html">http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html</a></p><p>三种感知哈希区别和优劣势请参照网络。</p><p>这里的图像特征用64位二进制表示，每张图片的特征需要8个字节存储。</p><h3 id="基于区域的颜色直方图"><a href="#基于区域的颜色直方图" class="headerlink" title="基于区域的颜色直方图"></a>基于区域的颜色直方图</h3><p>颜色直方图统计整张图片中各种像素的占比情况，严格来讲它并不考虑颜色分布在图片中的位置。而基于区域的颜色直方图在计算像素占比时，先将整张图片划分成不同的子区域，然后依次计算子区域的颜色占比情况，最后考虑子区域合并之后的结果。这种方式弥补了之前忽略颜色分布位置的缺陷，通过直方图数据评价图片的相似度更加可靠。</p><p><img src="https://pic1.zhimg.com/80/v2-e15d1847cfdb8801fa8705ef29d0e920_1440w.jpg"></p><p>如上图，将原始图片划分成5个子区域，依次计算每个子区域的颜色直方图数据。图片是RGB三通道，每个通道依次分成（8，3，12）个区间，那么每个子区域颜色直方图就可以使用8<em>3</em>12=288维向量表示。而我们又将原始图片划分成了5个子区域，那么整张图的特征就需要用 8<em>3</em>12*5 = 1440维向量来表示了。</p><p>这里如何计算直方图特征距离呢？由于这里288维向量中每个分量代表对应区间像素出现的概率，那么距离计算就要用到前面提到过的卡方距离。通过卡方距离公式计算两个288维特征向量之间的距离，从而反推原始图片之间的相似度。<br>注意：使用颜色直方图的方式提取图像特征的前提是：接受“如果两张图片颜色分布差不多，则认为它们相似”这一假设，这一假设不考虑图片中具体包含内容，比如包含马、狗、人之类的目标，而只考虑颜色。事实上，经过实践，大部分时候该假设确实成立。</p><h2 id="如何提高查询速度"><a href="#如何提高查询速度" class="headerlink" title="如何提高查询速度"></a>如何提高查询速度</h2><p>给定一个图像特征，如何从一堆图像特征中快速找到与之相同、或者与之距离最近的N个、或者与之距离在M之内的所有特征呢？一个个的比较肯定不可接受，我们需要提前给这些特征创建索引，提高查找的速度。</p><p>VP-Tree是一种很好的数据结构，能够解决紧邻搜索的问题，这里是它的Python实现：<a href="https://github.com/RickardSjogren/vptree%EF%BC%8C%E5%AE%83%E8%83%BD%E5%A4%9F%E5%85%88%E7%94%A8%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA%E5%87%BA%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%A0%91%EF%BC%8C%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%E3%80%82%E5%85%B7%E4%BD%93%E5%8E%9F%E7%90%86%E8%AF%B7%E5%8F%82%E7%85%A7%E7%BD%91%E7%BB%9C%E3%80%82">https://github.com/RickardSjogren/vptree，它能够先用图像特征构建出一个二叉树，提高查询速度。具体原理请参照网络。</a></p><h2 id="感知哈希-基于区域的颜色直方图demo"><a href="#感知哈希-基于区域的颜色直方图demo" class="headerlink" title="感知哈希+基于区域的颜色直方图demo"></a>感知哈希+基于区域的颜色直方图demo</h2><p>最后有一个demo，基于图像感知哈希（ahash、phash以及dhash）和基于区域的颜色直方图，完成了一个简单的图片搜索demo。服务端采用Python、Flask开发。</p><p><img src="https://pic4.zhimg.com/80/v2-6d58a808676afee0a578885e8634247b_1440w.jpg"><br><img src="https://pic1.zhimg.com/80/v2-b7c5a3997b9e43253b3e5e1991fc7084_1440w.jpg"><br><img src="https://pic4.zhimg.com/80/v2-ac1fa5971e38f0f2c5d515d50df13c1b_1440w.jpg"></p><p>源码地址：<a href="https://github.com/sherlockchou86/cbir-image-search">https://github.com/sherlockchou86/cbir-image-search</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;图像搜索引擎一般有三种实现方式&quot;&gt;&lt;a href=&quot;#图像搜索引擎</summary>
      
    
    
    
    <category term="算法" scheme="http://posts.hufeifei.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="algorithm" scheme="http://posts.hufeifei.cn/tags/algorithm/"/>
    
    <category term="图像识别" scheme="http://posts.hufeifei.cn/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Docker 的多阶段构建</title>
    <link href="http://posts.hufeifei.cn/backend/multi-stage-build-for-docker/"/>
    <id>http://posts.hufeifei.cn/backend/multi-stage-build-for-docker/</id>
    <published>2022-01-02T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><code>Docker</code>的口号是 <strong>Build,Ship,and Run Any App,Anywhere</strong>，在我们使用 Docker 的大部分时候，的确能感觉到其优越性，但是往往在我们 Build 一个应用的时候，是将我们的源代码也构建进去的，这对于类似于 golang 这样的编译型语言肯定是不行的，因为实际运行的时候我只需要把最终构建的二进制包给你就行，把源码也一起打包在镜像中，需要承担很多风险，即使是脚本语言，在构建的时候也可能需要使用到一些上线的工具，这样无疑也增大了我们的镜像体积。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>比如我们现在有一个最简单的 golang 服务，需要构建一个最小的<code>Docker</code> 镜像，源码如下：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line">    <span class="string">&quot;net/http&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    router := gin.Default()</span><br><span class="line">    router.GET(<span class="string">&quot;/ping&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">        c.String(http.StatusOK, <span class="string">&quot;PONG&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    router.Run(<span class="string">&quot;:8080&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们最终的目的都是将最终的可执行文件放到一个最小的镜像(比如<code>alpine</code>)中去执行，怎样得到最终的编译好的文件呢？基于 <code>Docker</code> 的指导思想，我们需要在一个标准的容器中编译，比如在一个 Ubuntu 镜像中先安装编译的环境，然后编译，最后也在该容器中执行即可。</p><p>但是如果我们想把编译后的文件放置到 <code>alpine</code> 镜像中执行呢？我们就得通过上面的 Ubuntu 镜像将编译完成的文件通过 <code>volume</code> 挂载到我们的主机上，然后我们再将这个文件挂载到 <code>alpine</code> 镜像中去。</p><p>这种解决方案理论上肯定是可行的，但是这样的话在构建镜像的时候我们就得定义两步了，第一步是先用一个通用的镜像编译镜像，第二步是将编译后的文件复制到 <code>alpine</code> 镜像中执行，而且通用镜像编译后的文件在 <code>alpine</code> 镜像中不一定能执行。</p><p>定义编译阶段的 <code>Dockerfile</code>：(保存为<strong>Dockerfile.build</strong>)</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> golang</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /go/src/app</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /go/src/app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> go get -u -v github.com/kardianos/govendor</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> govendor sync</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> GOOS=linux GOARCH=386 go build -v -o /go/src/app/app-server</span></span><br></pre></td></tr></table></figure><p>定义<code>alpine</code>镜像：(保存为<strong>Dockerfile.old</strong>)</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> alpine:latest</span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk add -U tzdata</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ln -sf /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /root/</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> app-server .</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;./app-server&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>根据我们的执行步骤，我们还可以简单定义成一个脚本：(保存为<strong>build.sh</strong>)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo Building cnych/docker-multi-stage-demo:build</span><br><span class="line"></span><br><span class="line">docker build -t cnych/docker-multi-stage-demo:build . -f Dockerfile.build</span><br><span class="line"></span><br><span class="line">docker create --name extract cnych/docker-multi-stage-demo:build</span><br><span class="line">docker cp extract:/go/src/app/app-server ./app-server</span><br><span class="line">docker rm -f extract</span><br><span class="line"></span><br><span class="line">echo Building cnych/docker-multi-stage-demo:old</span><br><span class="line"></span><br><span class="line">docker build --no-cache -t cnych/docker-multi-stage-demo:old . -f Dockerfile.old</span><br><span class="line">rm ./app-server</span><br></pre></td></tr></table></figure><p>当我们执行完上面的构建脚本后，就实现了我们的目标。</p><h2 id="多阶段构建"><a href="#多阶段构建" class="headerlink" title="多阶段构建"></a>多阶段构建</h2><p>有没有一种更加简单的方式来实现上面的镜像构建过程呢？<strong>Docker 17.05</strong>版本以后，官方就提供了一个新的特性：<code>Multi-stage builds</code>（多阶段构建）。 使用多阶段构建，你可以在一个 <code>Dockerfile</code> 中使用多个 FROM 语句。每个 FROM 指令都可以使用不同的基础镜像，并表示开始一个新的构建阶段。你可以很方便的将一个阶段的文件复制到另外一个阶段，在最终的镜像中保留下你需要的内容即可。</p><p>我们可以调整前面一节的 <code>Dockerfile</code> 来使用多阶段构建：(保存为<strong>Dockerfile</strong>)</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> golang AS build-<span class="keyword">env</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /go/src/app</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /go/src/app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> go get -u -v github.com/kardianos/govendor</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> govendor sync</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> GOOS=linux GOARCH=386 go build -v -o /go/src/app/app-server</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> alpine</span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apk add -U tzdata</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ln -sf /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=build-env /go/src/app/app-server /usr/<span class="built_in">local</span>/bin/app-server</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8080</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [ <span class="string">&quot;app-server&quot;</span> ]</span></span><br></pre></td></tr></table></figure><p>现在我们只需要一个<code>Dockerfile</code>文件即可，也不需要拆分构建脚本了，只需要执行 build 命令即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build -t cnych/docker-multi-stage-demo:latest .</span></span><br></pre></td></tr></table></figure><p>默认情况下，构建阶段是没有命令的，我们可以通过它们的索引来引用它们，第一个 FROM 指令从<code>0</code>开始，我们也可以用<code>AS</code>指令为阶段命令，比如我们这里的将第一阶段命名为<code>build-env</code>，然后在其他阶段需要引用的时候使用<code>--from=build-env</code>参数即可。</p><p>最后我们简单的运行下该容器测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run --rm -p 8080:8080 cnych/docker-multi-stage-demo:latest</span></span><br></pre></td></tr></table></figure><p>运行成功后，我们可以在浏览器中打开<code>http://127.0.0.1:8080/ping</code>地址，可以看到<strong>PONG</strong>返回。</p><p>现在我们就把两个镜像的文件最终合并到一个镜像里面了。</p><p>文章中涉及到代码可以前往 github 查看：<a href="https://github.com/cnych/docker-multi-stage-demo">https://github.com/cnych/docker-multi-stage-demo</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://docs.docker.com/develop/develop-images/multistage-build">https://docs.docker.com/develop/develop-images/multistage-build</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt;的口号是 &lt;strong&gt;Build,</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Docker" scheme="http://posts.hufeifei.cn/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>使用Spring Boot创建docker image</title>
    <link href="http://posts.hufeifei.cn/backend/springboot-docker-image/"/>
    <id>http://posts.hufeifei.cn/backend/springboot-docker-image/</id>
    <published>2022-01-02T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在很久很久以前，我们是怎么创建Spring Boot的docker image呢？最最通用的办法就是将Spring boot的应用程序打包成一个fat jar，然后写一个docker file，将这个fat jar制作成为一个docker image然后运行。</p><p>今天我们来体验一下Spring Boot 2.3.3 带来的快速创建docker image的功能。</p><h2 id="传统做法和它的缺点"><a href="#传统做法和它的缺点" class="headerlink" title="传统做法和它的缺点"></a>传统做法和它的缺点</h2><p>现在我们创建一个非常简单的Spring Boot程序：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;/getInfo&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getInfo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;www.flydean.com&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认情况下，我们build出来的是一个fat jar：springboot-with-docker-0.0.1-SNAPSHOT.jar</p><p>我们解压看一下它的内容：</p><p><img src="https://img-blog.csdnimg.cn/20200828221049624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_0,text_aHR0cDovL3d3dy5mbHlkZWFuLmNvbQ==,size_25,color_8F8F8F,t_70"></p><p>Spring boot的fat jar分为三个部分，第一部分就是BOOT-INF, 里面的class目录放的是我们自己编写的class文件。而lib目录存放的是项目依赖的其他jar包。</p><p>第二部分是META-INF，里面定义了jar包的属性信息。</p><p>第三部分是Spring Boot的类加载器，fat jar包的启动是通过Spring Boot的jarLauncher来创建LaunchedURLClassLoader，通过它来加载lib下面的jar包，最后以一个新线程启动应用的Main函数。</p><p>这里不多讲Spring Boot的启动。</p><p>我们看一下，如果想要用这个fat jar来创建docker image应该怎么写：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM openjdk:8-jdk-alpine</span><br><span class="line">EXPOSE 8080</span><br><span class="line">ARG JAR_FILE=target/springboot-with-docker-0.0.1-SNAPSHOT.jar</span><br><span class="line">ADD $&#123;JAR_FILE&#125; app.jar</span><br><span class="line">ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]</span><br></pre></td></tr></table></figure><p>这样写有两个问题。</p><p>第一个问题：我们是用的far jar，在使用far jar的过程中会有一定的性能问题，肯定要比解压过后的性能要低，尤其是在容器环境中运行的情况下，可能会更加突出。</p><p>第二个问题：我们知道docker的image是按layer来构建的，按layer构建的好处就是可以减少image构建的时间和重用之前的layer。</p><p>但是如果使用的是fat jar包，即使我们只修改了我们自己的代码，也会导致整个fat jar重新更新，从而影响docker image的构建速度。</p><h2 id="使用Buildpacks"><a href="#使用Buildpacks" class="headerlink" title="使用Buildpacks"></a>使用Buildpacks</h2><p>传统的办法除了有上面的两个问题，还有一个就是需要自己构建docker file，有没有一键构建docker image的方法呢？</p><p>答案是肯定的。</p><p>Spring Boot在2.3.0之后，引入了Cloud Native 的buildpacks，通过这个工具，我们可以非常非常方便的创建docker image。</p><p>在Maven和Gradle中，Spring Boot引入了新的phase： spring-boot:build-image</p><p>我们可以直接运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn  spring-boot:build-image</span><br></pre></td></tr></table></figure><p>运行之，很不幸的是，你可能会遇到下面的错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:2.3.3.RELEASE:build-image (default-cli) on project springboot-with-docker: Execution default-cli of goal org.springframework.boot:spring-boot-maven-plugin:2.3.3.RELEASE:build-image failed: Docker API call to &#x27;localhost/v1.24/images/create?fromImage=gcr.io%2Fpaketo-buildpacks%2Fbuilder%3Abase-platform-api-0.3&#x27; failed with status code 500 &quot;Internal Server Error&quot; and message &quot;Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)&quot; -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p>这是因为我们无法从gcr.io中拉取镜像！</p><p>没关系，如果你会正确的上网方式的话，那么我估计你已经找到了一个代理。</p><p>将你的代理配置到Docker的代理项里面，我使用的是Docker desktop,下面是我的配置：</p><p><img src="https://img-blog.csdnimg.cn/20200828224337779.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_0,text_aHR0cDovL3d3dy5mbHlkZWFuLmNvbQ==,size_25,color_8F8F8F,t_70"></p><p>重新运行 mvn spring-boot:build-image</p><p>等待执行结果：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[INFO] --- spring-boot-maven-plugin:<span class="number">2.3</span><span class="number">.3</span>.RELEASE:build-image (<span class="keyword">default</span>-cli) @ springboot-with-docker ---</span><br><span class="line">[INFO] Building image <span class="string">&#x27;docker.io/library/springboot-with-docker:0.0.1-SNAPSHOT&#x27;</span></span><br><span class="line">[INFO] </span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br><span class="line">[INFO]  &gt; Pulling builder image <span class="string">&#x27;gcr.io/paketo-buildpacks/builder:base-platform-api-0.3&#x27;</span> <span class="number">0</span>%</span><br></pre></td></tr></table></figure><p>你可以看到，我们的确是需要从gcr.io拉取image。</p><h2 id="Layered-Jars"><a href="#Layered-Jars" class="headerlink" title="Layered Jars"></a>Layered Jars</h2><p>如果你不想使用Cloud Native Buildpacks，还是想使用传统的Dockerfile。 没关系，SpringBoot为我们提供了独特的分层jar包系统。</p><p>怎么开启呢？ 我们需要在POM文件中加上下面的配置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;layers&gt;</span><br><span class="line">                    &lt;enabled&gt;<span class="keyword">true</span>&lt;/enabled&gt;</span><br><span class="line">                &lt;/layers&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">&lt;/build&gt;</span><br></pre></td></tr></table></figure><p>再次打包，看下jar包的内容：</p><p><img src="https://img-blog.csdnimg.cn/20200828230028902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_0,text_aHR0cDovL3d3dy5mbHlkZWFuLmNvbQ==,size_25,color_8F8F8F,t_70"></p><p>看起来和之前的jar包没什么不同，只不过多了一个layers.idx 这个index文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- &quot;dependencies&quot;:</span><br><span class="line">  - &quot;BOOT-INF/lib/&quot;</span><br><span class="line">- &quot;spring-boot-loader&quot;:</span><br><span class="line">  - &quot;org/&quot;</span><br><span class="line">- &quot;snapshot-dependencies&quot;:</span><br><span class="line">- &quot;application&quot;:</span><br><span class="line">  - &quot;BOOT-INF/classes/&quot;</span><br><span class="line">  - &quot;BOOT-INF/classpath.idx&quot;</span><br><span class="line">  - &quot;BOOT-INF/layers.idx&quot;</span><br><span class="line">  - &quot;META-INF/&quot;</span><br></pre></td></tr></table></figure><p>index文件主要分为4个部分：</p><ul><li>dependencies - 非SNAPSHOT的依赖jar包</li><li>snapshot-dependencies - SNAPSHOT的依赖jar包</li><li>spring-boot-loader - Spring boot的class loader文件</li><li>application - 应用程序的class和resources文件</li></ul><blockquote><p>注意，这里的index文件是有顺序的，它和我们将要添加到docker image中的layer顺序是一致的。</p></blockquote><p>最少变化的将会最先添加到layer中，变动最大的放在最后面的layer。</p><p>我们可以使用layertools jarmode来对生成的fat jar进行校验或者解压缩：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">java -Djarmode=layertools -jar springboot-with-docker-0.0.1-SNAPSHOT.jar </span><br><span class="line">Usage:</span><br><span class="line">  java -Djarmode=layertools -jar springboot-with-docker-0.0.1-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  list     List layers from the jar that can be extracted</span><br><span class="line">  extract  Extracts layers from the jar for image creation</span><br><span class="line">  help     Help about any command</span><br></pre></td></tr></table></figure><p>使用list命令，我们可列出jar包中的layer信息。使用extract我们可以解压出不同的layer。</p><p>我们执行下extract命令，看下结果：</p><p><img src="https://img-blog.csdnimg.cn/20200828231134598.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_0,text_aHR0cDovL3d3dy5mbHlkZWFuLmNvbQ==,size_25,color_8F8F8F,t_70"></p><p>可以看到，我们根据layers.idx解压出了不同的文件夹。</p><p>我们看一下使用layer的dockerFile应该怎么写：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">FROM adoptopenjdk:11-jre-hotspot as builder</span><br><span class="line">WORKDIR application</span><br><span class="line">ARG JAR_FILE=target/*.jar</span><br><span class="line">COPY $&#123;JAR_FILE&#125; application.jar</span><br><span class="line">RUN java -Djarmode=layertools -jar application.jar extract</span><br><span class="line"></span><br><span class="line">FROM adoptopenjdk:11-jre-hotspot</span><br><span class="line">WORKDIR application</span><br><span class="line">COPY --from=builder application/dependencies/ ./</span><br><span class="line">COPY --from=builder application/spring-boot-loader/ ./</span><br><span class="line">COPY --from=builder application/snapshot-dependencies/ ./</span><br><span class="line">COPY --from=builder application/application/ ./</span><br><span class="line">ENTRYPOINT [&quot;java&quot;, &quot;org.springframework.boot.loader.JarLauncher&quot;]</span><br></pre></td></tr></table></figure><p>这样我们的一个分层的DockerImage就创建完成了。</p><h2 id="自定义Layer"><a href="#自定义Layer" class="headerlink" title="自定义Layer"></a>自定义Layer</h2><p>如果我们需要自定义Layer该怎么做呢？</p><p>我们可以创建一个独立的layers.xml文件：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">layers</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.springframework.org/schema/boot/layers&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://www.springframework.org/schema/boot/layers              https://www.springframework.org/schema/boot/layers/layers-2.3.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">application</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">into</span> <span class="attr">layer</span>=<span class="string">&quot;spring-boot-loader&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">include</span>&gt;</span>org/springframework/boot/loader/**<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">into</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">into</span> <span class="attr">layer</span>=<span class="string">&quot;application&quot;</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">application</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">into</span> <span class="attr">layer</span>=<span class="string">&quot;snapshot-dependencies&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">include</span>&gt;</span>*:*:*SNAPSHOT<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">into</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">into</span> <span class="attr">layer</span>=<span class="string">&quot;company-dependencies&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">include</span>&gt;</span>com.flydean:*<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">into</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">into</span> <span class="attr">layer</span>=<span class="string">&quot;dependencies&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">layerOrder</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layer</span>&gt;</span>dependencies<span class="tag">&lt;/<span class="name">layer</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layer</span>&gt;</span>spring-boot-loader<span class="tag">&lt;/<span class="name">layer</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layer</span>&gt;</span>snapshot-dependencies<span class="tag">&lt;/<span class="name">layer</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layer</span>&gt;</span>company-dependencies<span class="tag">&lt;/<span class="name">layer</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">layer</span>&gt;</span>application<span class="tag">&lt;/<span class="name">layer</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">layerOrder</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">layers</span>&gt;</span></span><br></pre></td></tr></table></figure><p>怎么使用这个layer.xml呢？</p><p>添加到build plugin中就可以了：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">layers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span>$&#123;project.basedir&#125;/src/main/resources/layers.xml<span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">layers</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>本文的例子：<a href="https://github.com/ddean2009/learn-springboot2/tree/master/springboot-with-docker">springboot-with-docker</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Docker" scheme="http://posts.hufeifei.cn/tags/Docker/"/>
    
    <category term="SpringBoot" scheme="http://posts.hufeifei.cn/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>JVM CPU Profiler技术原理及源码深度解析</title>
    <link href="http://posts.hufeifei.cn/backend/java-agent-profiler/"/>
    <id>http://posts.hufeifei.cn/backend/java-agent-profiler/</id>
    <published>2021-12-01T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>研发人员在遇到线上报警或需要优化系统性能时，常常需要分析程序运行行为和性能瓶颈。<a href="https://www.zhihu.com/search?q=Profiling%E6%8A%80%E6%9C%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">Profiling技术</a>是一种在应用运行时收集程序相关信息的动态分析手段，常用的JVM Profiler可以从多个方面对程序进行动态分析，如CPU、Memory、Thread、Classes、GC等，其中CPU Profiling的应用最为广泛。</p><p>CPU Profiling经常被用于分析代码的执行热点，如“哪个方法占用CPU的执行时间最长”、“每个方法占用CPU的比例是多少”等等，通过CPU Profiling得到上述相关信息后，研发人员就可以轻松针对热点瓶颈进行分析和性能优化，进而突破性能瓶颈，大幅提升系统的吞吐量。</p><h2 id="CPU-Profiler简介"><a href="#CPU-Profiler简介" class="headerlink" title="CPU Profiler简介"></a>CPU Profiler简介</h2><p>社区实现的JVM Profiler很多，比如已经商用且功能强大的<a href="https://www.ej-technologies.com/products/jprofiler/overview.html">JProfiler</a>，也有免费开源的产品，如<a href="https://github.com/uber-common/jvm-profiler">JVM-Profiler</a>，功能各有所长。我们日常使用的Intellij IDEA最新版内部也集成了一个简单好用的Profiler，详细的介绍参见<a href="https://blog.jetbrains.com/idea/2018/09/intellij-idea-2018-3-eap-git-submodules-jvm-profiler-macos-and-linux-and-more/">官方Blog</a>。</p><p>在用IDEA打开需要诊断的Java项目后，在“Preferences -&gt; Build, Execution, Deployment -&gt; Java Profiler”界面添加一个“CPU Profiler”，然后回到项目，单击右上角的“Run with Profiler”启动项目并开始CPU Profiling过程。一定时间后（推荐5min），在Profiler界面点击“Stop Profiling and Show Results”，即可看到Profiling的结果，包含火焰图和调用树，如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-ac27c0c0e77855d9c3cad0ad8c84dfd0_720w.jpg" alt="Intellij IDEA - 性能火焰图"></p><p><img src="https://pic3.zhimg.com/80/v2-e6e1a7bef2ff9a0aa49eaf2a9a0e8592_720w.jpg" alt="Intellij IDEA - 调用堆栈树"></p><p>火焰图是根据调用栈的样本集生成的可视化性能分析图，《<a href="http://www.ruanyifeng.com/blog/2017/09/flame-graph.html">如何读懂火焰图？</a>》一文对火焰图进行了不错的讲解，大家可以参考一下。简而言之，看火焰图时我们需要关注“平顶”，因为那里就是我们程序的CPU热点。调用树是另一种可视化分析的手段，与火焰图一样，也是根据同一份样本集而生成，按需选择即可。</p><p>这里要说明一下，因为我们没有在项目中引入任何依赖，仅仅是“Run with Profiler”，Profiler就能获取我们程序运行时的信息。这个功能其实是通过JVM Agent实现的，为了更好地帮助大家系统性的了解它，我们在这里先对JVM Agent做个简单的介绍。</p><h2 id="JVM-Agent简介"><a href="#JVM-Agent简介" class="headerlink" title="JVM Agent简介"></a>JVM Agent简介</h2><p>JVM Agent是一个按一定规则编写的特殊<a href="https://www.zhihu.com/search?q=%E7%A8%8B%E5%BA%8F%E5%BA%93&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">程序库</a>，可以在启动阶段通过命令行参数传递给JVM，<strong>作为一个伴生库与目标JVM运行在同一个进程中</strong>。在Agent中可以通过固定的接口获取JVM进程内的相关信息。Agent既可以是用C/C++/Rust编写的JVMTI Agent，也可以是用Java编写的Java Agent。</p><p>执行Java命令，我们可以看到Agent相关的命令行参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-agentlib:&lt;库名&gt;[=&lt;选项&gt;]</span><br><span class="line">              加载本机代理库 &lt;库名&gt;, 例如 -agentlib:jdwp</span><br><span class="line">              另请参阅 -agentlib:jdwp=help</span><br><span class="line">-agentpath:&lt;路径名&gt;[=&lt;选项&gt;]</span><br><span class="line">              按完整路径名加载本机代理库</span><br><span class="line">-javaagent:&lt;jar 路径&gt;[=&lt;选项&gt;]</span><br><span class="line">              加载 Java 编程语言代理, 请参阅 java.lang.instrument</span><br></pre></td></tr></table></figure><h2 id="JVMTI-Agent"><a href="#JVMTI-Agent" class="headerlink" title="JVMTI Agent"></a>JVMTI Agent</h2><p>JVMTI（JVM Tool Interface）是JVM提供的一套标准的C/C++编程接口，是实现Debugger、Profiler、Monitor、Thread Analyser等工具的统一基础，在主流Java虚拟机中都有实现。</p><p>当我们要基于JVMTI实现一个Agent时，需要实现如下<a href="https://www.zhihu.com/search?q=%E5%85%A5%E5%8F%A3%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">入口函数</a>：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// $JAVA_HOME/include/jvmti.h</span></span><br><span class="line"></span><br><span class="line"><span class="function">JNIEXPORT jint JNICALL <span class="title">Agent_OnLoad</span><span class="params">(JavaVM *vm, <span class="keyword">char</span> *options, <span class="keyword">void</span> *reserved)</span></span>;</span><br></pre></td></tr></table></figure><p>使用C/C++实现该函数，并将代码编译为动态连接库（Linux上是.so），通过-agentpath参数将库的完整路径传递给Java进程，JVM就会在<strong>启动阶段</strong>的合适时机执行该函数。在函数内部，我们可以通过JavaVM指针参数拿到JNI和JVMTI的<strong>函数指针表</strong>，这样我们就拥有了与JVM进行各种复杂交互的能力。</p><p>更多JVMTI相关的细节可以参考<a href="https://docs.oracle.com/en/java/javase/12/docs/specs/jvmti.html">官方文档</a>。</p><h2 id="Java-Agent"><a href="#Java-Agent" class="headerlink" title="Java Agent"></a>Java Agent</h2><p>在很多场景下，我们没有必要必须使用C/C++来开发JVMTI Agent，因为成本高且不易维护。JVM自身基于JVMTI封装了一套Java的<strong>Instrument API</strong>接口，允许使用Java语言开发Java Agent（只是一个jar包），大大降低了Agent的开发成本。社区开源的产品如<a href="https://github.com/oldmanpushcart/greys-anatomy">Greys</a>、<a href="https://github.com/alibaba/arthas">Arthas</a>、<a href="https://github.com/alibaba/jvm-sandbox">JVM-Sandbox</a>、<a href="https://github.com/uber-common/jvm-profiler">JVM-Profiler</a>等都是纯Java编写的，也是以Java Agent形式来运行。</p><p>在Java Agent中，我们需要在jar包的MANIFEST.MF中将<strong>Premain-Class</strong>指定为一个入口类，并在该入口类中实现如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">premain</span><span class="params">(String args, Instrumentation ins)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// implement</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样打包出来的jar就是一个Java Agent，可以通过-javaagent参数将jar传递给Java进程伴随启动，JVM同样会在启动阶段的合适时机执行该方法。</p><p>在该方法内部，参数<strong>Instrumentation</strong>接口提供了<strong>Retransform Classes</strong>的能力，我们利用该接口就可以<strong>对宿主进程的Class进行修改</strong>，实现方法耗时统计、故障注入、Trace等功能。Instrumentation接口提供的能力较为单一，仅与Class字节码操作相关，但由于我们现在已经处于<a href="https://www.zhihu.com/search?q=%E5%AE%BF%E4%B8%BB%E8%BF%9B%E7%A8%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">宿主进程</a>环境内，就可以<strong>利用JMX直接获取宿主进程的内存、线程、锁等信息</strong>。无论是Instrument API还是JMX，<strong>它们内部仍是统一基于JVMTI来实现</strong>。</p><p>更多Instrument API相关的细节可以参考<a href="https://docs.oracle.com/en/java/javase/12/docs/api/java.instrument/java/lang/instrument/package-summary.html">官方文档</a>。</p><h2 id="CPU-Profiler原理解析"><a href="#CPU-Profiler原理解析" class="headerlink" title="CPU Profiler原理解析"></a>CPU Profiler原理解析</h2><p>在了解完Profiler如何以Agent的形式执行后，我们可以开始尝试构造一个简单的CPU Profiler。但在此之前，还有必要了解下CPU Profiling技术的两种实现方式及其区别。</p><h3 id="Sampling-vs-Instrumentation"><a href="#Sampling-vs-Instrumentation" class="headerlink" title="Sampling vs Instrumentation"></a>Sampling vs Instrumentation</h3><p>使用过JProfiler的同学应该都知道，JProfiler的CPU Profiling功能提供了两种方式选项: <strong>Sampling</strong>和<strong>Instrumentation</strong>，它们也是实现CPU Profiler的两种手段。</p><p>Sampling方式顾名思义，基于对StackTrace的“采样”进行实现，核心原理如下：</p><ol><li>引入Profiler依赖，或直接利用Agent技术注入目标JVM进程并启动Profiler。</li><li>启动一个采样定时器，以固定的采样频率每隔一段时间（毫秒级）对所有线程的调用栈进行Dump。</li><li>汇总并统计每次调用栈的Dump结果，在一定时间内采到足够的样本后，导出统计结果，内容是每个方法被采样到的次数及方法的调用关系。</li></ol><p>Instrumentation则是利用Instrument API，对所有必要的Class进行字节码增强，在进入每个方法前进行埋点，方法执行结束后统计本次方法执行耗时，最终进行汇总。二者都能得到想要的结果，那么它们有什么区别呢？或者说，孰优孰劣？</p><p>Instrumentation方式对几乎所有方法添加了额外的AOP逻辑，<strong>这会导致对线上服务造成巨额的性能影响</strong>，但其优势是：<strong>绝对精准的方法调用次数、调用时间统计</strong>。</p><p>Sampling方式基于无侵入的额外线程对所有线程的调用栈快照进行固定频率抽样，相对前者来说它的<strong>性能开销很低</strong>。但由于它基于“采样”的模式，以及JVM固有的<strong>只能在安全点（Safe Point）进行采样的“缺陷”</strong>，会导致统计结果存在一定的偏差。譬如说：某些方法执行时间极短，但执行频率很高，真实占用了大量的CPU Time，但Sampling Profiler的采样周期不能无限调小，这会导致性能开销骤增，所以会导致大量的样本调用栈中并不存在刚才提到的”高频小方法“，进而导致最终结果无法反映真实的CPU热点。更多Sampling相关的问题可以参考《<a href="https://psy-lob-saw.blogspot.com/2016/02/why-most-sampling-java-profilers-are.html">Why (Most) Sampling Java Profilers Are Fucking Terrible</a>》。</p><p>具体到“孰优孰劣”的问题层面，这两种实现技术并没有非常明显的高下之判，只有在分场景讨论下才有意义。Sampling由于低开销的特性，更适合用在CPU密集型的应用中，以及不可接受大量性能开销的线上服务中。而Instrumentation则更适合用在I/O密集的应用中、对性能开销不敏感以及确实需要精确统计的场景中。社区的Profiler更多的是基于Sampling来实现，本文也是基于Sampling来进行讲解。</p><h3 id="基于Java-Agent-JMX实现"><a href="#基于Java-Agent-JMX实现" class="headerlink" title="基于Java Agent + JMX实现"></a>基于Java Agent + JMX实现</h3><p>一个最简单的Sampling CPU Profiler可以用Java Agent + JMX方式来实现。以Java Agent为入口，进入目标JVM进程后开启一个ScheduledExecutorService，定时利用JMX的threadMXBean.dumpAllThreads()来导出所有线程的StackTrace，最终汇总并导出即可。</p><p>Uber的<a href="https://github.com/uber-common/jvm-profiler">JVM-Profiler</a>实现原理也是如此，关键部分代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com/uber/profiling/profilers/StacktraceCollectorProfiler.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * StacktraceCollectorProfiler等同于文中所述CpuProfiler，仅命名偏好不同而已</span></span><br><span class="line"><span class="comment"> * jvm-profiler的CpuProfiler指代的是CpuLoad指标的Profiler</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现了Profiler接口，外部由统一的ScheduledExecutorService对所有Profiler定时执行</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">profile</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(<span class="keyword">false</span>, <span class="keyword">false</span>);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">for</span> (ThreadInfo threadInfo : threadInfos) &#123;</span><br><span class="line">        String threadName = threadInfo.getThreadName();</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        StackTraceElement[] stackTraceElements = threadInfo.getStackTrace();</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = stackTraceElements.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            StackTraceElement stackTraceElement = stackTraceElements[i];</span><br><span class="line">            <span class="comment">// ...</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Uber提供的定时器默认Interval是100ms，对于CPU Profiler来说，这略显粗糙。但由于dumpAllThreads()的执行开销不容小觑，Interval不宜设置的过小，所以该方法的CPU Profiling结果会存在不小的误差。</p><p><a href="https://github.com/uber-common/jvm-profiler">JVM-Profiler</a>的优点在于支持多种指标的Profiling（StackTrace、CPUBusy、Memory、I/O、Method），且支持将Profiling结果通过Kafka上报回中心Server进行分析，也即支持集群诊断。</p><h3 id="基于JVMTI-GetStackTrace实现"><a href="#基于JVMTI-GetStackTrace实现" class="headerlink" title="基于JVMTI + GetStackTrace实现"></a>基于JVMTI + GetStackTrace实现</h3><p>使用Java实现Profiler相对较简单，但也存在一些问题，譬如说Java Agent代码与业务代码共享AppClassLoader，被JVM直接加载的agent.jar如果引入了第三方依赖，可能会<strong>对业务Class造成污染</strong>。截止发稿时，JVM-Profiler都存在这个问题，它引入了Kafka-Client、http-Client、Jackson等组件，如果与业务代码中的组件版本发生冲突，可能会引发未知错误。Greys/Arthas/JVM-Sandbox的解决方式是分离入口与核心代码，使用定制的ClassLoader加载核心代码，避免影响业务代码。</p><p>在更底层的C/C++层面，我们可以直接对接JVMTI接口，使用原生C API对JVM进行操作，功能更丰富更强大，但开发效率偏低。基于上节同样的原理开发CPU Profiler，使用JVMTI需要进行如下这些步骤：</p><ol><li>编写Agent_OnLoad()，在入口通过JNI的JavaVM*指针的GetEnv()函数拿到JVMTI的jvmtiEnv指针：</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// agent.c</span></span><br><span class="line"></span><br><span class="line"><span class="function">JNIEXPORT jint JNICALL <span class="title">Agent_OnLoad</span><span class="params">(JavaVM *vm, <span class="keyword">char</span> *options, <span class="keyword">void</span> *reserved)</span> </span>&#123;</span><br><span class="line">    jvmtiEnv *jvmti;</span><br><span class="line">    (*vm)-&gt;GetEnv((<span class="keyword">void</span> **)&amp;jvmti, JVMTI_VERSION_1_0);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">return</span> JNI_OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>开启一个线程定时循环，定时使用jvmtiEnv指针配合调用如下几个<a href="https://www.zhihu.com/search?q=JVMTI%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">JVMTI函数</a>：</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取所有线程的jthread</span></span><br><span class="line"><span class="function">jvmtiError <span class="title">GetAllThreads</span><span class="params">(jvmtiEnv *env, jint *threads_count_ptr, jthread **threads_ptr)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据jthread获取该线程信息（name、daemon、priority...）</span></span><br><span class="line"><span class="function">jvmtiError <span class="title">GetThreadInfo</span><span class="params">(jvmtiEnv *env, jthread thread, jvmtiThreadInfo* info_ptr)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据jthread获取该线程调用栈</span></span><br><span class="line"><span class="function">jvmtiError <span class="title">GetStackTrace</span><span class="params">(jvmtiEnv *env,</span></span></span><br><span class="line"><span class="params"><span class="function">                         jthread thread,</span></span></span><br><span class="line"><span class="params"><span class="function">                         jint start_depth,</span></span></span><br><span class="line"><span class="params"><span class="function">                         jint max_frame_count,</span></span></span><br><span class="line"><span class="params"><span class="function">                         jvmtiFrameInfo *frame_buffer,</span></span></span><br><span class="line"><span class="params"><span class="function">                         jint *count_ptr)</span></span>;</span><br></pre></td></tr></table></figure><p>主逻辑大致是：首先调用GetAllThreads()获取所有线程的“句柄”jthread，然后遍历根据jthread调用GetThreadInfo()获取线程信息，按线程名过滤掉不需要的线程后，继续遍历根据jthread调用GetStackTrace()获取线程的调用栈。</p><ol start="3"><li>在Buffer中保存每一次的采样结果，最终生成必要的统计数据即可。</li></ol><p>按如上步骤即可实现基于JVMTI的CPU Profiler。但需要说明的是，即便是基于原生JVMTI接口使用GetStackTrace()的方式获取调用栈，也存在与JMX相同的问题——<strong>只能在安全点（Safe Point）进行采样</strong>。</p><h4 id="SafePoint-Bias问题"><a href="#SafePoint-Bias问题" class="headerlink" title="SafePoint Bias问题"></a>SafePoint Bias问题</h4><p>基于Sampling的CPU Profiler通过采集程序在不同时间点的调用栈样本来近似地推算出热点方法，因此，从理论上来讲Sampling CPU Profiler必须遵循以下两个原则：</p><ol><li>样本必须足够多。</li><li>程序中所有正在运行的代码点都必须以<strong>相同的概率</strong>被Profiler采样。</li></ol><p>如果只能在安全点采样，就违背了第二条原则。因为<strong>我们只能采集到位于安全点时刻的调用栈快照，意味着某些代码可能永远没有机会被采样，即使它真实耗费了大量的CPU执行时间</strong>，这种现象被称为“SafePoint Bias”。</p><p>上文我们提到，基于JMX与基于JVMTI的Profiler实现都存在SafePoint Bias，但一个值得了解的细节是：<strong>单独来说，JVMTI的GetStackTrace()函数并不需要在Caller的安全点执行，但当调用GetStackTrace()获取其他线程的调用栈时，必须等待，直到目标线程进入安全点；而且，GetStackTrace()仅能通过单独的线程同步定时调用，不能在UNIX信号处理器的Handler中被异步调用。综合来说，GetStackTrace()存在与JMX一样的SafePoint Bias</strong>。更多安全点相关的知识可以参考《<a href="https://psy-lob-saw.blogspot.com/2015/12/safepoints.html">Safepoints: Meaning, Side Effects and Overheads</a>》。</p><p>那么，如何避免SafePoint Bias？社区提供了一种Hack思路——AsyncGetCallTrace。</p><h3 id="基于JVMTI-AsyncGetCallTrace实现"><a href="#基于JVMTI-AsyncGetCallTrace实现" class="headerlink" title="基于JVMTI + AsyncGetCallTrace实现"></a>基于JVMTI + AsyncGetCallTrace实现</h3><p>如上节所述，假如我们拥有一个函数可以获取当前线程的调用栈且不受安全点干扰，另外它还支持在UNIX信号处理器中被异步调用，那么我们只需注册一个UNIX信号处理器，在Handler中调用该函数获取当前线程的调用栈即可。<strong>由于UNIX信号会被发送给进程的随机一线程进行处理</strong>，因此最终信号会均匀分布在所有线程上，也就均匀获取了所有线程的<a href="https://www.zhihu.com/search?q=%E8%B0%83%E7%94%A8%E6%A0%88%E6%A0%B7%E6%9C%AC&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">调用栈样本</a>。</p><p>OracleJDK/OpenJDK内部提供了这么一个函数——AsyncGetCallTrace，它的原型如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 栈帧</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line"> jint lineno;</span><br><span class="line"> jmethodID method_id;</span><br><span class="line">&#125; AGCT_CallFrame;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用栈</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    JNIEnv *env;</span><br><span class="line">    jint num_frames;</span><br><span class="line">    AGCT_CallFrame *frames;</span><br><span class="line">&#125; AGCT_CallTrace;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据ucontext将调用栈填充进trace指针</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">AsyncGetCallTrace</span><span class="params">(AGCT_CallTrace *trace, jint depth, <span class="keyword">void</span> *ucontext)</span></span>;</span><br></pre></td></tr></table></figure><p>通过原型可以看到，该函数的使用方式非常简洁，直接通过ucontext就能获取到完整的Java调用栈。</p><p>顾名思义，AsyncGetCallTrace是“async”的，不受安全点影响，这样的话采样就可能发生在任何时间，包括Native代码执行期间、GC期间等，在这时我们是无法获取Java调用栈的，AGCT_CallTrace的num_frames字段正常情况下标识了获取到的调用栈深度，但在如前所述的<strong>异常情况下它就表示为负数，最常见的-2代表此刻正在GC</strong>。</p><p>由于AsyncGetCallTrace非标准JVMTI函数，因此我们无法在jvmti.h中找到该函数声明，且由于其目标文件也早已链接进JVM二进制文件中，所以无法通过简单的声明来获取该函数的地址，这需要通过一些Trick方式来解决。简单说，Agent最终是作为动态链接库加载到目标JVM进程的地址空间中，因此可以在Agent_OnLoad内通过glibc提供的**dlsym()**函数拿到当前地址空间（即目标JVM进程地址空间）名为“AsyncGetCallTrace”的符号地址。这样就拿到了该函数的指针，按照上述原型进行类型转换后，就可以正常调用了。</p><p>通过AsyncGetCallTrace实现CPU Profiler的大致流程：</p><ol><li>编写Agent_OnLoad()，在入口拿到jvmtiEnv和AsyncGetCallTrace指针，获取AsyncGetCallTrace方式如下:</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">void</span> <span class="params">(*AsyncGetCallTrace)</span><span class="params">(AGCT_CallTrace *traces, jint depth, <span class="keyword">void</span> *ucontext)</span></span>;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">AsyncGetCallTrace agct_ptr = (AsyncGetCallTrace)dlsym(RTLD_DEFAULT, <span class="string">&quot;AsyncGetCallTrace&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (agct_ptr == <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="keyword">void</span> *libjvm = dlopen(<span class="string">&quot;libjvm.so&quot;</span>, RTLD_NOW);</span><br><span class="line">    <span class="keyword">if</span> (!libjvm) &#123;</span><br><span class="line">        <span class="comment">// 处理dlerror()...</span></span><br><span class="line">    &#125;</span><br><span class="line">    agct_ptr = (AsyncGetCallTrace)dlsym(libjvm, <span class="string">&quot;AsyncGetCallTrace&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>在OnLoad阶段，我们还需要做一件事，即注册OnClassLoad和OnClassPrepare这两个Hook，<strong>原因是jmethodID是延迟分配的，使用AGCT获取Traces依赖预先分配好的数据。我们在OnClassPrepare的CallBack中尝试获取该Class的所有Methods，这样就使JVMTI提前分配了所有方法的jmethodID</strong>，如下所示：</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> JNICALL <span class="title">OnClassLoad</span><span class="params">(jvmtiEnv *jvmti, JNIEnv* jni, jthread thread, jclass klass)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> JNICALL <span class="title">OnClassPrepare</span><span class="params">(jvmtiEnv *jvmti, JNIEnv *jni, jthread thread, jclass klass)</span> </span>&#123;</span><br><span class="line">    jint method_count;</span><br><span class="line">    jmethodID *methods;</span><br><span class="line">    jvmti-&gt;GetClassMethods(klass, &amp;method_count, &amp;methods);</span><br><span class="line">    <span class="keyword">delete</span> [] methods;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">jvmtiEventCallbacks callbacks = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">callbacks.ClassLoad = OnClassLoad;</span><br><span class="line">callbacks.ClassPrepare = OnClassPrepare;</span><br><span class="line">jvmti-&gt;SetEventCallbacks(&amp;callbacks, <span class="keyword">sizeof</span>(callbacks));</span><br><span class="line">jvmti-&gt;SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_CLASS_LOAD, <span class="literal">NULL</span>);</span><br><span class="line">jvmti-&gt;SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_CLASS_PREPARE, <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure><ol start="3"><li>利用SIGPROF信号来进行定时采样：</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这里信号handler传进来的的ucontext即AsyncGetCallTrace需要的ucontext</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">signal_handler</span><span class="params">(<span class="keyword">int</span> signo, <span class="keyword">siginfo_t</span> *siginfo, <span class="keyword">void</span> *ucontext)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用AsyncCallTrace进行采样，注意处理num_frames为负的异常情况</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册SIGPROF信号的handler</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sigaction</span> <span class="title">sa</span>;</span></span><br><span class="line">sigemptyset(&amp;sa.sa_mask);</span><br><span class="line">sa.sa_sigaction = signal_handler;</span><br><span class="line">sa.sa_flags = SA_RESTART | SA_SIGINFO;</span><br><span class="line">sigaction(SIGPROF, &amp;sa, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定时产生SIGPROF信号</span></span><br><span class="line"><span class="comment">// interval是nanoseconds表示的采样间隔，AsyncGetCallTrace相对于同步采样来说可以适当高频一些</span></span><br><span class="line"><span class="keyword">long</span> sec = interval / <span class="number">1000000000</span>;</span><br><span class="line"><span class="keyword">long</span> usec = (interval % <span class="number">1000000000</span>) / <span class="number">1000</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">itimerval</span> <span class="title">tv</span> =</span> &#123;&#123;sec, usec&#125;, &#123;sec, usec&#125;&#125;;</span><br><span class="line">setitimer(ITIMER_PROF, &amp;tv, <span class="literal">NULL</span>);</span><br></pre></td></tr></table></figure><ol start="4"><li>在Buffer中保存每一次的采样结果，最终生成必要的统计数据即可。</li></ol><p>按如上步骤即可实现基于AsyncGetCallTrace的CPU Profiler，这是社区中目前性能开销最低、相对效率最高的CPU Profiler实现方式，在Linux环境下结合perf_events还能做到同时采样Java栈与Native栈，也就能同时分析Native代码中存在的性能热点。该方式的典型开源实现有<a href="https://github.com/jvm-profiling-tools/async-profiler">Async-Profiler</a>和<a href="https://github.com/jvm-profiling-tools/honest-profiler">Honest-Profiler</a>，Async-Profiler实现质量较高，感兴趣的话建议大家阅读参考源码。有趣的是，IntelliJ IDEA内置的Java Profiler，其实就是Async-Profiler的包装。更多关于AsyncGetCallTrace的内容，大家可以参考《<a href="http://psy-lob-saw.blogspot.com/2016/06/the-pros-and-cons-of-agct.html">The Pros and Cons of AsyncGetCallTrace Profilers</a>》。</p><h2 id="生成性能火焰图"><a href="#生成性能火焰图" class="headerlink" title="生成性能火焰图"></a>生成性能火焰图</h2><p>现在我们拥有了采样调用栈的能力，但是调用栈样本集是以二维数组的数据结构形式存在于内存中的，如何将其转换为可视化的火焰图呢？</p><p>火焰图通常是一个<a href="https://www.zhihu.com/search?q=svg%E6%96%87%E4%BB%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">svg文件</a>，部分优秀项目可以根据文本文件自动生成火焰图文件，仅对文本文件的格式有一定要求。<a href="https://github.com/brendangregg/FlameGraph">FlameGraph</a>项目的核心只是一个Perl脚本，可以根据我们提供的调用栈文本生成相应的火焰图svg文件。调用栈的文本格式相当简单，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">base_func;func1;func2;func3 10</span><br><span class="line">base_func;funca;funcb 15</span><br></pre></td></tr></table></figure><p>将我们采样到的调用栈样本集进行整合后，需输出如上所示的文本格式。每一行代表一“类“调用栈，空格左边是调用栈的方法名排列，以分号分割，左栈底右栈顶，空格右边是该样本出现的次数。</p><p>将样本文件交给flamegraph.pl脚本执行，就能输出相应的火焰图了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ flamegraph.pl stacktraces.txt &gt; stacktraces.svg</span><br></pre></td></tr></table></figure><p>效果如下图所示：</p><p><img src="https://pic3.zhimg.com/80/v2-64a07d373beb9eb31d7accd933331e0a_720w.jpg" alt="img"></p><p>通过flamegraph.pl生成的火焰图</p><h2 id="HotSpot的Dynamic-Attach机制解析"><a href="#HotSpot的Dynamic-Attach机制解析" class="headerlink" title="HotSpot的Dynamic Attach机制解析"></a>HotSpot的Dynamic Attach机制解析</h2><p>到目前为止，我们已经了解了CPU Profiler完整的工作原理，然而使用过JProfiler/Arthas的同学可能会有疑问，很多情况下可以直接对线上运行中的服务进行Profling，并不需要在Java进程的启动参数添加<a href="https://www.zhihu.com/search?q=Agent%E5%8F%82%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">Agent参数</a>，这是通过什么手段做到的？答案是<strong>Dynamic Attach</strong>。</p><p>JDK在1.6以后提供了Attach API，允许向运行中的JVM进程添加Agent，这项手段被广泛使用在各种Profiler和字节码增强工具中，其官方简介如下：</p><blockquote><p>This is a Sun extension that allows a tool to ‘attach’ to another process running Java code and launch a JVM TI agent or a java.lang.instrument agent in that process.</p></blockquote><p>总的来说，Dynamic Attach是HotSpot提供的一种特殊能力，它允许一个进程向另一个运行中的JVM进程发送一些命令并执行，命令并不限于加载Agent，还包括Dump内存、<a href="https://www.zhihu.com/search?q=Dump%E7%BA%BF%E7%A8%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">Dump线程</a>等等。</p><h3 id="通过sun-tools进行Attach"><a href="#通过sun-tools进行Attach" class="headerlink" title="通过sun.tools进行Attach"></a>通过sun.tools进行Attach</h3><p>Attach虽然是HotSpot提供的能力，但JDK在Java层面也对其做了封装。</p><p>前文已经提到，对于Java Agent来说，PreMain方法在Agent作为启动参数运行的时候执行，其实我们还可以额外实现一个AgentMain方法，并在MANIFEST.MF中将<strong>Agent-Class</strong>指定为该Class：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">agentmain</span><span class="params">(String args, Instrumentation ins)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// implement</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样打包出来的jar，既可以作为-javaagent参数启动，也可以被Attach到运行中的目标JVM进程。JDK已经封装了简单的API让我们直接Attach一个Java Agent，下面以Arthas中的代码进行演示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com/taobao/arthas/core/Arthas.java</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.sun.tools.attach.VirtualMachine;</span><br><span class="line"><span class="keyword">import</span> com.sun.tools.attach.VirtualMachineDescriptor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">attachAgent</span><span class="params">(Configure configure)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    VirtualMachineDescriptor virtualMachineDescriptor = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到所有JVM进程，找出目标进程</span></span><br><span class="line">    <span class="keyword">for</span> (VirtualMachineDescriptor descriptor : VirtualMachine.list()) &#123;</span><br><span class="line">        String pid = descriptor.id();</span><br><span class="line">        <span class="keyword">if</span> (pid.equals(Integer.toString(configure.getJavaPid()))) &#123;</span><br><span class="line">            virtualMachineDescriptor = descriptor;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    VirtualMachine virtualMachine = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 针对某个JVM进程调用VirtualMachine.attach()方法，拿到VirtualMachine实例</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == virtualMachineDescriptor) &#123;</span><br><span class="line">            virtualMachine = VirtualMachine.attach(<span class="string">&quot;&quot;</span> + configure.getJavaPid());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            virtualMachine = VirtualMachine.attach(virtualMachineDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用VirtualMachine#loadAgent()，将arthasAgentPath指定的jar attach到目标JVM进程中</span></span><br><span class="line">        <span class="comment">// 第二个参数为attach参数，即agentmain的首个String参数args</span></span><br><span class="line">        virtualMachine.loadAgent(arthasAgentPath, configure.getArthasCore() + <span class="string">&quot;;&quot;</span> + configure.toString());</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != virtualMachine) &#123;</span><br><span class="line">            <span class="comment">// 调用VirtualMachine#detach()释放</span></span><br><span class="line">            virtualMachine.detach();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="直接对HotSpot进行Attach"><a href="#直接对HotSpot进行Attach" class="headerlink" title="直接对HotSpot进行Attach"></a>直接对HotSpot进行Attach</h3><p>sun.tools封装的API足够简单易用，但只能使用Java编写，也只能用在Java Agent上，因此有些时候我们必须手工对JVM进程直接进行Attach。对于JVMTI，除了Agent_OnLoad()之外，我们还需实现一个Agent_OnAttach()函数，当将JVMTI Agent Attach到目标进程时，从该函数开始执行：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// $JAVA_HOME/include/jvmti.h</span></span><br><span class="line"></span><br><span class="line"><span class="function">JNIEXPORT jint JNICALL <span class="title">Agent_OnAttach</span><span class="params">(JavaVM *vm, <span class="keyword">char</span> *options, <span class="keyword">void</span> *reserved)</span></span>;</span><br></pre></td></tr></table></figure><p>下面我们以Async-Profiler中的jattach源码为线索，探究一下如何利用Attach机制给运行中的JVM进程发送命令。jattach是Async-Profiler提供的一个Driver，使用方式比较直观：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Usage:</span><br><span class="line">    jattach &lt;pid&gt; &lt;cmd&gt; [args ...]</span><br><span class="line">Args:</span><br><span class="line">    &lt;pid&gt;  目标JVM进程的进程ID</span><br><span class="line">    &lt;cmd&gt;  要执行的命令</span><br><span class="line">    &lt;args&gt; 命令参数</span><br></pre></td></tr></table></figure><p>使用方式如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jattach 1234 load /absolute/path/to/agent/libagent.so <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>执行上述命令，libagent.so就被加载到ID为1234的JVM进程中并开始执行Agent_OnAttach函数了。有一点需要注意，<strong>执行Attach的进程euid及egid，与被Attach的目标JVM进程必须相同</strong>。接下来开始分析<a href="https://www.zhihu.com/search?q=jattach%E6%BA%90%E7%A0%81&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">jattach源码</a>。</p><p>如下所示的Main函数描述了一次Attach的整体流程：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// async-profiler/src/jattach/jattach.c</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 解析命令行参数</span></span><br><span class="line">    <span class="comment">// 检查euid与egid</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!check_socket(nspid) &amp;&amp; !start_attach_mechanism(pid, nspid)) &#123;</span><br><span class="line">        perror(<span class="string">&quot;Could not start attach mechanism&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> fd = connect_socket(nspid);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>) &#123;</span><br><span class="line">        perror(<span class="string">&quot;Could not connect to socket&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Connected to remote JVM\n&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!write_command(fd, argc - <span class="number">2</span>, argv + <span class="number">2</span>)) &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error writing to socket&quot;</span>);</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Response code = &quot;</span>);</span><br><span class="line">    fflush(<span class="built_in">stdout</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> result = read_response(fd);</span><br><span class="line">    close(fd);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>忽略掉命令行参数解析与检查euid和egid的过程。jattach首先调用了check_socket函数进行了“socket检查？”，check_socket源码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// async-profiler/src/jattach/jattach.c</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Check if remote JVM has already opened socket for Dynamic Attach</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">check_socket</span><span class="params">(<span class="keyword">int</span> pid)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> path[MAX_PATH];</span><br><span class="line">    <span class="built_in">snprintf</span>(path, MAX_PATH, <span class="string">&quot;%s/.java_pid%d&quot;</span>, get_temp_directory(), pid); <span class="comment">// get_temp_directory()在Linux下固定返回&quot;/tmp&quot;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">stat</span> <span class="title">stats</span>;</span></span><br><span class="line">    <span class="keyword">return</span> stat(path, &amp;stats) == <span class="number">0</span> &amp;&amp; S_ISSOCK(stats.st_mode);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们知道，UNIX操作系统提供了一种基于文件的Socket接口，称为“UNIX Socket”（一种常用的<a href="https://www.zhihu.com/search?q=%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">进程间通信</a>方式）。在该函数中使用S_ISSOCK宏来判断该文件是否被绑定到了UNIX Socket，如此看来，“/tmp/**.java_pid<pid>**”文件很有可能就是<a href="https://www.zhihu.com/search?q=%E5%A4%96%E9%83%A8%E8%BF%9B%E7%A8%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">外部进程</a>与JVM进程间通信的桥梁。</p><p>查阅官方文档，得到如下描述：</p><blockquote><p>The attach listener thread then communicates with the source JVM in an OS dependent manner:</p></blockquote><ul><li>On Solaris, the Doors IPC mechanism is used. The door is attached to a file in the file system so that clients can access it.</li><li>On Linux, a Unix domain socket is used. This socket is bound to a file in the filesystem so that clients can access it.</li><li>On Windows, the created thread is given the name of a pipe which is served by the client. The result of the operations are written to this pipe by the target JVM.</li></ul><p>证明了我们的猜想是正确的。目前为止check_socket函数的作用很容易理解了：<strong>判断外部进程与目标JVM进程之间是否已经建立了UNIX Socket连接</strong>。</p><p>回到Main函数，在使用check_socket确定连接尚未建立后，紧接着调用start_attach_mechanism函数，函数名很直观地描述了它的作用，源码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// async-profiler/src/jattach/jattach.c</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Force remote JVM to start Attach listener.</span></span><br><span class="line"><span class="comment">// HotSpot will start Attach listener in response to SIGQUIT if it sees .attach_pid file</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">start_attach_mechanism</span><span class="params">(<span class="keyword">int</span> pid, <span class="keyword">int</span> nspid)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> path[MAX_PATH];</span><br><span class="line">    <span class="built_in">snprintf</span>(path, MAX_PATH, <span class="string">&quot;/proc/%d/cwd/.attach_pid%d&quot;</span>, nspid, nspid);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> fd = creat(path, <span class="number">0660</span>);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span> || (close(fd) == <span class="number">0</span> &amp;&amp; !check_file_owner(path))) &#123;</span><br><span class="line">        <span class="comment">// Failed to create attach trigger in current directory. Retry in /tmp</span></span><br><span class="line">        <span class="built_in">snprintf</span>(path, MAX_PATH, <span class="string">&quot;%s/.attach_pid%d&quot;</span>, get_temp_directory(), nspid);</span><br><span class="line">        fd = creat(path, <span class="number">0660</span>);</span><br><span class="line">        <span class="keyword">if</span> (fd == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        close(fd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We have to still use the host namespace pid here for the kill() call</span></span><br><span class="line">    kill(pid, SIGQUIT);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Start with 20 ms sleep and increment delay each iteration</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timespec</span> <span class="title">ts</span> =</span> &#123;<span class="number">0</span>, <span class="number">20000000</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        nanosleep(&amp;ts, <span class="literal">NULL</span>);</span><br><span class="line">        result = check_socket(nspid);</span><br><span class="line">    &#125; <span class="keyword">while</span> (!result &amp;&amp; (ts.tv_nsec += <span class="number">20000000</span>) &lt; <span class="number">300000000</span>);</span><br><span class="line"></span><br><span class="line">    unlink(path);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>start_attach_mechanism函数首先创建了一个名为“/tmp/<strong>.attach_pid<pid><strong>”的空文件，然后向目标JVM进程发送了一个</strong>SIGQUIT</strong>信号，这个信号似乎触发了JVM的某种机制？紧接着，start_attach_mechanism函数开始陷入了一种等待，每20ms调用一次check_socket函数检查连接是否被建立，如果等了300ms还没有成功就放弃。函数的最后调用Unlink删掉.attach_pid文件并返回。</p><p>如此看来，HotSpot似乎提供了一种特殊的机制，只要给它发送一个SIGQUIT信号，并预先准备好.attach_pid文件，HotSpot会主动创建一个地址为“/tmp/.java_pid”的UNIX Socket，接下来主动Connect这个地址即可建立连接执行命令。</p><p>查阅文档，得到如下描述：</p><blockquote><p>Dynamic attach has an attach listener thread in the target JVM. This is a thread that is started when the first attach request occurs. On Linux and Solaris, the client creates a file named .attach_pid(pid) and sends a SIGQUIT to the target JVM process. The existence of this file causes the SIGQUIT handler in HotSpot to start the attach listener thread. On Windows, the client uses the Win32 CreateRemoteThread function to create a new thread in the target process.</p></blockquote><p>这样一来就很明确了，<strong>在Linux上我们只需创建一个“/tmp/.attach_pid”文件，并向目标JVM进程发送一个SIGQUIT信号，HotSpot就会开始监听“/tmp/.java_pid”地址上的UNIX Socket，接收并执行相关Attach的命令</strong>。至于为什么一定要创建.attach_pid文件才可以触发Attach Listener的创建，经查阅资料，我们得到了两种说法：一是JVM不止接收从外部Attach进程发送的SIGQUIT信号，必须配合外部进程创建的外部文件才能确定这是一次Attach请求；二是为了安全。</p><p>继续看jattach的源码，果不其然，它调用了connect_socket函数对“/tmp/.java_pid”进行连接，connect_socket源码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// async-profiler/src/jattach/jattach.c</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Connect to UNIX domain socket created by JVM for Dynamic Attach</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">connect_socket</span><span class="params">(<span class="keyword">int</span> pid)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd = socket(PF_UNIX, SOCK_STREAM, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_un</span> <span class="title">addr</span>;</span></span><br><span class="line">    addr.sun_family = AF_UNIX;</span><br><span class="line">    <span class="built_in">snprintf</span>(addr.sun_path, <span class="keyword">sizeof</span>(addr.sun_path), <span class="string">&quot;%s/.java_pid%d&quot;</span>, get_temp_directory(), pid);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (connect(fd, (struct sockaddr*)&amp;addr, <span class="keyword">sizeof</span>(addr)) == <span class="number">-1</span>) &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一个很普通的Socket创建函数，返回Socket文件描述符。</p><p>回到Main函数，主流程紧接着调用write_command函数向该Socket写入了从命令行传进来的参数，并且调用read_response函数接收从目标JVM进程返回的数据。两个很常见的Socket读写函数，源码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// async-profiler/src/jattach/jattach.c</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Send command with arguments to socket</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">write_command</span><span class="params">(<span class="keyword">int</span> fd, <span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Protocol version</span></span><br><span class="line">    <span class="keyword">if</span> (write(fd, <span class="string">&quot;1&quot;</span>, <span class="number">2</span>) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">char</span>* arg = i &lt; argc ? argv[i] : <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">if</span> (write(fd, arg, <span class="built_in">strlen</span>(arg) + <span class="number">1</span>) &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mirror response from remote JVM to stdout</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">read_response</span><span class="params">(<span class="keyword">int</span> fd)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buf[<span class="number">8192</span>];</span><br><span class="line">    <span class="keyword">ssize_t</span> bytes = read(fd, buf, <span class="keyword">sizeof</span>(buf) - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (bytes &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">&quot;Error reading response&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First line of response is the command result code</span></span><br><span class="line">    buf[bytes] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> result = atoi(buf);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        fwrite(buf, <span class="number">1</span>, bytes, <span class="built_in">stdout</span>);</span><br><span class="line">        bytes = read(fd, buf, <span class="keyword">sizeof</span>(buf));</span><br><span class="line">    &#125; <span class="keyword">while</span> (bytes &gt; <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>浏览write_command函数就可知外部进程与目标JVM进程之间发送的数据格式相当简单，基本如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;PROTOCOL VERSION&gt;\0&lt;COMMAND&gt;\0&lt;ARG1&gt;\0&lt;ARG2&gt;\0&lt;ARG3&gt;\0</span><br></pre></td></tr></table></figure><p>以先前我们使用的Load命令为例，发送给HotSpot时格式如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1\0load\0/absolute/path/to/agent/libagent.so\0true\0\0</span><br></pre></td></tr></table></figure><p>至此，我们已经了解了如何手工对JVM进程直接进行Attach。</p><h3 id="Attach补充介绍"><a href="#Attach补充介绍" class="headerlink" title="Attach补充介绍"></a>Attach补充介绍</h3><p>Load命令仅仅是HotSpot所支持的诸多命令中的一种，用于动态加载基于JVMTI的Agent，完整的命令表如下所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> AttachOperationFunctionInfo funcs[] = &#123;</span><br><span class="line">  &#123; <span class="string">&quot;agentProperties&quot;</span>,  get_agent_properties &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;datadump&quot;</span>,         data_dump &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;dumpheap&quot;</span>,         dump_heap &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;load&quot;</span>,             JvmtiExport::load_agent_library &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;properties&quot;</span>,       get_system_properties &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;threaddump&quot;</span>,       thread_dump &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;inspectheap&quot;</span>,      heap_inspection &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;setflag&quot;</span>,          set_flag &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;printflag&quot;</span>,        print_flag &#125;,</span><br><span class="line">  &#123; <span class="string">&quot;jcmd&quot;</span>,             jcmd &#125;,</span><br><span class="line">  &#123; <span class="literal">NULL</span>,               <span class="literal">NULL</span> &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>读者可以尝试下<a href="https://www.zhihu.com/search?q=threaddump%E5%91%BD%E4%BB%A4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:86545748%7D">threaddump命令</a>，然后对相同的进程进行jstack，对比观察输出，其实是完全相同的，其它命令大家可以自行进行探索。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，善用各类Profiler是提升性能优化效率的一把利器，了解Profiler本身的实现原理更能帮助我们避免对工具的各种误用。CPU Profiler所依赖的Attach、JVMTI、Instrumentation、JMX等皆是JVM平台比较通用的技术，在此基础上，我们去实现Memory Profiler、Thread Profiler、GC Analyzer等工具也没有想象中那么神秘和复杂了。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://docs.oracle.com/en/java/javase/12/docs/specs/jvmti.html">JVM Tool Interface</a></li><li><a href="https://psy-lob-saw.blogspot.com/2016/06/the-pros-and-cons-of-agct.html">The Pros and Cons of AsyncGetCallTrace Profilers</a></li><li><a href="https://psy-lob-saw.blogspot.com/2016/02/why-most-sampling-java-profilers-are.html">Why (Most) Sampling Java Profilers Are Fucking Terrible</a></li><li><a href="https://psy-lob-saw.blogspot.com/2015/12/safepoints.html">Safepoints: Meaning, Side Effects and Overheads</a></li><li><a href="http://openjdk.java.net/groups/hotspot/docs/Serviceability.html">Serviceability in HotSpot</a></li><li><a href="http://www.ruanyifeng.com/blog/2017/09/flame-graph.html">如何读懂火焰图？</a></li><li><a href="https://blog.jetbrains.com/idea/2018/09/intellij-idea-2018-3-eap-git-submodules-jvm-profiler-macos-and-linux-and-more/">IntelliJ IDEA 2018.3 EAP: Git Submodules, JVM Profiler (macOS and Linux) and more</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Java" scheme="http://posts.hufeifei.cn/tags/Java/"/>
    
    <category term="FlameGraph" scheme="http://posts.hufeifei.cn/tags/FlameGraph/"/>
    
  </entry>
  
  <entry>
    <title>谈阿里核心业务监控平台 SunFire 的技术架构</title>
    <link href="http://posts.hufeifei.cn/backend/alimonitor-sunfire-architecture/"/>
    <id>http://posts.hufeifei.cn/backend/alimonitor-sunfire-architecture/</id>
    <published>2021-11-22T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>在 2016 年双 11 全球购物狂欢节中，天猫全天交易额 1207 亿元，前 30 分钟每秒交易峰值 17.5 万笔，每秒支付峰值 12 万笔。承载这些秒级数据背后的监控产品是如何实现的呢？接下来本文将从阿里监控体系、监控产品、监控技术架构及实现分别进行详细讲述。</p><p>阿里有众多监控产品，且各产品分工明确，百花齐放。整个阿里监控体系如下图：</p><p><img src="https://static001.infoq.cn/resource/image/bb/2b/bb8fd397d31beaa3d0bc0d16f519902b.jpg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>集团层面的监控，以平台为主，全部为阿里自主研发（除引入了第三方基调、博睿等外部检测系统，用于各地 CDN 用户体验监控），这些监控平台覆盖了阿里集团 80% 的监控需求。</p><p>此外，每个事业群均根据自身特性自主研发了多套监控系统，以满足自身特定业务场景的监控需求，如广告的 GoldenEye、菜鸟的棱镜、阿里云的天基、蚂蚁的金融云（基于 XFlush）、中间件的 EagleEye 等，这些监控系统均有各自的使用场景。</p><p>阿里的监控规模早已达到了千万量级的监控项，PB 级的监控数据，亿级的报警通知，基于数据挖掘、机器学习等技术的智能化监控将会越来越重要。</p><p>阿里全球运行指挥中心（GOC）基于历史监控数据，通过异常检测、异常标注、挖掘训练、机器学习、故障模拟等方式，进行业务故障的自动化定位，并赋能监控中心 7*24 小时专业监控值班人员，使阿里集团具备第一时间发现业务指标异常，并快速进行应急响应、故障恢复的能力，将故障对线上业务的影响降到最低。</p><p>接下来将详细讲述本文的主角，承载阿里核心业务监控的 SunFire 监控平台。</p><h2 id="SunFire-简介"><a href="#SunFire-简介" class="headerlink" title="SunFire 简介"></a>SunFire 简介</h2><p>SunFire 是一整套海量日志实时分析解决方案，以日志、REST 接口、Shell 脚本等作为数据采集来源，提供设备、应用、业务等各种视角的监控能力，从而帮您快速发现问题、定位问题、分析问题、解决问题，为线上系统可用率提供有效保障。</p><p>SunFire 利用文件传输、流式计算、分布式文件存储、数据可视化、数据建模等技术，提供实时、智能、可定制、多视角、全方位的监控体系。其主要优势有：</p><ul><li>全方位实时监控：提供设备、应用、业务等各种视角的监控能力，关键指标秒级、普通指标分钟级，高可靠、高时效、低延迟。</li><li>灵活的报警规则：可根据业务特征、时间段、重要程度等维度设置报警规则，实现不误报、不漏报。</li><li>管理简单：分钟级万台设备的监控部署能力，故障自动恢复，集群可伸缩</li><li>自定义便捷配置：丰富的自定义产品配置功能，便捷、高效的完成产品配置、报警配置。</li><li>可视化：丰富的可视化 Dashboard，帮助您定制个性化的监控大盘。</li><li>低资源占用：在完成大量监控数据可靠传输的同时，保证对宿主机的 CPU、内存等资源极低占用率。</li></ul><p>Sunfire 技术架构如下：</p><p><img src="https://static001.infoq.cn/resource/image/fe/c3/fe8537a7abd2fc1bd30381ab1b30c4c3.jpg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Sunfire"></p><h3 id="主要模块实现及功能"><a href="#主要模块实现及功能" class="headerlink" title="主要模块实现及功能"></a>主要模块实现及功能</h3><p>针对架构图中的各个组件，其中最关键的为采集（Agent）、计算（Map、Reduce）组件，接下来将对这两个组件进行详细介绍。</p><h3 id="1-采集"><a href="#1-采集" class="headerlink" title="1. 采集"></a>1. 采集</h3><p>Agent 负责所有监控数据的原始采集，它以 Agent 形式部署在应用系统上，负责原始日志的采集、系统命令的执行。日志原始数据的采集，按周期查询日志的服务，且日志查询要低耗、智能。Agent 上不执行计算逻辑。主要具有以下特点：</p><p><strong>低耗</strong></p><p>采集日志，不可避免要考虑日志压缩的问题，通常做日志压缩则意味着它必须做两件事情：一是磁盘日志文件的内容要读到应用程序态；二是要执行压缩算法。</p><p>这两个过程就是 CPU 消耗的来源。但是它必须做压缩，因为日志需要从多个机房传输到集中的机房。跨机房传输占用的带宽不容小觑，必须压缩才能维持运转。所以低耗的第一个要素，就是避免跨机房传输。SunFire 达到此目标的方式是运行时组件自包含在机房内部，需要全量数据时才从各机房查询合并。</p><p>网上搜索 zero-copy，会知道文件传输其实是可以不经过用户态的，可以在 Linux 的核心态用类似 DMA 的思想，实现极低 CPU 占用的文件传输。SunFire 的 Agent 当然不能放过这个利好，对它的充分利用是 Agent 低耗的根本原因。以前这部分传输代码是用 c 语言编写的 sendfile 逻辑，集成到 Java 工程里，后来被直接改造为了 Java 实现。</p><p>最后，在下方的计算平台中会提到，要求 Agent 的日志查询服务具备“按周期查询日志”的能力。这是目前 Agent 工程里最大的难题，我们都用过 RAF（RandomAccessFile），你给它一个游标，指定 offset，再给它一个长度，指定读取的文件 size，它可以很低耗的扒出文件里的这部分内容。然而问题在于：周期≠offset。从周期转换为 offset 是一个痛苦的过程。</p><p>在流式计算里一般不会遇到这个问题，因为在流式架构里，Agent 是水龙头，主动权掌握在 Agent 手里，它可以从 0 开始 push 文件内容，push 到哪里就做一个标记，下次从标记继续往后 push，不断重复。这个标记就是 offset，所以流式不会有任何问题。</p><p>而计算平台周期任务驱动架构里，pull 的方式就无法提供 offset，只能提供 Term（周期，比如 2015-11-11 00:00 分）。Agent 解决此问题的方式算是简单粗暴，那就是二分查找法。而且日志还有一个天然优势，它是连续性的。所以按照对二分查找法稍加优化，就能达到“越猜越准”的效果（因为区间在缩小，区间越小，它里面的日志分布就越平均）。</p><p>于是，Agent 代码里的 LogFinder 组件撑起了这个职责，利用上述两个利好，实现了一个把 CPU 控制在 5% 以下的算法，目前能够维持运转。其中 CPU 的消耗不用多说，肯定是来自于猜的过程，因为每一次猜测，都意味着要从日志的某个 offset 拉出一小段内容来核实，会导致文件内容进入用户态并解析。这部分算法依然有很大的提升空间。</p><p><strong>日志滚动</strong></p><p>做过 Agent 的同学肯定都被日志滚动困扰过，各种各样的滚动姿势都需要支持。SunFire 的 pull 方式当然也会遇到这个问题，于是我们简单粗暴的穷举出了某次 pull 可能会遇到的所有场景，比如：</p><ul><li>正常猜到了 offset</li><li>整个日志都猜不到 offset</li><li>上次猜到了 offset，但是下次再来的时候发现不对劲（比如滚动了）</li></ul><p>这段逻辑代码穷举的分支之多，在一开始谁都没有想到。不过仔细分析了很多次，发现每个分支都必不可少。</p><p><strong>查询接口</strong></p><p>Agent 提供的查询服务分为 first query 和 ordinary query 两种。做这个区分的原因是：一个周期的查询请求只有第一次需要猜 offset，之后只需要顺序下移即可。而且计算组件里有大量的和 Agent 查询接口互相配合的逻辑，比如一个周期拉到什么位置上算是确定结束？一次 ordinary query 得到的日志里如果末尾是截断的（只有一半）该如何处理…… 这些逻辑虽然缜密，但十分繁琐，甚至让人望而却步。但现状如此，这些实现逻辑保障了 SunFire 的高一致性，不用担心数据不全、报警不准，随便怎么重启计算组件，随便怎么重启 Agent。但这些优势的背后，是值得深思的代码复杂度。</p><p><strong>路径扫描</strong></p><p>为了让用户配置简单便捷，SunFire 提供给用户选择日志的方式不是手写，而是像 windows 的文件夹一样可以浏览线上系统的日志目录和文件，让他双击一个心仪的文件来完成配置。但这种便捷带来的问题就是路径里若有变量就会出问题。所以 Agent 做了一个简单的 dir 扫描功能。Agent 能从应用目录往下扫描，找到同深度文件夹下“合适”的目标日志。</p><h3 id="2-计算"><a href="#2-计算" class="headerlink" title="2. 计算"></a>2. 计算</h3><p>由 Map、Reduce 组成计算平台，负责所有采集内容的加工计算，具备故障自动恢复能力及弹性伸缩能力。计算平台一直以来都是发展最快、改造最多的领域，因为它是很多需求的直接生产者，也是性能压力的直接承担者。因此，在经过多年的反思后，最终走向了一条插件化、周期驱动、自协调、异步化的道路。主要具有以下几个特点：</p><p><strong>纯异步</strong></p><p>原来的 SunFire 计算系统里，线程池繁复，从一个线程池处理完还会丢到下一个线程池里；为了避免并发 bug，加锁也很多。这其中最大的问题有两个：CPU 密集型的逻辑和 I/O 密集型混合。</p><p>对于第一点，只要发生混合，无论你怎么调整线程池参数，都会导致各式各样的问题。线程调的多，会导致某些时刻多线程抢占 CPU，load 飙高；线程调的少，会导致某些时刻所有线程都进入阻塞等待，堆积如山的活儿没人干。</p><p>对于第二点，最典型的例子就是日志包合并。比如一台 Map 上的一个日志计算任务，它要收集 10 个 Agent 的日志，那肯定是并发去收集的，10 个日志包陆续（同时）到达，到达之后各自解析，解析完了 data 要进行 merge。这个 merge 过程如果涉及到互斥区（比如嵌套 Map 的填充），就必须加锁，否则 bug 满天飞。</p><p>但其实我们重新编排一下任务就能杜绝所有的锁。比如上面的例子，我们能否让这个日志计算任务的 10 个 Agent 的子任务，全部在同一个线程里做？这当然是可行的，只要回答两个问题就行：</p><ul><li>如果串行，那 10 个 I/O 动作（拉日志包）怎么办？串行不就浪费 cpu 浪费时间吗？</li><li>把它们都放到一个线程里，那我怎么发挥多核机器的性能？</li></ul><p>第一个问题，答案就是异步 I/O。只要肯花时间，所有的 I/O 都可以用 NIO 来实现，无锁，事件监听，不会涉及阻塞等待。即使串行也不会浪费 CPU。第二个问题，就是一个大局观问题了。现实中我们面临的场景往往是用户配置了 100 个产品，每个产品都会拆解为子任务发送到每台 Map，而一台 Map 只有 4 个核。所以，你让一个核负责 25 个任务已经足够榨干机器性能了，没必要追求更细粒度子任务并发。因此，计算平台付出了很大的精力，做了协程框架。</p><p>我们用 Akka 作为协程框架，有了协程框架后再也不用关注线程池调度等问题了，于是我们可以轻松的设计协程角色，实现 CPU 密集型和 I/O 密集型的分离、或者为了无锁而做任务编排。接下来，尽量用 NIO 覆盖所有的 I/O 场景，杜绝 I/O 密集型逻辑，让所有的协程都是纯跑 CPU。按照这种方式，计算平台已经基本能够榨干机器的性能。</p><h2 id="周期驱动"><a href="#周期驱动" class="headerlink" title="周期驱动"></a>周期驱动</h2><p>所谓周期驱动型任务调度，说白了就是 Map/Reduce。Brain 被选举出来之后，定时捞出用户的配置，转换为计算作业模型，生成一个周期（比如某分钟的）的任务, 我们称之为拓扑 (Topology), 拓扑也很形象的表现出 Map/Reduce 多层计算结构的特征。所有任务所需的信息，都保存在 topology 对象中，包括计算插件、输入输出插件逻辑、Map 有几个、每个 Map 负责哪些输入等等。这些信息虽然很多，但其实来源可以简单理解为两个：一是用户的配置；二是运维元数据。拓扑被安装到一台 Reduce 机器（A）上。</p><p>A 上的 Reduce 任务判断当前集群里有多少台 Map 机器，就产生多少个任务（每个任务被平均分配一批 Agent），这些任务被安装到每台机器上 Map。被安装的 Map 任务其实就是一个协程，它负责了一批 Agent，于是它就将每个 Agent 的拉取任务再安装为一个协程。至此，安装过程结束。Agent 拉取任务协程（也称之为 input 输入协程，因为它是数据输入源）在周期到点后，开始执行，拉取日志，解析日志，将结果交予 Map 协程；Map 协程在得到了所有 Agent 的输入结果并 merge 完成后，将 merge 结果回报到 Reduce 协程（这是一个远程协程消息，跨机器）；Reduce 协程得到了所有 Map 协程的汇报结果后，数据到齐，写入到 Hbase 存储，结束。</p><p>上述过程非常简单，不高不大也不上，但经过多年大促的考验，其实非常的务实。能解决问题的架构，就是好的架构，能简单，为何要把事情做得复杂呢？</p><p>这种架构里，有一个非常重要的特性：任务是按周期隔离的。也就是说，同一个配置，它的 2015-11-11 00:00 分的任务和 2015-11-11 00:01 分的任务，是两个任务，没有任何关系，各自驱动，各自执行。理想情况下，我们可以做出结论：一旦某个周期的任务结束了，它得到的数据必然是准确的（只要每个 Agent 都正常响应了）。所以采用了这种架构之后，SunFire 很少再遇到数据不准的问题，当出现业务故障的时候我们都可以断定监控数据是准确的，甚至秒级都可以断定是准确的，因为秒级也就是 5 秒为周期的任务，和分钟级没有本质区别，只是周期范围不同而已。能获得这个能力当然也要归功于 Agent 的“按周期查询日志”的能力。</p><h2 id="任务重试"><a href="#任务重试" class="headerlink" title="任务重试"></a>任务重试</h2><p>在上节描述的 Brain-&gt;Reduce-&gt;Map 的任务安装流程里，我们对每一个上游赋予一个职责：监督下游。当机器发生重启或宕机，会丢失一大批协程角色。每一种角色的丢失，都需要重试恢复。监督主要通过监听 Terminated 事件实现，Terminated 事件会在下游挂掉 (不论是该协程挂掉还是所在的机器挂掉或是断网等) 的时候发送给上游。由于拓扑是提前生成好且具备完备的描述信息，因此每个角色都可以根据拓扑的信息来重新生成下游任务完成重试。</p><ul><li>若 Brain 丢失，则 Brain 会再次选主, Brain 读取最后生成到的任务周期, 再继续生成任务。</li><li>若 Reduce 丢失，每个任务在 Brain 上都有一个 TopologySupervisor 角色, 来监听 Reduce 协程的 Terminated 事件来执行重试动作。</li><li>若 Map 丢失，Reduce 本身也监听了所有 Map 的 Terminated 事件来执行重试动作。</li><li>为确保万无一失，若 Reduce 没有在规定时间内返回完成事件给 Brain，Brain 同样会根据一定规则重试这个任务。</li></ul><p>过程依然非常简单，而且从理论上是可证的，无论怎么重启宕机，都可以确保数据不丢，只不过可能会稍有延迟（因为部分任务要重新做）。</p><p>输入共享：在用户实际使用 SunFire 的过程中，常常会有这样的情况：用户配了几个不同的配置，其计算逻辑可能是不同的，比如有的是单纯计算行数，有的计算平均值，有的需要正则匹配出日志中的内容，但这几个配置可能都用的同一份日志，那么一定希望几个配置共享同一份拉取的日志内容。否则重复拉取日志会造成极大的资源消耗。那么我们就必须实现输入共享，输入共享的实现比较复杂，主要依赖两点：</p><ul><li>其一是依赖安装流，因为拓扑是提前安装的，因此在安装到真正开始拉取日志的这段时间内，我们希望能够通过拓扑信息判断出需要共享的输入源，构建出输入源和对应 Map 的映射关系。</li><li>其二是依赖 Map 节点和 Agent 之间的一致性哈希，保证 Brain 在生成任务时，同一个机器上的日志，永远是分配相同的一个 Map 节点去拉取的（除非它对应的 Map 挂了）。</li></ul><p>站在 Map 节点的视角来看：在各个任务的 Reduce 拿着拓扑来注册的时候，我拿出输入源（对日志监控来说通常可以用一个 IP 地址和一个日志路径来描述）和 Map 之间的关系，暂存下来，每当一个新的 Reduce 来注册 Map，我都判断这个 Map 所需的输入源是否存在了，如果有，那就给这个输入源增加一个上游，等到这个输入源的周期到了，那就触发拉取，不再共享了。</p><h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><p><strong>存储</strong>：负责所有计算结果的持久化存储，可以无限伸缩，且查询历史数据保持和查询实时数据相同的低延迟。Sunfire 原始数据存储使用的是阿里集团的 Hbase 产品（HBase ：Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统），用户配置存储使用的是 MongoDB。</p><p><strong>展示</strong>：负责提供用户交互，让用户通过简洁的建模过程来打造个性化的监控产品。基于插件化、组件化的构建方式，用户可以快速增加新类型的监控产品。</p><p><strong>自我管控</strong>：即 OPS-Agent、Ops-web 组件，负责海量 Agent 的自动化安装监测，并且承担了整个监控平台各个角色的状态检测、一键安装、故障恢复、容量监测等职责。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;在 2016 年双 11 全球购物狂欢节中，天猫全天交易额 1207 亿元，</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Alibaba" scheme="http://posts.hufeifei.cn/tags/Alibaba/"/>
    
    <category term="monitor" scheme="http://posts.hufeifei.cn/tags/monitor/"/>
    
    <category term="Sunfire" scheme="http://posts.hufeifei.cn/tags/Sunfire/"/>
    
  </entry>
  
  <entry>
    <title>阿里业务全链路智能监控探索</title>
    <link href="http://posts.hufeifei.cn/backend/alimonitor-sunfire/"/>
    <id>http://posts.hufeifei.cn/backend/alimonitor-sunfire/</id>
    <published>2021-11-22T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/a7d4cb54743148c9967ec5c0c288b39b.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><h2 id="阿里监控的发展与挑战"><a href="#阿里监控的发展与挑战" class="headerlink" title="阿里监控的发展与挑战"></a>阿里监控的发展与挑战</h2><p>我把整个阿里巴巴监控发展分成四个阶段：</p><p>第一个是在2011年以前，这是一个草莽阶段，在这个阶段大家的监控系统参差不齐，各种自研的，开源系统都上，能够抓到老鼠的都是好猫，当然这种模式随着规模大了以后变的难以维护。</p><p>第二个就是监控平台化的阶段，在这个阶段其实解决的更多是监控的技术问题，就是怎么做监控的采集、存储、报警等等这些技术上面的问题，主要代表就Alimonnitor系统，它能帮助用户把采集、计算、报警这些问题都处理掉，当时主要的用户是我们运维的同学PE，他们只关心我要采什么数据，然后在上面写脚本，直接在我们的监控系统里面配置，最后就能监控起来。Alimonitor开始大家用的非常爽，到后面也暴露出很多问题：</p><p>第一个就是自定义的东西就很多，标准化的程度低，那你这些监控的运维管控就很困难，进一步的数据分析挖掘就更难做了。举个例子，我一个磁盘容量监控，就有二十几个各种不同的监控项，这些监控项的指标命名、格式、单位都是不一样的。</p><p>第二个就是它对用户的要求比苛刻，复杂程度比较高，比较适合像PE这样一些专业的运维同学。</p><p>第三个阶段是标准化的阶段，在这个阶段我们主推的就是我们现在的Sunfire监控平台。</p><p>我们把基础的监控都已经标准化掉，只要一个应用上线这些基础的监控数据全部都有了，你只需对一些报警规则做一些设置就可以了，这样对于不是很专业的人用起来会更加的方便，同时在这个阶段有很多的自动化的诊断工具应运而生的，在这个阶段是一个诊断排查工具大爆发的一个阶段。</p><p>第四个阶段就是智能化的阶段，智能化阶段要就是要实现无人化，整个监控运维的一体化。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/ff05142faa13455f9a2dfaef915af551.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>下面说一下整个集团监控平台的规模，平台是以租户的形式来进行划分的，一个租户差不多相当于一个事业部的规模，比如说是天猫、淘宝、优酷等等，现在我们基本上已经覆盖全集团80%以上的事业部。</p><p>我们整个平台的监控服务器有4000多个台，这里不是说监控了多少服务器，而是现在用了这么多机器在做监控，这里不包括我们的存储服务，主要是计算以及采集计算等等这部分的服务。</p><p>整个对监控的应用大概是在一万个以上，平台主要是通过分析日志的方式做数据采集的，现在每秒钟平均的日志处理量大概是在2T左右。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/96c2635b737e4bb39dd7d31bdddd9f0b.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>接下来讲一下阿里监控一些挑战，主要围绕业务监控来说：</p><p>近年来，随着集团新业务、新技术的快速发展，传统的业务总量的“监控大盘”已经越来越不能满足监控需求，主要表现在以下几个方面：</p><p><strong>缺乏全局视角</strong></p><p>“监控大盘”主要反映的是单个业务或应用的运行状态，缺少全局的业务视角能反应整个“业务域”的上下游整体的运行情况，比如交易系统成功率下跌，想看看是不是优惠出问题了，但是不知道“优惠”的业务监控在哪里，只能依赖”优惠”的同学去排查，钉钉电话沟通，大家来拼凑信息，上下游协调成本很高。</p><p><strong>监控标准不统一</strong></p><p>一直以来“业务监控”都是自定义的，依赖开发人员的个人经验，往往系统、业务监控混在一起，没有标准，业务之间不能比较；各系统监控能力参差不齐，很容易出现业务链路中的监控断层；业务监控缺少一套行之有效的方法论，新人或者新业务对于业务要怎么监控，不知道如何下手、不知道自己配的监控是否覆盖全面，只有等到故障发生以后才去补监控</p><p><strong>缺少业务视图</strong></p><p>随着阿里业务飞速发展，特别是“大中台”的建设，使得传统的“总量”监控已经不能满足需求，比如一个“交易”中台业务就会有数十个“业务方”调用，单纯的总量监控会把小调用量的业务淹没，必须按每个业务方的“业务身份”进行监控。</p><p>对于像“盒马”、“淘鲜达”这样的新零售业务，这样的问题更加突出，一家门店出现交易异常对于“交易总量”来说是微不足道的，但是对这件门店的客户体验来说是灾难性的?</p><p><strong>监控配置成本高</strong></p><p>“业务监控”一直都是由“开发人员”纯手工打造，需要经过日志埋点、监控配置、报警阈值设置，整个过程费时费力，缺乏自动化、智能化监控的手段，这也是造成各系统监控能力参差不齐的重要原因，一些新业务因为无力投入大量精力配置监控，导致业务监控能力缺失。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/e62e96d7b6c44577ade96c79a267f4f2.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><h2 id="业务全链路监控的思考和建设"><a href="#业务全链路监控的思考和建设" class="headerlink" title="业务全链路监控的思考和建设"></a>业务全链路监控的思考和建设</h2><p>第二部分重点讲一下业务全链路监控是怎么做的，怎么解决我们的痛点。</p><p>一开始我们的监控系统都是针对单个应用的，比如说交易的开发只关心交易指标是是否正常，优惠的同学只关心自己系统的健康状况，他们各自配置数据大盘，报警等等。</p><p>一旦出现问题怎么办？大家只能在群里面问，或者电话会议，互相讨论，定位到底是谁出了问题。</p><p>同时上层的领导更加着急，因为他看不到整体情况，就只能挨个问情况。所以单系统监控的问题就是看不到全景，上下游协同成本非常之高；系统的监控能力是依赖于开发人员自己的素养，能力高一点的，监控完善一些，技能不足或者意识不够的可能就是空白。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/9b5debd866c14c359f3112257fc191cc.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>后来有很多trace的监控出来，比如说阿里的鹰眼系统，它主要解决的是如何做系统间链路排查，这样从原来完全看不到全景，发展到可以看一部分系统调用链路上面的情况。</p><p>同时trace监控也存在一些不足，什么问题呢？首先，那它是没有业务含义的，就比如说我们中台交易的业务，其实服务于很多业务方，每个业务方调用都是调用同一个API，在系统链路里只能看到总量，比如我们拿盒马来说，系统链路就看不出问题，因为盒马在整个交易里面量会占的比较小，总量的波动也很小。</p><p>其次，系统链路的细节太多，缺乏对强弱依赖的梳理，导致整个链路形如蛛网，反而看不清全局。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/133458f3e78b41a3a479c43cb834153a.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>业务全链路监控就是要从业务的视角出发，监控整个业务流程的健康状况，无需多个系统切换，直观看到全局和上下游，方便快速发现、定位问题。</p><p>建立了完整的“业务监控模型”，为业务建立起一个从“宏观”到“微观”的全景式业务监控体系，结束了业务监控没有标准，只能纯手工打造的历史。业务监控模型主要包括3部分：</p><p>业务域：一个完整的业务或产品称为“业务域”，如电商的“交易域”、“营销域”、“支付域”等。</p><p>业务活动：业务域中的的核心业务用例叫做“业务活动”，如交易域的“下单确认”、“创建订单”等，业务活动是整个监控模型的核心，每个业务活动都会有标准的【黄金指标】来反应自身的健康状况，业务活动之间建立上下游关系就形成了业务链路。</p><p>系统服务：业务活动中的依赖的关键方法称作“系统服务”，如“下单确认”包含：查询会员、查询商品、查询优惠等关键方法，每个系统服务也通过【黄金指标】来表示其健康状况。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/f8fbc3ebac81450b86c3c93069813ab3.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>这是我们整个平台的产品大图，最底层是数据采集，所有的数据都会在这里被处理，上面是各个服务层，包括我们的元数据中心，以及对外提供数据的服务，事件中心包括报警事件查询，通知规则定义等等，最后一个是我们的整个监控指标库，所有监控指标都要落到这里统一管理。</p><p>业务层主要分成两块业务：一块是应用监控，应用监控里面主要是一些基础的标准化的监控，第二块是业务监控，原来主要是靠用户自定义，缺少标准和对数据有效组织，以后更多的会以业务全链路作为一个入口，把所有的业务指标以及后面的应用监控来串联起来。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/7d8fa43dc53948c9b5dc29de6e3329ac.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>以“业务模型”为基础，我们总结出了一套如何做“业务全链路监控”的方法论，并将其沉底到产品中。</p><p>梳理关键业务： 业务方需要梳理出自己的核心业务是什么（业务活动），以及这些核心业务的关键依赖有哪些（系统服务）。</p><p>监控数据埋点：提供了无侵入的配置化监控SDK，只要将‘业务活动’和‘系统服务’对应的方法填写到配置文件中即可，系统会自动收集，计算，上报监控数据。</p><p>监控链路：系统根据收集的数据自动生成业务链路，每个‘业务活动’和‘系统服务’节点都自动生成流量、耗时、成功率的黄金指标，同时每个‘节点’都可以通过钻取查看详细的监控数据，包括：不同机房、单元、分组的数据对比，每个业务身份的明细调用情况等。</p><p>异常检测：业务链路涉及节点众多，必须要有完善的异常检测机制来帮助用户自动发现问题，我们提供了“智能基线预警”和“专家规则预警”相结合的异常检测机制，无需用户逐个配置报警规则，自动发现异常节点，实时将这些节点‘标红’，异常的详细信息也会同步显示，方便用户快速发现和定位问题。</p><p>通过业务全链路监控，可以做到对业务域的监控标准化和全覆盖，避免了自定义监控覆盖不全面、不标准、配置工作量大的问题，使得老板、PD、运营、监控值班等用户都可以快速了解业务是否有问题。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/26c50ccec08c41f9896efc71e2f6cacd.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>这里引入Google的黄金指标概念，改变了业务监控完全依赖自定义的现状，为业务监控树立了标准。</p><p>流量 ：业务在单位时间内的调用量，如：服务的QPS、每秒订单笔数等。</p><p>耗时 ：业务的具体处理时长，需区分成功耗时和失败耗时。</p><p>错误 ：调用出错数量、成功率、错误码。</p><p>饱和度 ：应用已使用资源的占比。 由于饱和度更多反应的是应用的层面情况，所以业务监控使用流量、耗时、错误这三个指标就能很好的回答“业务”是否健康的问题，在“业务全链路监控”中每个业务活动和系统服务都会标配这三个监控指标。</p><p>除了黄金指标以外，还可以根据各自业务的不同特点，定义各种分维度的辅助指标，比如：按不同的业务身份，按商家、按门店分，不同的错误码等等，用于进一步细化和定位问题。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/4a6c7466289b41b1a31060f2302fb465.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>前面我们已经确定了“业务域”、“业务活动”和“系统服务”三层业务模型，再通过“黄金指标”来描述这些业务模型的监控状况，这样我们就能‘量化’清楚一件事情：业务是否健康。</p><p>能量化“业务健康”这件事情价值很大，是很多AIOps的基础，比如我们在做无人职守发布或者智能调度的时候，就需要明确我在调度前后或者发布的前后，业务到底健不健康，是否正常，否则智能化运维就是一句空话，无法大规模推广。</p><p>横向业务维度：传统的‘总量’指标已经不能满足中台、盒马这样的业务监控需求了，通过可扩展的业务维度实现对业务身份、商家、门店的精细化监控。</p><p>像“交易”这样的中台业务会被几十个业务方调用，总量没有异常并不代表具体的业务方没有问题，而是需要监控每一个业务方各自的调用情况，只要有一个出现异常就要预警。</p><p>业务全链路监控提供了“横向业务维度”功能，能够方便的配置‘业务身份’、‘商家’、‘门店’等特定的业务维度，可以对一个业务域中所有的“业务活动”和“系统服务”按一个维度过滤，比如可以对交易链路按“盒马”这个业务身份过滤，从而在链路上看到的是盒马的交易调用情况。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/c8ff602fcaaf4dbb94ce90ff7bc91a66.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>监控SDK使用AOP切面技术实现了配置化埋点能力，业务系统引入监控SDK后，通过简单的一个配置文件即可完成监控埋点，自动完成数据的拦截、计算、上报，与业务代码完全解耦。</p><p>自动生成核心链路、黄金指标、下钻的业务维度大盘，无需用户配置，用户还可以通过可视化编辑页面对链路进行调整。</p><p>业务码：用来表示这行日志是属于哪个一个业务域，哪个业务活动或者系统服务。</p><p>错误码：错误信息里面最重要的是错误码，产生了一次错误以后到底是什么样的错误就全靠错误码进行区分，这样你在做定位的时候才能有的放矢。</p><p>错误类型：错误一定要分类型，大致分为这么两类。一类是用户级的错误，一类是系统级的错误。 对于用户级的错误，比如用户登录的时候密码输入出错了，这些错误是已知道的，对整个系统是不会产生影响的，它是属于业务日常错误里的一部分，你需要在做监控的时候把这些错误排除掉，要不然你整个成功率指标就失准了。</p><p>扩展信息：扩展信息里输出的是每个业务特有的维度，也就是前面提到的“横向业务维度”，比如“业务方”，“商家名称”等，比较通用的是一个压测标识，记录这条数据是真实流量还是压测请求。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/17a0cb8c2313444fadf18580a1cbe9b6.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>接下来讲一下监控存储，我们所有的数据格式化以后，就会存储到监控数据库里，今年我们监控平台在存储上最主要做的一件事情就是把监控数据全部标准化，数据从原来的HBASE迁移到HiTSDB里面。HiTSDB是一个高性能的时序数据库，已经在阿里云上发布了，现在已经在上面存储了几十亿条时间线。</p><p>在HiTSDB的上层实现了一套监控数据查询语言（MQL），通过MQL我们实现了监控数据查询的标准化，上层的图形展示和报警都会通过这种方式去查询，会大大简化操作。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/02d4780726e7455f894933ce0279df3b.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>业务链路监控发现异常以后，就需要对异常做更加精确的定位，最主要的是两大平台。</p><p>一个是我们前面提到过的鹰眼系统，原理和Dapper类似，主要做系统Trace链路排查。</p><p>第二个我们叫A3系统，它是一个日志的异常诊断的一个工具，我们的日志里边肯定会很多的错误堆栈，各种错误信息，A3通过一种比较智能化的手段对错误进行分析和归类，统计每种错误的出现时间、次数、类型和原始样本等信息，帮你找出应用里面有哪些错误是比较高危，哪些错误是最近新产生的，这样能快速帮助你定位问题。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/69fe9cd305ba41c287b83cba1ab812a1.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>在业务链路监控中，跨部门或者跨BU的链路监控一直是个难题，业务全链路路监控通过“黄金指标”+“业务维度”来解决这个难题。</p><p>首先每个部门把直接对外提供服务的核心业务梳理出来，并设定它的“黄金指标”，我们设定这样的黄金指标为这个部门的SLA指标，其它部门的系统调用这个业务时，就看这些SLA的指标是否正常，而不再关心这个业务的内部调用细节。</p><p>一般这样的业务调用方很多，我们通过一个“业务身份”的维度来对这些SLA指标进行细分，每个业务身份会有独立的SLA指标数据，调用方只看自己的业务身份对应的监控数据即可判断对方业务是否正常。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/219ce932d11840c9b8a7ada86fa8b0d9.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>每个部门都定义自己的对外核心业务和SLA指标，相互之间通过数据说话，形成一种标准化的服务能力评价体系，随着这个体系的不断推进，业务全链路可以把各个部门的业务节点串联起来，组成一个更大的大网，在这个里面每个部门都对外暴露的自己的关键业务，每个关键业务通过“黄金指标”量化服务能力，这样就可以快速定位到底是谁的问题，建立起一个正在的全景式监控。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/c25433d5eb0e49c9919a8abfb8da59db.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>接下来，我们通过一个交易链路实例来看一下效果，链路中列出交易域的关键业务，也就是交易对其它部门提供的核心服务，省略了每个业务的‘系统服务’等细节，整个链路可以通过“业务身份”来过滤不同调用方的调用情况，通过智能检测程序实现对每个调用方的SLA监控，一旦出现异常会通过短信等消息通知用户，并将节点标记为异常。</p><p>每个业务节点都可以钻取查看更详细的业务活动大盘，它是业务异常排查的一个总的入口，整合了这个业务活动的所有业务指标，关联应用的系统指标，以及系统调用的链路等等信息。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/d5487a9b83a541c999b598df5c9fe172.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><h2 id="智能异常检测的探索"><a href="#智能异常检测的探索" class="headerlink" title="智能异常检测的探索"></a>智能异常检测的探索</h2><p>最后我讲一下在业务全链路监控里面我们是怎么来做智能化检测的。我们会把一个业务活动的监控指标分为三个层次：</p><p>第一层是这个业务活动的黄金指标，对第一层的指标我们会追求它的准确率，会为每一个指标建立单独的预测模型，然后通过时间序列等算法，尽可能做到对这个指标的准确判断以及报警。</p><p>第二层是这个业务活动依赖各系统服务的监控指标，对于这一层指标我们采用的准确性与成本和效率均衡的策略，通过一些轻量级波动检测算法来实现异常检测，而且这一层次的检测是由上一层触发，只有在上一层检测发现异常时才触发，不会一直定时执行。</p><p>第三层是这个业务活动所在应用的监控指标，包括他整个系统调用链路上的各种应用指标，包括系统指标、各类中间件、缓存、数据库等等，这一层的数据量是最大的，我们不会直接用智能算法分析这些指标，而是分析它们产生的各类报警事件和变更事件。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/0301b0121bbc4b55b960fefcab87fc32.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>有了数据分层以后，我们设计了一个自上而下的分析流程，首先是检测核心指标是否正常，在核心指标出现异常以后，我们会快速检测它下面的依赖的系统服务是否有问题，确定是那些服务出现了问题，尽可能的减少开发人员的排查范围，指明排查方向。</p><p>比如说机房网络故障，只有这个机房的服务指标下跌了，其他机房都没有下跌，这个时候就可以比较明确的发现是这个机房的问题。</p><p>再下面就对它整个系统链路上面的排查，可以通过分析链路上的异常事件来做，可以根据这些事件在整个链路中的位置，以及跟核心指标异常的时间相关性给出一些可疑程度的排序。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/c8605472716247219dfd6f6210f1ec45.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p><p>最后说一下核心指标的异常检测，我们称为“智能基线”预警，目前有超过1200个核心指标接入到智能基线。</p><p>它的准确率和召回率相对于人工智阈值有明显提高。任何一个接入智能基线的指标，“智能基线”系统都会计算出它的预测基线，以及合理的边界，当实际数据超出边界达到一定时间后就会触发报警。</p><p>用户还可以对智能基线做一些微调，比如说高灵敏度还是低灵敏都，是只对下跌报警还是上涨下跌都要报警，等等。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180918/1a601941575949fd8e4555c9916c7927.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="img"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;img src=&quot;http://5b0988e595225.cdn.soh</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Alibaba" scheme="http://posts.hufeifei.cn/tags/Alibaba/"/>
    
    <category term="monitor" scheme="http://posts.hufeifei.cn/tags/monitor/"/>
    
    <category term="Sunfire" scheme="http://posts.hufeifei.cn/tags/Sunfire/"/>
    
  </entry>
  
  <entry>
    <title>一文图解Kubernetes的持久化存储解决方案</title>
    <link href="http://posts.hufeifei.cn/backend/kubernetes-persistent/"/>
    <id>http://posts.hufeifei.cn/backend/kubernetes-persistent/</id>
    <published>2021-11-19T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes（下称k8s）作为目前行业内使用最广泛的容器编排工具，已经深入到各个技术领域，正在彻底改变应用程序的开发和部署方式；但从另一个方面讲，k8s的架构是不断变化的。容器的创建和销毁，从本质上讲，它们的生命周期是短暂的。因而，K8s的发展历程势必无法绕开持久化的问题，本文就将从这一点出发，为大家讲解k8s在持久化存储方面所提供的解决方案，帮助大家更好的理解k8s的整体技术生态。</p><p>本文的章节内容分配如下：</p><ul><li>概述</li><li>K8s有哪些持久化方案</li><li>Docker存储</li><li>K8s原生存储</li><li>开源存储项目Ceph&amp;Rook</li><li>总结</li></ul><hr><h2 id="K8s有哪些持久化方案"><a href="#K8s有哪些持久化方案" class="headerlink" title="K8s有哪些持久化方案"></a>K8s有哪些持久化方案</h2><ul><li><strong>外部存储方案：</strong></li></ul><p>先抛一张<a href="https://landscape.cncf.io/">CNCF（云原生计算基金会）</a>公布的云原生存储解决方案一览图，这里只截取了存储的部分。</p><p><img src="https://pic1.zhimg.com/80/v2-cf616af7b8b84824c5381376c82eb50c_720w.jpg" alt="img"></p><p>图中列举的存储方案，目前都可以集成到Kubernetes平台。</p><ul><li><p><strong>Docker存储卷</strong></p><p>当使用Docker作为K8s的容器方案时，Docker自身所支持的存储卷也就成为了可选方案之一。Docker存储卷是容器服务在单节点的存储组织形式，作为解决数据存储、容器运行时的技术方案；</p></li><li><p><strong>K8s存储卷</strong></p><p>K8s自己的持久化存储方案更关注于应用和集群层面，主要用于容器集群的存储编排，从应用使用存储的角度提供存储服务。另一方面，K8s的持久化存储方案能够完全兼容自身的对象，如Pod对象等，即插即用，无需二次开发。</p></li></ul><p>下面，我们就对这几种存储方案一一进行解释。</p><h2 id="Docker存储"><a href="#Docker存储" class="headerlink" title="Docker存储"></a>Docker存储</h2><p><strong>容器的读写层</strong></p><p>为了提高节点存储的使用效率，容器不光在不同运行的容器之间共享镜像资源，而且还实现了在不同镜像之间共享数据。共享镜像数据的实现原理：镜像是分层组合而成的，即一个完整的镜像会包含多个数据层，每层数据相互叠加、覆盖组成了最终的完整镜像。</p><p><img src="https://pic4.zhimg.com/80/v2-d5b7bd1951c46c530e0bd9a402b90047_720w.jpg" alt="img"></p><p>为了实现多个容器间共享镜像数据，容器镜像每一层都是只读的。而容器使用镜像时，在多个镜像分层的最上面还添加了一个读写层。每一个容器在运行时，都会基于当前镜像在其最上层挂载一个读写层，用户针对容器的所有操作都在读写层中完成。一旦容器销毁，这个读写层也随之销毁。</p><p><strong>容器的数据卷</strong></p><p>容器中的应用读写数据都是发生在容器的读写层，镜像层+读写层映射为容器内部文件系统、负责容器内部存储的底层架构。当我们需要容器内部应用和外部存储进行交互时，还需要一个外置存储，容器数据卷即提供了这样的功能。</p><p>另一方面，容器本身的存储数据都是临时存储，在容器销毁的时候数据会一起删除。而通过数据卷将外部存储挂载到容器文件系统，应用可以引用外部数据，也可以将自己产出的数据持久化到数据卷中，因此容器数据卷是容器实现数据持久化的主要实现方式。<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.452ex" xmlns="http://www.w3.org/2000/svg" width="62.545ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 27644.9 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">容</text></g><g data-mml-node="mi" transform="translate(900,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">器</text></g><g data-mml-node="mi" transform="translate(1800,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">存</text></g><g data-mml-node="mi" transform="translate(2700,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">储</text></g><g data-mml-node="mi" transform="translate(3600,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">组</text></g><g data-mml-node="mi" transform="translate(4500,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">成</text></g><g data-mml-node="mi" transform="translate(5400,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">：</text></g><g data-mml-node="mi" transform="translate(6300,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">只</text></g><g data-mml-node="mi" transform="translate(7200,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">读</text></g><g data-mml-node="mi" transform="translate(8100,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">层</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">（</text></g><g data-mml-node="mi" transform="translate(9900,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">容</text></g><g data-mml-node="mi" transform="translate(10800,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">器</text></g><g data-mml-node="mi" transform="translate(11700,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">镜</text></g><g data-mml-node="mi" transform="translate(12600,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">像</text></g><g data-mml-node="mi" transform="translate(13500,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">）</text></g><g data-mml-node="mo" transform="translate(14622.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(15622.4,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">读</text></g><g data-mml-node="mi" transform="translate(16522.4,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">写</text></g><g data-mml-node="mi" transform="translate(17422.4,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">层</text></g><g data-mml-node="mo" transform="translate(18544.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(19544.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">外</text></g><g data-mml-node="mi" transform="translate(20444.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">置</text></g><g data-mml-node="mi" transform="translate(21344.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">存</text></g><g data-mml-node="mi" transform="translate(22244.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">储</text></g><g data-mml-node="mi" transform="translate(23144.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">（</text></g><g data-mml-node="mi" transform="translate(24044.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">数</text></g><g data-mml-node="mi" transform="translate(24944.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">据</text></g><g data-mml-node="mi" transform="translate(25844.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">卷</text></g><g data-mml-node="mi" transform="translate(26744.9,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">）</text></g></g></g></svg></mjx-container><br>容器数据卷从作用范围可以分为：单机数据卷 和 集群数据卷。其中单机数据卷即为容器服务在一个节点上的数据卷挂载能力，docker volume 是单机数据卷的代表实现；</p><p>Docker Volume 是一个可供多个容器使用的目录，它绕过 UFS，包含以下特性：</p><blockquote><p>数据卷可以在容器之间共享和重用；<br>相比通过存储驱动实现的可写层，数据卷读写是直接对外置存储进行读写，效率更高；<br>对数据卷的更新是对外置存储读写，不会影响镜像和容器读写层；<br>数据卷可以一直存在，直到没有容器使用。</p></blockquote><h3 id="1）Docker-数据卷类型"><a href="#1）Docker-数据卷类型" class="headerlink" title="1）Docker 数据卷类型"></a>1）Docker 数据卷类型</h3><p>Bind：将主机目录/文件直接挂载到容器内部。</p><blockquote><p>需要使用主机的上的绝对路径，且可以自动创建主机目录；<br>容器可以修改挂载目录下的任何文件，是应用更具有便捷性，但也带来了安全隐患。</p></blockquote><p>Volume：使用第三方数据卷的时候使用这种方式。</p><blockquote><p>Volume命令行指令：docker volume (create/rm)；<br>是Docker提供的功能，所以在非 docker 环境下无法使用；<br>分为命名数据卷和匿名数据卷，其实现是一致的，区别是匿名数据卷的名字为随机码；<br>支持数据卷驱动扩展，实现更多外部存储类型的接入。</p></blockquote><p>Tmpfs：非持久化的卷类型，存储在内存中。</p><blockquote><p>数据易丢失。</p></blockquote><h3 id="2）数据卷使用语法"><a href="#2）数据卷使用语法" class="headerlink" title="2）数据卷使用语法"></a>2）数据卷使用语法</h3><ul><li><strong>Bind挂载语法</strong></li></ul><p>-v: src:dst:opts 只支持单机版。</p><blockquote><p>Src：表示卷映射源，主机目录或文件，需要是绝对地址；<br>Dst：容器内目标挂载地址；<br>Opts：可选，挂载属性：ro, consistent, delegated, cached, z, Z；<br>Consistent, delegated, cached：为mac系统配置共享传播属性；<br>Z、z：配置主机目录的selinux label。</p></blockquote><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name devtest -v /home:/data:ro,rslave nginx</span><br><span class="line">$ docker run -d --name devtest --mount <span class="built_in">type</span>=<span class="built_in">bind</span>,<span class="built_in">source</span>=/home,target=/data,<span class="built_in">readonly</span>,bind-propagation=rslave nginx</span><br><span class="line">$ docker run -d --name devtest -v /home:/data:z nginx</span><br></pre></td></tr></table></figure><ul><li><strong>Volume 挂载方式语法</strong></li></ul><p>-v: src:dst:opts 只支持单机版。</p><blockquote><p>Src：表示卷映射源，数据卷名、空；<br>Dst：容器内目标目录；<br>Opts：可选，挂载属性：ro（只读）。</p></blockquote><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name devtest -v myvol:/app:ro nginx</span><br><span class="line">$ docker run -d --name devtest --mount <span class="built_in">source</span>=myvol2,target=/app,<span class="built_in">readonly</span> nginx</span><br></pre></td></tr></table></figure><h3 id="3）Docker数据卷插件"><a href="#3）Docker数据卷插件" class="headerlink" title="3）Docker数据卷插件"></a>3）Docker数据卷插件</h3><p>Docker 数据卷实现了将容器外部存储挂载到容器文件系统的方式。为了扩展容器对外部存储类型的需求，docker 提出了通过存储插件的方式挂载不同类型的存储服务。扩展插件统称为 Volume Driver，可以为每种存储类型开发一种存储插件。</p><p>可以查看链接：<a href="https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins">https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins</a></p><p>其特性简单来说可以总结为2点：</p><blockquote><p>单个节点上可以部署多个存储插件；<br>一个存储插件负责一种存储类型的挂载服务。</p></blockquote><p><img src="https://pic4.zhimg.com/80/v2-e9541900fd6cf76b79e9a5a55a618963_720w.jpg" alt="img"></p><p>关于Volume plugin的代码实现，可以参考这篇小文章：<a href="https://jimmysong.io/blog/docker-create-plugin/">Docker 17.03-CE create plugin源码解析</a></p><p>Docker Plugin 是以Web Service的服务运行在每一台Docker Host上的，通过HTTP协议传输RPC风格的JSON数据完成通信。Plugin的启动和停止，并不归Docker管理，Docker Daemon依靠在缺省路径下查找Unix Socket文件，自动发现可用的插件。</p><p>当客户端与Daemon交互，使用插件创建数据卷时，Daemon会在后端找到插件对应的 socket 文件，建立连接并发起相应的API请求，最终结合Daemon自身的处理完成客户端的请求。</p><p>Docker Daemon 与 Volume driver 通信方式有：</p><ul><li>Sock文件：linux 下放在/run/docker/plugins 目录下</li><li>Spec文件：/etc/docker/plugins/convoy.spec 定义</li><li>Json文件：/usr/lib/docker/plugins/infinit.json 定义</li></ul><p>实现接口：</p><blockquote><p>Create, Remove, Mount, Path, Umount, Get, List, Capabilities;</p></blockquote><p>使用示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume create --driver nas -o diskid=<span class="string">""</span> -o host=<span class="string">"10.46.225.247"</span> -o path=”/nas1<span class="string">" -o mode="</span><span class="string">" --name nas1</span></span><br></pre></td></tr></table></figure><p>Docker Volume Driver 适用在单机容器环境或者 swarm 平台进行数据卷管理，随着 K8s 的流行其使用场景已经越来越少，这里不做赘述。</p><h2 id="K8s原生存储"><a href="#K8s原生存储" class="headerlink" title="K8s原生存储"></a>K8s原生存储</h2><p>如果说Docker注重的是单节点的存储能力，那K8s 数据卷关注的则是集群级别的数据卷编排能力。</p><h3 id="卷-Volume"><a href="#卷-Volume" class="headerlink" title="卷-Volume"></a>卷-Volume</h3><p>Kubernetes 提供是卷存储类型，从存在的生命周期可分为临时和持久卷。 从卷的类型分，又可以分为本地存储、网络存储、Secret/ConfigMap、CSI/Flexvolume、PVC；有兴趣的小伙伴可以参考一下官方文档：<a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/">https://kubernetes.io/zh/docs/concepts/storage/volumes/</a></p><p>这里就以一幅图来展示各个存储的存在形式。</p><p><img src="https://pic2.zhimg.com/80/v2-cfd94cca4b32bcf42cdd253a5812aff5_720w.jpg" alt="img"></p><p>如上图所示：</p><ul><li>最上层的pod和PVC由用户管理，pod创建volume卷，并指定存储方式。</li><li>中间部分由集群管理者创建StorageClass对象，StorageClass只需确定PV属性（存储类型，大小等）及创建PV所需要用的的存储插件；K8s会自动根据用户提交的PVC来找到对应的StorageClass，之后根据其定义的存储插件，创建出PV。</li><li>最下层指代各个实际的存储资源。</li></ul><h3 id="PV和PVC"><a href="#PV和PVC" class="headerlink" title="PV和PVC"></a>PV和PVC</h3><p>这里单独来聊聊PV和PVC，也是实际应用场景中最常用的一组概念，其中：</p><p>PV 是 PersistentVolume 的缩写，译为持久化存储卷；PV 在 K8s 中代表一个具体存储类型的卷，其对象中定义了具体存储类型和卷参数。即目标存储服务所有相关的信息都保存在 PV 中，K8s 引用 PV 中的存储信息执行挂载操作。</p><p>PVC的存在，是从应用角度对存储卷进行二次抽象；由于 PV 描述的是对具体存储类型，需要定义详细的存储信息，而应用层用户在消费存储服务的时候往往不希望对底层细节知道的太多，让应用编排层面来定义具体的存储服务不够友好。这时对存储服务再次进行抽象，只把用户关系的参数提炼出来，用 PVC 来抽象更底层的 PV。所以 PVC、PV 关注的对象不一样，PVC 关注用户对存储需求，给用户提供统一的存储定义方式；而 PV 关注的是存储细节，可以定义具体存储类型、存储挂载使用的详细参数等。其具体的对应关系如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-bd7774b3daa4dabeca12532ab77f4850_720w.jpg" alt="img"></p><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p>PVC 只有绑定了 PV 之后才能被 Pod 使用，而 PVC 绑定 PV 的过程即是消费 PV 的过程，这个过程是有一定规则的，下面规则都满足的 PV 才能被 PVC 绑定：</p><ul><li>VolumeMode：被消费 PV 的 VolumeMode 需要和 PVC 一致；</li><li>AccessMode：被消费 PV 的 AccessMode 需要和 PVC 一致；</li><li>StorageClassName：如果 PVC 定义了此参数，PV 必须有相关的参数定义才能进行绑定；</li><li>LabelSelector：通过 label 匹配的方式从 PV 列表中选择合适的 PV 绑定；</li><li>storage：被消费 PV 的 capacity 必须大于或者等于 PVC 的存储容量需求才能被绑定。</li></ul><p>PVC模板：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">disk-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">20Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">test-disk</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure><p>PV模板：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">failure-domain.beta.kubernetes.io/region:</span> <span class="string">cn-zn</span></span><br><span class="line">    <span class="attr">failure-domain.beta.kubernetes.io/zone:</span> <span class="string">cn-zn</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">d-wz9g2j5qbo37r2lamkg4</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">30Gi</span></span><br><span class="line">  <span class="attr">flexVolume:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">alicloud/disk</span></span><br><span class="line">    <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">      <span class="attr">VolumeId:</span> <span class="string">d-wz9g2j5qbo37r2lamkg4</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">test-disk</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br></pre></td></tr></table></figure><h2 id="开源存储项目Ceph-amp-Rook"><a href="#开源存储项目Ceph-amp-Rook" class="headerlink" title="开源存储项目Ceph&Rook"></a>开源存储项目Ceph&amp;Rook</h2><p>围绕云原生技术的工具和项目正在大量涌现。作为生产中最突出的问题之一，有相当一部分开源项目致力于解决“在云原生架构上处理存储”这个问题。</p><p>目前最受欢迎的存储项目是<strong>Ceph</strong>和<strong>Rook</strong>。</p><p>Ceph是一个动态管理的、水平可伸缩的分布式存储集群。Ceph提供了对存储资源的逻辑抽象。它被设计成不存在单点故障、可自我管理和基于软件的。Ceph同时为相同的存储集群提供块、对象或文件系统接口。它能够提供非常稳定的块存储系统，并且K8S对Ceph放出了完整的生态，几乎可以说是全面兼容。</p><p>Ceph的架构非常复杂，有许多底层技术，如RADOS、librados、RADOSGW、RDB，它的CRUSH 算法和监视器、OSD和MDS等组件。这里不深入解读其架构，关键在于，Ceph是一个分布式存储集群，它可提供更高的可伸缩性，在不牺牲性能的情况下消除了单点故障，并提供了对对象、块和文件的访问的统一存储。</p><p><img src="https://pic3.zhimg.com/80/v2-0d1ff3204a1eba68463e9fc56b2b7e0e_720w.jpg" alt="img"></p><p>对于Rook，我们可以从以下几点来了解这个有趣的项目。它旨在聚合Kubernetes和Ceph的工具——将计算和存储放在一个集群中。</p><ul><li>Rook 是一个开源的cloud-native storage编排, 提供平台和框架；为各种存储解决方案提供平台、框架和支持，以便与云原生环境本地集成。</li><li>Rook 将存储软件转变为自我管理、自我扩展和自我修复的存储服务，它通过自动化部署、引导、配置、置备、扩展、升级、迁移、灾难恢复、监控和资源管理来实现此目的。</li><li>Rook 使用底层云本机容器管理、调度和编排平台提供的工具来实现它自身的功能。</li><li>Rook 目前支持Ceph、NFS、Minio Object Store和CockroachDB。</li><li>Rook使用Kubernetes原语使Ceph存储系统能够在Kubernetes上运行。</li></ul><p>所以在ROOK的帮助之下我们甚至可以做到一键编排部署Ceph，同时部署结束之后的运维工作ROOK也会介入自动进行实现对存储拓展，即便是集群出现了问题ROOK也能在一定程度上保证存储的高可用性，绝大多数情况之下甚至不需要Ceph的运维知识都可以正常使用。</p><h3 id="安装方法"><a href="#安装方法" class="headerlink" title="安装方法"></a>安装方法</h3><ol><li><strong>获取rook仓库：</strong><a href="https://github.com/rook/rook.git">https://github.com/rook/rook.git</a></li><li><strong>获取部署yaml文件</strong>，在rook仓库之中的<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/common.yaml">cluster/examples/kubernetes/ceph/common.yaml</a> 文件。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行common.yaml文件 </span></span><br><span class="line">kubectl create -f common.yaml</span><br></pre></td></tr></table></figure><p><strong>3. 安装operator，编排文件为<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/operator.yaml">/cluster/examples/kubernetes/ceph/operator.yaml</a></strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行operator.yaml文件</span></span><br><span class="line">kubectl create -f operator.yaml</span><br></pre></td></tr></table></figure><p><strong>4. 安装完成之后需要等待所有的操作器正常运行之后才能继续还是ceph分部署集群的安装：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取命名空间下运行的pod，等待所以的pod都是running状态之后继续下一步</span></span><br><span class="line">kubectl -n rook-ceph get pod</span><br></pre></td></tr></table></figure><p><strong>5. 创建Ceph集群，编排文件为<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/cluster.yaml">/cluster/examples/kubernetes/ceph/cluster.yaml</a></strong></p><p>这里也需要进行一定的基础配置与修改才能继续，cluster.yaml文件内容如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephCluster</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">cephVersion:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ceph/ceph:v14.2.6</span></span><br><span class="line">    <span class="attr">allowUnsupported:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">dataDirHostPath:</span> <span class="string">/var/lib/rook</span></span><br><span class="line">  <span class="attr">skipUpgradeChecks:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">continueUpgradeAfterChecksEvenIfNotHealthy:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">mon:</span></span><br><span class="line">    <span class="comment">#这里是最重要的，mon是存储集群的监控器，我们K8S里面有多少主机这里的就必须使用多少个mon</span></span><br><span class="line">    <span class="attr">count:</span> <span class="number">3</span></span><br><span class="line">    <span class="attr">allowMultiplePerNode:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">dashboard:</span></span><br><span class="line">   <span class="comment">#这里是是否启用监控面板，基本上都会使用 </span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment">#监控面板是否使用SSL，如果是使用8443端口，不是则使用7000端口，由于这是运维人员使用建议不启用</span></span><br><span class="line">    <span class="attr">ssl:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">monitoring:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">rulesNamespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">network:</span></span><br><span class="line">    <span class="attr">hostNetwork:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">rbdMirroring:</span></span><br><span class="line">    <span class="attr">workers:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">crashCollector:</span></span><br><span class="line">    <span class="attr">disable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="attr">removeOSDsIfOutAndSafeToRemove:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">storage:</span></span><br><span class="line">    <span class="attr">useAllNodes:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">useAllDevices:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">  <span class="attr">disruptionManagement:</span></span><br><span class="line">    <span class="attr">managePodBudgets:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">osdMaintenanceTimeout:</span> <span class="number">30</span></span><br><span class="line">    <span class="attr">manageMachineDisruptionBudgets:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">machineDisruptionBudgetNamespace:</span> <span class="string">openshift-machine-api</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行cluster.yaml文件</span></span><br><span class="line">kubectl create -f cluster.yaml</span><br></pre></td></tr></table></figure><p><strong>6. 创建ceph控制面板</strong></p><p>如果上面部署时，启用了SSL则需要使用<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml">/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml</a>，否则使用同目录下的<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/dashboard-external-http.yaml">dashboard-external-http.yaml</a>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#dashboard没有启用SSL</span></span><br><span class="line">kubectl create -f dashboard-external-http.yaml</span><br><span class="line"><span class="comment">#dashboard启用SSL</span></span><br><span class="line">kubectl create -f dashboard-external-https.yaml</span><br></pre></td></tr></table></figure><p><strong>7. 创建Ceph工具</strong></p><p>运维人员可以直接通过对这个容器的shell进行Ceph集群的控制（后面有实例），编排文件是<a href="https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/toolbox.yaml">toolbox.yaml</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装Ceph工具</span></span><br><span class="line">kubectl create -f toolbox.yaml</span><br></pre></td></tr></table></figure><p><strong>8，创建存储系统与存储类</strong></p><p>集群搭建完毕之后便是存储的创建，目前Ceph支持块存储、文件系统存储、对象存储三种方案，K8S官方对接的存储方案是块存储，他也是比较稳定的方案，但是块存储目前不支持多主机读写；文件系统存储是支持多主机存储的性能也不错；对象存储系统IO性能太差不考虑，所以可以根据要求自行决定。</p><p>存储系统创建完成之后对这个系统添加一个存储类之后整个集群才能通过K8S的存储类直接使用Ceph存储。</p><ul><li>块存储系统+块存储类yaml文件：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephBlockPool</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">replicapool</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">failureDomain:</span> <span class="string">host</span></span><br><span class="line">  <span class="attr">replicated:</span></span><br><span class="line">    <span class="attr">size:</span> <span class="number">3</span> <span class="comment">#这里的数字分部署数量，一样有几台主机便写入对应的值</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">rook-ceph-block</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rook-ceph.rbd.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">    <span class="attr">clusterID:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">replicapool</span></span><br><span class="line">    <span class="attr">imageFormat:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="attr">imageFeatures:</span> <span class="string">layering</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">rook-csi-rbd-provisioner</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">rook-csi-rbd-node</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">    <span class="attr">csi.storage.k8s.io/fstype:</span> <span class="string">xfs</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br></pre></td></tr></table></figure><ul><li>文件系统存储yaml文件：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">ceph.rook.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CephFilesystem</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myfs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">metadataPool:</span></span><br><span class="line">    <span class="attr">replicated:</span></span><br><span class="line">      <span class="attr">size:</span> <span class="number">3</span> <span class="comment">#这里的数字分部署数量，一样有几台主机便写入对应的值</span></span><br><span class="line">  <span class="attr">dataPools:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">replicated:</span> </span><br><span class="line">        <span class="attr">size:</span> <span class="number">3</span> <span class="comment">#这里的数字分部署数量，一样有几台主机便写入对应的值</span></span><br><span class="line">  <span class="attr">preservePoolsOnDelete:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">metadataServer:</span></span><br><span class="line">    <span class="attr">activeCount:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">activeStandby:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><ul><li>文件系统存储类yaml文件：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">csi-cephfs</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">rook-ceph.cephfs.csi.ceph.com</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">clusterID:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">fsName:</span> <span class="string">myfs</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">myfs-data0</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-name:</span> <span class="string">rook-csi-cephfs-provisioner</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/provisioner-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-name:</span> <span class="string">rook-csi-cephfs-node</span></span><br><span class="line">  <span class="attr">csi.storage.k8s.io/node-stage-secret-namespace:</span> <span class="string">rook-ceph</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Delete</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文通过介绍并图解K8s中各个存储方案实现方式以及可选择的开源项目，为读者呈现更全面的K8s存储方案选择。在我们实际的使用场景中，亦需要根据特定的需求来制定符合项目要求的存储方案，从而达到最好的实现效果。也希望有更多的朋友能够加入到kubernetes的队伍中来，让kubernetes真正深入到众多的用户和企业中去。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Docker" scheme="http://posts.hufeifei.cn/tags/Docker/"/>
    
    <category term="Kubernetes" scheme="http://posts.hufeifei.cn/tags/Kubernetes/"/>
    
    <category term="PersistentVolume" scheme="http://posts.hufeifei.cn/tags/PersistentVolume/"/>
    
  </entry>
  
  <entry>
    <title>亲历者说：Kubernetes API 与 Operator，不为人知的开发者战争</title>
    <link href="http://posts.hufeifei.cn/backend/kubernetes-operator/"/>
    <id>http://posts.hufeifei.cn/backend/kubernetes-operator/</id>
    <published>2021-11-18T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>如果我问你，如何把一个 etcd 集群部署在 Google Cloud 或者阿里云上，你一定会不假思索的给出答案：当然是用 etcd Operator！</p><p>实际上，几乎在一夜之间，Kubernetes Operator 这个新生事物，就成了开发和部署分布式应用的一项事实标准。时至今日，无论是 etcd、TiDB、Redis，还是 Kafka、RocketMQ、Spark、TensorFlow，几乎每一个你能叫上名字来的分布式项目，都由官方维护着各自的 Kubernetes Operator。而 Operator 官方库里，也一直维护着一个知名分布式项目的 Operator 汇总。</p><p><a href="https://github.com/operator-framework/awesome-operators">https://github.com/operator-framework/awesome-operators</a></p><p>短短一年多时间，这个列表的长度已经增长了几十倍。 </p><p><img src="https://yqfile.alicdn.com/ab73cf1b874993bbac6bb9b758b255415fe6c3ba.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="operator_jpeg"></p><p>而且更有意思的是，如果你仔细翻阅这个 Operator 列表，你就不难发现这样一个有趣的事实：现今 Kubernetes Operator 的意义，恐怕已经远远超过了“分布式应用部署”的这个原始的范畴，而已然成为了容器化时代应用开发与发布的一个全新途径。所以，你才会在这个列表里看到，Android SDK 的开发者们，正在使用 Operator “一键”生成和更新 Android 开发环境；而 Linux 系统工程师们，则在使用Operator “一键”重现性能测试集群。</p><p>如果说，Docker 镜像的提出，完成了应用静态描述的标准化。那么 Kubernetes Operator 的出现，终于为应用的动态描述提出了一套行之有效的实现规范。更为重要的是，对于 TiDB、Kafka、RocketMQ 等分布式应用的开发者来说，这些应用运行起来之后的动态描述，才是对一个分布式应用真正有意义的信息。</p><p>而在此之前，用户如果要想将 TiDB、Kafka 这样的分布式应用很好的使用起来，就不得不去尝试编写一套复杂的管理脚本，甚至为此学习大量与项目本身无关的运维知识。更为麻烦的是，这些脚本、知识、和经验，并没有一个很好的办法能够有效的沉淀下来。而任何一种技术的传授，如果严重依赖于口口相传而不是固化的代码和逻辑的话，那么它的维护成本和使用门槛，就可以说是“灾难级”的。</p><p>所以说，Kubernetes Operator 发布之初最大的意义，就在于它将分布式应用的使用门槛直接降到了最低。</p><p>那么这个门槛具体有多低呢？</p><p>一般来说，无论这个分布式应用项目有多复杂，只要它为用户提供了 Operator，那么这个项目的使用就只需要两条命令即可搞定，以 Kafka 为例：</p><p><img src="https://yqfile.alicdn.com/27e507eee278577a09671b5b7848b662d667a245.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="kafka_jpeg"></p><p>这两条命令执行完成后，一个 Kafka 集群运行所需的节点，以及它们所依赖的 ZooKeeper 节点，就会以容器的方式自动出现在你的 Kubernetes 集群里了。</p><p>不过，简化运维和部署，其实只是 Operator 在用户层面的表象。而在更底层的技术层面，Operator 最大的价值，在于它为“容器究竟能不能管理有状态应用”这个颇具争议话题，画上了一个优雅的句号。</p><p>要知道，在2014-2015年的时候，伴随着 Docker 公司和 Docker 项目的走红，整个云计算生态几乎都陷入了名为“容器”的狂热当中。然而，相比于 “容器化”浪潮的如火如荼，这个圈子却始终对“有状态应用”讳莫如深。</p><p>事实上，有状态应用（比如， 前面提到的Kafka ）跟无状态应用（比如，一个简单的Jave Web网站）的不同之处，就在于前者对某些外部资源有着绑定性的依赖，比如远程存储，或者网络设备，以及，有状态应用的多个示例之间往往有着拓扑关系。这两种设计，在软件工程的世界里可以说再普通不过了，而且我们几乎可以下这样一个结论：所有的分布式应用都是有状态应用。</p><p>但是，在容器的世界里，分布式应用却成了一个“异类”。我们知道，容器的本质，其实就是一个被限制了“世界观”的进程。在这种隔离和限制的大基调下，容器技术本身的“人格基因”，就是对外部世界（即：宿主机）的“视而不见”和“充耳不闻”。所以我们经常说，容器的“状态”一定是“易失”的。其实，容器对它的“小世界”之外的状态和数据漠不关心，正是这种“隔离性”的主要体现。</p><p>但状态“易失”并不能说是容器的缺陷：我们既然对容器可以重现完整的应用执行环境的“一致性”拍手称赞，那就必然要对这种能力背后的限制了然于心。这种默契，也正是早期的 Docker 公司所向披靡的重要背景：在这个阶段，相比于“容器化”的巨大吸引力，开发者是可以暂时接受一部分应用不能运行在容器里的。</p><p>而分布式应用容器化的困境，其实就在于它成为了这种“容器化”默契的“终极破坏者”。</p><p>一个应用本身可以拥有多个可扩展的实例，这本来是容器化应用令人津津乐道的一个优势。但是一旦这些实例像分布式应用这样具有了拓扑关系，以及，这些实例本身不完全等价的时候，容器化的解决方案就再次变得“丑陋”起来：这种情况下，应用开发者们不仅又要为这些容器实例编写一套难以维护的管理脚本，还必须要想办法应对容器重启后状态丢失的难题。而这些容器状态的维护，实际上往往需要打破容器的隔离性、让容器对外部世界有所感知才能做到，这就使得容器化与有状态，成为了两种完全相悖的需求。</p><p>不过，从上面的叙述中相信你也应该已经察觉到，分布式应用容器化的难点，并不在于容器本身有什么重大缺陷，而在于我们一直以来缺乏一种对“状态”的合理的抽象与描述，使得状态可以和容器进程本身解耦开来。这也就解释了为什么，在 Kubernetes 这样的外部编排框架逐渐成熟起了之后，业界才逐渐对有状态应用管理开始有了比较清晰的理解和认识。</p><p>而我们知道， Kubernetes 项目最具价值的理念，就是它围绕 etcd 构建出来的一套“面向终态”编排体系，这套体系在开源社区里，就是大名鼎鼎的“声明式 API”。</p><p>“声明式 API”的核心原理，就是当用户向 Kubernetes 提交了一个 API 对象的描述之后，Kubernetes 会负责为你保证整个集群里各项资源的状态，都与你的 API 对象描述的需求相一致。更重要的是，这个保证是一项“无条件的”、“没有期限”的承诺：对于每个保存在 etcd 里的 API 对象，Kubernetes 都通过启动一种叫做“控制器模式”（Controller Pattern）的无限循环，不断检查，然后调谐，最后确保整个集群的状态与这个 API 对象的描述一致。</p><p><img src="https://yqfile.alicdn.com/b1ea243556d0471472eb818cfa6d48c17bcac892.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="_jpeg"></p><p>比如，你提交的 API 对象是一个应用，描述的是这个应用必须有三个实例，那么无论接下来你的 API 对象发生任何“风吹草动”，控制器都会检查一遍这个集群里是不是真的有三个应用实例在运行。并且，它会根据这次检查的结果来决定，是不是需要对集群做某些操作来完成这次“调谐”过程。当然，这里控制器正是依靠 etcd 的 Watch API 来实现对 API 对象变化的感知的。在整个过程中，你提交的 API 对象就是 Kubernetes 控制器眼中的“金科玉律”，是接下来控制器执行调谐逻辑要达到的唯一状态。这就是我们所说的“终态”的含义。</p><p>而 Operator 的设计，其实就是把这个“控制器”模式的思想，贯彻的更加彻底。在 Operator 里，你提交的 API 对象不再是一个单体应用的描述，而是一个完整的分布式应用集群的描述。这里的区别在于，整个分布式应用集群的状态和定义，都成了Kubernetes 控制器需要保证的“终态”。比如，这个应用有几个实例，实例间的关系如何处理，实例需要把数据存储在哪里，如何对实例数据进行备份和恢复，都是这个控制器需要根据 API 对象的变化进行处理的逻辑。</p><p>从上述叙述中，你就应该能够明白， Operator 其实就是一段代码，这段代码 Watch 了 etcd 里一个描述分布式应用集群的API 对象，然后这段代码通过实现 Kubernetes 的控制器模式，来保证这个集群始终跟用户的定义完全相同。而在这个过程中，Operator 也有能力利用 Kubernetes 的存储、网络插件等外部资源，协同的为应用状态的保持提供帮助。</p><p>所以说，Operator 本身在实现上，其实是在 Kubernetes 声明式 API 基础上的一种“微创新”。它合理的利用了 Kubernetes API 可以添加自定义 API 类型的能力，然后又巧妙的通过 Kubernetes 原生的“控制器模式”，完成了一个面向分布式应用终态的调谐过程。</p><p>而 Operator 本身在用法上，则是一个需要用户大量编写代码的的开发者工具。 不过，这个编写代码的过程，并没有像很多人当初料想的那样导致 Operator 项目走向小众，反而在短短三年的时间里， Operator 就迅速成为了容器化分布式应用管理的事实标准。时至今日，Operator 项目的生态地位已经毋庸置疑。就在刚刚结束的2018年 KubeCon 北美峰会上，Operator 项目和大量的用户案例一次又一次出现在聚光灯前，不断的印证着这个小小的“微创新”对整个云计算社区所产生的深远影响。</p><p>不过，在 Operator 项目引人瞩目的成长经历背后，你是否考虑过这样一个问题：</p><p>Kubernetes 项目一直以来，其实都内置着一个管理有状态应用的能力叫作 StatefulSet。而如果你稍微了解 Kubernetes 项目的话就不难发现，Operator 和 StatefulSet，虽然在对应用状态的抽象上有所不同，但它们的设计原理，几乎是完全一致的，即：这两种机制的本质，都是围绕Kubernetes API 对象的“终态”进行调谐的一个控制器（Controller）而已。</p><p>可是，为什么在一个开源社区里，会同时存在这样的两个核心原理完全一致、设计目标也几乎相同的有状态应用管理方案呢？作为 CoreOS 公司后来广为人知的“左膀右臂”之一（即：etcd 和 Operator），Operator 项目能够在 Kubernetes 生态里争取到今天的位置，是不是也是 CoreOS 公司的开源战略使然呢？</p><p>事实上，Operator 项目并没有像很多人想象的那样出生就含着金钥匙。只不过，在当时的确没有人能想到，当 CoreOS 的两名工程师带着一个业余项目从一间平淡无奇的公寓走出后不久，一场围绕着 Kubernetes API 生态、以争夺“分布式应用开发者”为核心的的重量级角逐，就徐徐拉开了序幕。</p><hr><p>2016 年秋天，原 CoreOS 公司的工程师邓洪超像往常一样，来到了同事位于福斯特城（Foster City）的公寓进行结对编程。每周四相约在这里结对，是这两位工程师多年来约定俗成的惯例。</p><p><img src="https://yqfile.alicdn.com/b9c153d0d3fe06a0fe6fe46452764568a7c191a3.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="CoreOS 当年开发 etcd 所在的车库"><br>不过，与以往不同的是，相比于往常天马行空般的头脑风暴，这一次，这两位工程师的脑子里正在琢磨着的，是一个非常“接地气”的小项目。</p><p>我们知道，Kubernetes 项目实现“容器编排”的核心，在于一个叫做“控制器模式”的机制，即：通过对 etcd 里的 API 对象的变化进行监视（Watch），Kubernetes 项目就可以在一个叫做 Controller 的组件里对这些变化进行响应。而无论是 Pod 等应用对象，还是 iptables、存储设备等服务对象，任何一个 API 对象发生变化，那么 Kubernetes 接下来需要执行的响应逻辑，就是对应的 Controller 里定义的编排动作。</p><p>所以，一个自然而然的想法就是，作为 Kubernetes 项目的用户，我能不能自己编写一个 Controller 来定义我所期望的编排动作呢？比如：当一个 Pod 对象被更新的时候，我的 Controller 可以在“原地”对 Pod 进行“重启”，而不是像 Deployment 那样必须先删除 Pod，然后再创建 Pod。</p><p>这个想法，其实是很多应用开发者以及 PaaS 用户的强烈需求，也是一直以来萦绕在 CoreOS 公司 CEO Alex Polvi 脑海里的一个念头。而在一次简单的内部讨论提及之后，这个念头很快就激发出了两位工程师的技术灵感，成为了周四结对编程的新主题。<br>而这一次，他们决定把这个小项目，起名叫做：Operator。</p><p>所以顾名思义，Operator 这个项目最开始的初衷，是用来帮助开发者实现运维（Operate）能力的。但 Operator 的核心思想，却并不是“替开发者做运维工作”，而是“让开发者自己编写运维工具”。更有意思的是，这个运维工具的编写标准，或者说，编写 Operator 代码可以参考的模板，正是 Kubernetes 的“控制器模式（Controller Pattern）”。</p><p>前面已经说过， Kubernetes 的“控制器模式”，是围绕着比如 Pod 这样的 API 对象，在 Controller 通过响应它的增删改查来定义对 Pod 的编排动作。</p><p>而 Operator 的设计思路，就是允许开发者在 Kubernetes 里添加一个新的 API 对象，用来描述一个分布式应用的集群。然后，在这个 API 对象的 Controller 里，开发者就可以定义对这个分布式应用集群的运维动作了。</p><p>举个例子， 假设下面这个 YAML 文件定义的，是一个 3 节点 etcd 集群的描述：</p><p><img src="https://yqfile.alicdn.com/f54a2d1676c87883fb7fdc465026328ead4b13c1.jpeg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="yaml_jpeg"></p><p>有了这样一个 etcdCluster 对象，那么开发者接下来要做的事情，就是编写一个 etcdCluster Controller，使得当任何用户提交这样一个 YAML 文件给 Kubernetes 之后，我们自己编写的 Controller 就会响应 etcdCluster “增加”事件，为用户创建出 3 个节点的 etcd 集群出来。然后，它还会按照我们在 Controller 编写的事件响应逻辑，自动的对这个集群的节点更新、删除等事件做出处理，执行我定义的其他运维功能。像这样一个 etcdCluster Controller，就是 etcd Operator 的核心组成部分了。</p><p>而作为 etcd 的开发者，CoreOS 的两位工程师把对 etcd 集群的运维工作编写成 Go 语言代码，一点都不困难。可是，要完成这个 Operator 真正困难在于：Kubernetes 只认识 Pod、Node、Service 等这些 Kubernetes 自己原生的 API 对象，它怎么可能认识开发者自己定义的这个 etcdCluster 对象呢？</p><p>在当时， Kubernetes 项目允许用户自己添加 API 对象的插件能力，叫做 Third Party Resource，简称：TPR。</p><p>TPR 允许你提交一个 YAML 文件，来定义你想要的的新 API 对象的名字，比如：etcdCluster；也允许你定义这个对象允许的合法的属性，比如：int 格式的 size 字段， string 格式的 version 字段。然后，你就可以提交一个具体的 etcdCluster 对象的描述文件给 Kubernetes，等待该对应的 Controller 进行处理。<br>而这个 Controller，就是 Operator 的主干代码了。</p><p>所以接下来，CoreOS 的两位工程师轻车熟路，在 Operator 里对 etcdCluster 对象的增、删、改事件的响应位置，写上了创建、删除、更新 etcd 节点的操作逻辑。然后，调试运行，看着一个 etcd 集群按照 YAML 文件里的描述被创建起来。大功告成！</p><p>就这样，在一个普通的周四下午，世界上第一个 Operator 诞生在了湾区的一所公寓当中。</p><p>而对于 CoreOS 的两位工程师来说，编写这个小工具的主要目的，就是借助 Kubernetes 的核心原理来自动化的管理 etcd 集群，更重要的是，不需要使用 Kubernetes 里自带的 StatefulSet。</p><p>你可能已经知道，Kubernetes 里本身就内置了一个叫做 StatefulSet 的功能，是专门用来管理有状态应用的。而 StatefulSet 的核心原理，其实是对分布式应用的两种状态进行了保持：</p><ul><li>分布式应用的拓扑状态，或者说，节点之间的启动顺序；</li><li>分布式应用的存储状态，或者说，每个节点依赖的持久化数据。</li></ul><p>可是，为了能够实现上述两种状态的保持机制，StatefulSet 的设计就给应用开发者带来了额外的束缚。</p><p>比如，etcd 集群各节点之间的拓扑关系，并不依赖于节点名字或者角色（比如 Master 或者 Slave）来确定，而是记录在每个 etcd 节点的启动参数当中。这使得 StatefulSet 通过“为节点分配有序的 DNS 名字”的拓扑保持方式，实际上没有了用武之地，反而还得要求开发者在节点的启动命令里添加大量的逻辑来生成正确的启动命令，非常不优雅。类似的，对于存储状态来说，etcd 集群对数据的备份和恢复方法，也跟 StatefulSet 依赖的的远程持久化数据卷方案并没有太大关系。</p><p>不难看到， StatefulSet 其实比较适用于应用本身节点管理能力不完善的项目，比如 MySQL。而对于 etcd 这种已经借助 Raft 实现了自管理的分布式应用来说， StatefulSet 的使用方法和带来的各种限制，其实是非常别扭的。</p><p>而带着工程师特有的较真儿精神，邓洪超和他的同事借助 Kubernetes 原生的扩展机制实现的，正是一个比 StatefulSet 更加灵活、能够把控制权重新交还给开发者的分布式应用管理工具。他们把这个工具起名叫做 Operator，并在几个月后的 KubeCon 上</p><p>进行了一次 Demo ，推荐大家尝试使用 Operator 来部署 etcd 集群。</p><p>没有人能想到的是，这个当时还处于 PoC 状态的小项目一经公布，就立刻激发起了整个社区的模仿和学习的热潮。</p><p>很快，大量的应用开发者纷纷涌进 Kubernetes 社区，争先恐后的宣布自己的分布式项目可以通过 Operator 运行起来。而敏锐的公有云提供商们很快看出了这其中的端倪：Operator 这个小框架，已然成为了分布式应用和有状态应用“上云”的必经之路。Prometheus，Rook，伴随着越来越多的、以往在容器里运行起来困难重重的应用，通过 Operator 走上了 Kubernetes 之后，Kubernetes 项目第一次出现在了开发者生态的核心位置。这个局面，已经远远超出了邓洪超甚至 CoreOS 公司自己的预期。<br>更重要的是，不同于 StatefulSet 等 Kubernetes 原生的编排概念，Operator 依赖的 Kubernetes 能力，只有最核心的声明式 API 与控制器模式；Operator 具体的实现逻辑，则编写在自定义 Controller 的代码中。这种设计给开发者赋予了极高的自由度，这在整个云计算和 PaaS 领域的发展过程中，都是非常罕见的。</p><p>此外，相比于 Helm、Docker Compose 等描述应用静态关系的编排工具，Operator 定义的乃是应用运行起来后整个集群的动态逻辑。得益于 Kubernetes 项目良好的声明式 API 的设计和开发者友好的 API 编程范式，Operator 在保证上述自由度的同时，又可以始终如一的展现出清晰的架构和设计逻辑，使得应用的开发者们，可以通过复制粘贴就快速搭建出一个 Operator 的框架，然后专注于填写自己的业务逻辑。</p><p>在向来讲究“用脚投票”的开发者生态当中，Operator 这样一个编程友好、架构清晰、方便代码复制粘贴的小工具，本身就已经具备了某些成功的特质。</p><p>然而，Operator 的意外走红，并没有让 CoreOS 公司“一夜成名”，反而差点将这个初出茅庐的项目，扼杀在萌芽状态。<br>在当时的 Kubernetes 社区里，跟应用开发者打交道并不是一个非常常见的事情。而 Operator 项目的诞生，却把 Kubernetes 项目第一次拉近到了开发者的面前，这让整个社区感觉了不适应。而作为 Kubernetes 项目 API 治理的负责人，Google 团队对这种冲突的感受最为明显。</p><p>对于 Google 团队来说，Controller 以及控制器模式，应该是一个隐藏在 Kubernetes 内部实现里的核心机制，并不适合直接开放给开发者来使用。退一步说，即使开放出去，这个 Controller 的设计和用法，也应该按照 Kubernetes 现有的 API 层规范来进行，最好能成为 Kubernetes 内置 Controller Manager 管理下的一部分。可是， Operator 却把直接编写 Controller 代码的自由度完全交给了开发者，成为了一个游离于 Kubernetes Controller Manager 之外的外部组件。</p><p>带着这个想法，社区里的很多团队从 Operator 项目诞生一开始，就对它的设计和演进方向提出了质疑，甚至建议将 Operator 的名字修改为 Custom Kubernetes Controller。而无巧不成书，就在 Google 和 CoreOS 在 Controller 的话语权上争执不下的时候， Kubernetes 项目的发起人之一 Brendan Burns 突然宣布加入了微软，这让 Google 团队和 Operator 项目的关系一下子跌倒了冰点。</p><p>你可能会有些困惑：Brendan Burns 与 Kubernetes 的关系我是清楚的，但这跟 Operator 又有什么瓜葛吗？</p><p>实际上，你可能很难想到，Brendan Burns 和他的团队，才是 TPR （Third Party Resource）这个特性最初的发起人。<br>所以，几乎在一夜之间，Operator 项目链路上的每一个环节，都与 Google 团队完美的擦肩而过。眼睁睁的看着这个正冉冉升起的开发者工具突然就跟自己完全没了关系，这个中滋味，确实不太好受。</p><p>于是，在 2017年初，Google 团队和 RedHat 公司开始主动在社区推广 UAS（User Aggregated APIServer），也就是后来 APIServer Aggregator 的雏形。APIServer Aggregator 的设计思路是允许用户编写一个自定义的 APIServer，在这里面添加自定义 API。然后，这个 APIServer 就可以跟 Kubernetes 原生的 APIServer 绑定部署在一起统一提供服务了。不难看到，这个设计与 Google 团队认为自定义 API 必须在 Kubernetes 现有框架下进行管理的想法还是比较一致的。</p><p>紧接着，RedHat 和 Google 联盟开始游说社区使用 UAS 机制取代 TPR，并且建议直接从 Kubernetes 项目里废弃 TPR 这个功能。一时间，社区里谣言四起，不少已经通过 TPR 实现的项目，也开始转而使用 UAS 来重构以求自保。 而 Operator 这个严重依赖于 TPR 的小项目，还没来得及发展壮大，就被推向了关闭的边缘。</p><p>面对几乎要与社区背道而驰的困境，CoreOS 公司的 CTO Brandon Philips 做出了一个大胆的决定：让社区里的所有开发者发声，挽救 TPR 和 Operator。</p><p>2017 年 2月，Brandon Philips 在 GitHub 上开了一个帖子（Gist）， 号召所有使用 TPR 或者 Operator 项目的开发者在这里留下的自己的项目链接或者描述。这个帖子，迅速的成为了当年容器技术圈最热门的事件之一，登上了 HackerNews 的头条。有趣的是，这个帖子直到今天也仍然健在，甚至还在被更新，你可以点击这个链接去感受一下当时的盛况。<a href="https://gist.github.com/philips/a97a143546c87b86b870a82a753db14c">https://gist.github.com/philips/a97a143546c87b86b870a82a753db14c</a></p><p>而伴随着 Kubernetes 项目的迅速崛起，短短一年时间不到，夹缝中求生存的 Operator 项目，开始对公有云市场产生了不可逆转的影响，也逐步改变了开发者们对“云”以及云上应用开发模式的基本认知。甚至就连 Google Cloud 自己最大的客户之一 Snapchat ，也成为了 Operator 项目的忠实用户。在来自社区的巨大压力下，在这个由成千上万开发者们自发维护起来的 Operator 生态面前，Google 和 RedHat 公司最终选择了反省和退让。</p><p>有意思的是，这个退让的结果，再一次为这次闹剧增添了几分戏剧性。</p><p>就在 Brandon Phillips 的开发者搜集帖发布了不到三个月后，RedHat 和 Google 公司的工程师突然在 Kubernetes 社区里宣布：TPR 即将被废弃，取而代之的是一个名叫 CRD，Custom Resource Definition 的东西。</p><p>于是，开发者们开始忧心忡忡的按照文档，将原本使用 TPR 的代码都升级成 CRD。而就在这时，他们却惊奇的发现，这两种机制除了名字之外，好像并没有任何不同。所谓的升级工作，其实就是将代码里的 TPR 字样全局替换成 CRD 而已。</p><p>难道，这只是虚惊一场？</p><p>其实，很少有人注意到，在 TPR 被替换成 CRD 之后，Brendan Burns 和微软团队就再也没有出现在“自定义 API”这个至关重要的领域里了。而 CRD 现在的负责人，都是来自 Google 和 RedHat 的工程师。</p><p>在这次升级事件之后不久，CoreOS 公司在它的官方网站上发布了一篇叫做：TPR Is Dead! Kubernetes 1.7 Turns to CRD 的博客（<a href="https://coreos.com/blog/custom-resource-kubernetes-v17%EF%BC%89%EF%BC%8C%E6%97%A8%E5%9C%A8%E6%8C%87%E5%AF%BC%E7%94%A8%E6%88%B7%E4%BB%8E">https://coreos.com/blog/custom-resource-kubernetes-v17），旨在指导用户从</a> TRP 升级成 CRD。不过，现在回头再看一眼这篇文章，平淡无奇的讲述背后，你能否感受到当年这场“开发者战争”的蛛丝马迹呢？</p><p>其实，Operator 并不平坦的晋级之路，只是 Kubernetes API 生态风起云涌的冰山一角。几乎在每个星期，甚至每一天，都有太多围绕着 Kubernetes 开发者生态的角逐，在这个无比繁荣的社区背后，以不为人知的方式开始或者谢幕。</p><p>而这一切纷争的根本原因却无比直白。Kubernetes 项目，已经被广泛认可为云计算时代应用开发者们的终端入口。这正是为何，无论是 Google、微软，还是 CoreOS 以及 Heptio，所有这个生态里的大小玩家，都在不遗余力的在 Kubernetes API 层上捍卫着自己的话语权，以期在这个未来云时代的开发者入口上，争取到自己的一席之地。</p><p>而在完成了对收 CoreOS 的收购之后，RedHat 终于在这一领域拿到了可以跟 Google 和微软一较高低的关键位置。2018年，RedHat 不失时机的发布了 Operator Framework，希望通过 Operator 周边工具和生态的进一步完善，把 Operator 确立成为分布式应用开发与管理的关键依赖。而伴随着 Operator 越来越多的介入到应用开发和部署流程之后， Kubernetes API 一定会继续向上演进，进一步影响开发者的认知和编程习惯。这，已经成为了云计算生态继续发展下去的必然趋势。</p><p>而作为这个趋势坚定不移的贯彻者，无论是 Istio，还是 Knative，都在用同样的经历告诉我们这样的道理：只有构建在 Kubernetes 这个云时代基础设施事实标准上的开发者工具，才有可能成为下一个开发者领域的 “Operator” 。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;如果我问你，如何把一个 etcd 集群部署在 Google Cloud 或者</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Kubernetes" scheme="http://posts.hufeifei.cn/tags/Kubernetes/"/>
    
    <category term="Operator" scheme="http://posts.hufeifei.cn/tags/Operator/"/>
    
  </entry>
  
  <entry>
    <title>npm 如何处理依赖与依赖冲突</title>
    <link href="http://posts.hufeifei.cn/frontend/how-npm-handles-dependency-version-conflict/"/>
    <id>http://posts.hufeifei.cn/frontend/how-npm-handles-dependency-version-conflict/</id>
    <published>2021-11-04T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="依赖地狱"><a href="#依赖地狱" class="headerlink" title="依赖地狱"></a>依赖地狱</h2><p>早期版本的的 npm (v2) 管理模块依赖的方式并不复杂。它读取每个模块的依赖列表，并下载匹配版本的依赖模块到该模块目录内的 <code>node_modules</code> 文件夹下；如果该依赖又依赖了其他的模块，会继续下载该依赖的依赖到该模块目录的 <code>node_modules</code> 文件夹下——如此递归执行下去，最终形成一颗庞大的依赖树。</p><p>例如，当前项目有依赖的模块 <code>A</code>, <code>B</code>, <code>A</code> 又依赖于模块 <code>C</code>, <code>D</code>, <code>B</code> 又依赖于模块 <code>C</code>, <code>E</code>，此时，项目的 <code>node_modules</code> 目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A</span><br><span class="line">    │   └── node_modules</span><br><span class="line">    │       ├── C</span><br><span class="line">    │       └── D</span><br><span class="line">    └── B</span><br><span class="line">        └── node_modules</span><br><span class="line">            ├── C</span><br><span class="line">            └── E</span><br></pre></td></tr></table></figure><p>可以想象，这样做的确能尽量保证每个模块自身的可用性。但是，当项目规模达到一定程度时，也会造成许多问题：</p><ol><li>依赖树的层级非常深。如果需要定位某依赖的依赖，很难找到该依赖的文件所在（例如，如果想定位模块 <code>E</code>，就不得不先知道他在依赖树中的位置）；</li><li>不同的依赖树分支里，可能有大量实际上是同样版本的依赖（例如，<code>A</code> 目录下的 <code>C</code> 和 <code>B</code> 目录下面的 <code>C</code> 如果版本一致，实际上完全一样）；</li><li>安装时额外下载或拷贝了大量重复的资源，并且实际上也占用了大量的硬盘空间资源等（例如，<code>C</code> 模块在依赖目录中出现了两次）；</li><li>安装速度慢，甚至因为目录层级太深导致文件路径太长的缘故，在 windows 系统下删除 <code>node_modules</code> 文件夹也可能失败！</li></ol><p>正是因为这些问题的存在，彼时的 <code>node_modules</code> 又被叫做<code>依赖地狱(Dependency Hell)</code>。</p><h2 id="依赖共享与冲突"><a href="#依赖共享与冲突" class="headerlink" title="依赖共享与冲突"></a>依赖共享与冲突</h2><p>在 npm v3 版本之后，npm 采用了更合理的方式去解决之前的依赖地狱的问题。npm v3 尝试把依赖以及依赖的依赖都尽量的平铺在项目根目录下的 <code>node_modules</code> 文件夹下以<strong>共享使用</strong>；如果遇到因为需要的版本要求不一致导致冲突，没办法放在平铺目录下的，回退到 npm v2 的处理方式，在该模块下的 <code>node_modules</code> 里存放冲突的模块。</p><p>例如，当前项目有依赖的模块 <code>A@1.0.0</code>, <code>B@1.0.0</code>, <code>A@1.0.0</code> 依赖于模块 <code>C@1.0.0</code>, <code>D@0.6.5</code>, <code>B@1.0.0</code> 又依赖于模块 <code>C@2.0.0</code>, <code>E@1.0.3</code>。注意，此时由于模块 <code>C</code> 的两个版本 <code>C@1.0.0</code> 和 <code>C@2.0.0</code> 被分别依赖，鉴于模块在同一个 <code>node_modules</code> 目录中是按照模块名目录存放，因此这两个版本没办法同时平铺在同一目录，因此，其中一个版本的 <code>C</code> 模块将会以 npm v2 的处理方式放入子 <code>node_modules</code> 目录中。</p><p>那么，应该是哪一个版本的 <code>C</code> 会被这样处理呢？考虑以下操作时序：</p><ol><li><p>在空目录下，通过 <code>npm install --save A@1.0.0</code> 先安装 <code>A</code>。由于它和它的依赖在 <code>node_modules</code> 下都不会产生冲突，因此能够直接平铺的放入其中。此时目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A@1.0.0</span><br><span class="line">    ├── C@1.0.0</span><br><span class="line">    └── D@0.6.5</span><br></pre></td></tr></table></figure></li><li><p>继续通过 <code>npm install --save B@1.0.0</code> 安装 <code>B</code>。<code>B</code> 自身以及它的依赖 <code>E</code> 也没有冲突，直接平铺放入 <code>node_modules</code> 下；但是 <code>B</code> 的另一依赖 <code>C@2.0.0</code> 因为 <code>C@1.0.0</code> 已经存在了，出现了版本冲突，它将不得不被放置于 <code>B</code> 目录下的 <code>node_modules</code> 中。此时目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A@1.0.0</span><br><span class="line">    ├── B@1.0.0</span><br><span class="line">    │   └── node_modules</span><br><span class="line">    │       └── C@2.0.0</span><br><span class="line">    ├── C@1.0.0</span><br><span class="line">    ├── D@0.6.5</span><br><span class="line">    └── E@1.0.3</span><br></pre></td></tr></table></figure><p>通过以上分析可知，如果先安装 <code>B</code> 再安装 <code>A</code>，<code>C@1.0.0</code> 将位于 <code>A</code> 目录下的 <code>node_modules</code> 中。这说明：<strong>模块的安装顺序可能影响 <code>node_modules</code> 内的文件结构</strong>。</p></li><li><p>在上面的先 <code>A</code> 后 <code>B</code> 的情形下，继续安装依赖 <code>F@1.0.0</code>，它拥有依赖 <code>C@2.0.0</code> 和 <code>G@1.0.0</code>。类似的，它的依赖 <code>C@2.0.0</code> 因为版本冲突，不得不被放置于 <code>F</code> 的 <code>node_modules</code> 中。此时目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A@1.0.0</span><br><span class="line">    ├── B@1.0.0</span><br><span class="line">    │   └── node_modules</span><br><span class="line">    │       └── C@2.0.0</span><br><span class="line">    ├── C@1.0.0</span><br><span class="line">    ├── D@0.6.5</span><br><span class="line">    ├── E@1.0.3</span><br><span class="line">    └── F@1.0.0</span><br><span class="line">        └── node_modules</span><br><span class="line">            └── C@2.0.0</span><br></pre></td></tr></table></figure><p>观察发现，模块 <code>C@2.0.0</code> 还是出现了冗余。然而，假如安装的顺序是 <code>B</code> <code>A</code> <code>F</code>，可以想象，将不会出现模块冗余的情况。这说明：<strong>模块安装顺序可能影响 <code>node_modules</code> 内的文件数量</strong>。</p></li><li><p>假设模块 <code>A</code> 的新版本 <code>A@2.0.0</code>，它不再依赖 <code>C@1.0.0</code> 而是 <code>C@2.0.0</code>， 现在在以上项目中执行 <code>npm install A@2</code>，将会发生以下操作：</p><ul><li>移除模块 <code>A@1.0.0</code>；</li><li>移除模块 <code>C@1.0.0</code>，因为没有其他的模块依赖它；</li><li>添加模块 <code>A@2.0.0</code>；</li><li>在顶层 <code>node_modules</code> 中安装模块 <code>C@2.0.0</code>，因为顶层目录中没有版本冲突发生。</li></ul><p>此时的目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A@2.0.0</span><br><span class="line">    ├── B@1.0.0</span><br><span class="line">    │   └── node_modules</span><br><span class="line">    │       └── C@2.0.0</span><br><span class="line">    ├── C@2.0.0</span><br><span class="line">    ├── D@0.6.5</span><br><span class="line">    ├── E@1.0.3</span><br><span class="line">    └── F@1.0.0</span><br><span class="line">        └── node_modules</span><br><span class="line">            └── C@2.0.0</span><br></pre></td></tr></table></figure><p>可以发现，目录中冗余了多个 <code>C@2.0.0</code> 模块！所幸 npm 提供了一个单独的命令 <code>npm dedupe</code> 用以去掉类似情况下产生的冗余拷贝。在 dedupe 之后，目录结构如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">└── node_modules</span><br><span class="line">    ├── A@2.0.0</span><br><span class="line">    ├── B@1.0.0</span><br><span class="line">    ├── C@2.0.0</span><br><span class="line">    ├── D@0.6.5</span><br><span class="line">    ├── E@1.0.3</span><br><span class="line">    └── F@1.0.0</span><br></pre></td></tr></table></figure></li></ol><p>顺便提一句：<code>yarn</code> 在安装依赖时会自动执行 <code>dedupe</code> 操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> yarn dedupe</span></span><br><span class="line">yarn dedupe v1.17.3</span><br><span class="line">error The dedupe command isn&#x27;t necessary. `yarn install` will already dedupe.</span><br><span class="line">info Visit https://yarnpkg.com/en/docs/cli/dedupe for documentation about this command.</span><br></pre></td></tr></table></figure><p>可见 yarn 在设计时得确是抓住了很多细小的点去改善使用体验。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://npm.github.io/how-npm-works-docs/index.html">https://npm.github.io/how-npm-works-docs/index.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;依赖地狱&quot;&gt;&lt;a href=&quot;#依赖地狱&quot; class=&quot;head</summary>
      
    
    
    
    <category term="前端" scheme="http://posts.hufeifei.cn/categories/%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="node.js" scheme="http://posts.hufeifei.cn/tags/node-js/"/>
    
    <category term="npm" scheme="http://posts.hufeifei.cn/tags/npm/"/>
    
  </entry>
  
  <entry>
    <title>Nginx为什么快到根本停不下来？</title>
    <link href="http://posts.hufeifei.cn/backend/nginx/"/>
    <id>http://posts.hufeifei.cn/backend/nginx/</id>
    <published>2021-11-01T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><h3 id="Nginx-的进程模型"><a href="#Nginx-的进程模型" class="headerlink" title="Nginx 的进程模型"></a>Nginx 的进程模型</h3><p><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/19/1705d5bae51f9935~tplv-t2oaga2asx-watermark.awebp" alt="img"></p><p>Nginx 服务器，正常运行过程中：</p><ol><li><p><strong>多进程</strong>：一个 Master 进程、多个 Worker 进程</p></li><li><p>Master 进程：管理 Worker 进程</p><ol><li>对外接口：接收<code>外部的操作</code>（信号）</li><li>对内转发：根据<code>外部的操作</code>的不同，通过<code>信号</code>管理 Worker</li><li>监控：监控 worker 进程的运行状态，worker 进程异常终止后，自动重启 worker 进程</li></ol></li><li><p>Worker 进程：所有 Worker 进程都是平等的</p><ol><li>实际处理：网络请求，由 Worker 进程处理；</li></ol></li><li><p>Worker 进程数量：在 nginx.conf 中配置，一般设置为<code>核心数</code>，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。</p></li></ol><p>思考：</p><ol><li>请求是连接到 Nginx，Master 进程负责处理和转发？</li><li>如何选定哪个 Worker 进程处理请求？请求的处理结果，是否还要经过 Master 进程？</li></ol><p><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/19/1705d5b0e5da5404~tplv-t2oaga2asx-watermark.awebp" alt="img"></p><p>HTTP 连接建立和请求处理过程：</p><ol><li>Nginx 启动时，Master 进程，加载配置文件</li><li>Master 进程，初始化监听的 socket</li><li>Master 进程，fork 出多个 Worker 进程</li><li>Worker 进程，竞争新的连接，获胜方通过三次握手，建立 Socket 连接，并处理请求</li></ol><p>Nginx 高性能、高并发：</p><ol><li>Nginx 采用：<code>多进程</code> + <code>异步非阻塞</code>方式（<code>IO 多路复用</code> epoll）</li><li>请求的完整过程：<ol><li>建立连接</li><li>读取请求：解析请求</li><li>处理请求</li><li>响应请求</li></ol></li><li>请求的完整过程，对应到底层，就是：读写 socket 事件</li></ol><h3 id="Nginx-的事件处理模型"><a href="#Nginx-的事件处理模型" class="headerlink" title="Nginx 的事件处理模型"></a>Nginx 的事件处理模型</h3><p>request：Nginx 中 http 请求。</p><p>基本的 HTTP Web Server 工作模式：</p><ol><li><strong>接收请求</strong>：逐行读取<code>请求行</code>和<code>请求头</code>，判断段有请求体后，读取<code>请求体</code></li><li><strong>处理请求</strong></li><li><strong>返回响应</strong>：根据处理结果，生成相应的 HTTP 请求（<code>响应行</code>、<code>响应头</code>、<code>响应体</code>）</li></ol><p>Nginx 也是这个套路，整体流程一致。</p><p><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/19/1705d5b0e423d8e8~tplv-t2oaga2asx-watermark.awebp" alt="img"></p><h3 id="模块化体系结构"><a href="#模块化体系结构" class="headerlink" title="模块化体系结构"></a>模块化体系结构</h3><p><img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/19/1705d5b14dab9ed8~tplv-t2oaga2asx-watermark.awebp" alt="img"></p><p>nginx的模块根据其功能基本上可以分为以下几种类型：</p><ul><li><strong>event module</strong>: 搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括ngx_events_module， ngx_event_core_module和ngx_epoll_module等。nginx具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。</li><li><strong>phase handler</strong>: 此类型的模块也被直接称为handler模块。主要负责处理客户端请求并产生待响应内容，比如ngx_http_static_module模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。</li><li><strong>output filter</strong>: 也称为filter模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有html页面增加预定义的footbar一类的工作，或者对输出的图片的URL进行替换之类的工作。</li><li><strong>upstream</strong>: upstream模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。upstream模块是一种特殊的handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。</li><li><strong>load-balancer</strong>: 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。</li></ul><h2 id="常见问题剖析"><a href="#常见问题剖析" class="headerlink" title="常见问题剖析"></a>常见问题剖析</h2><h3 id="Nginx-vs-Apache"><a href="#Nginx-vs-Apache" class="headerlink" title="Nginx vs. Apache"></a>Nginx vs. Apache</h3><p>nginx vs. apache：</p><ul><li><a href="https://www.oschina.net/translate/nginx-vs-apache">https://www.oschina.net/translate/nginx-vs-apache</a></li></ul><p>网络 IO 模型：</p><ol><li>nginx：IO 多路复用，<code>epoll</code>(FreeBSD上是 <code>kqueue</code> )<ol><li>高性能</li><li>高并发</li><li>占用系统资源少</li></ol></li><li>apache：阻塞 + 多进程/多线程<ol><li>更稳定，bug 少</li><li>模块更丰富</li></ol></li></ol><p>参考：<a href="https://www.zhihu.com/question/19571087">https://www.zhihu.com/question/19571087</a></p><p>场景：</p><blockquote><p>处理多个请求时，可以采用：<code>IO 多路复用</code> 或者 <code>阻塞 IO</code> +<code>多线程</code></p><ol><li><strong>IO 多路服用</strong>：<code>一个</code> <code>线程</code>，跟踪多个 socket 状态，哪个<code>就绪</code>，就读写哪个；</li><li><strong>阻塞 IO</strong> + <strong>多线程</strong>：每一个请求，新建一个服务线程</li></ol></blockquote><p><strong>思考</strong>：<code>IO 多路复用</code> 和 <code>多线程</code> 的适用场景？</p><ul><li><code>IO 多路复用</code>：单个连接的请求处理速度没有优势，适合 IO 密集型场景，事件驱动<ul><li><strong>大并发量</strong>：只使用一个线程，处理大量的并发请求，降低<strong>上下文环境</strong>切换损耗，也不需要考虑并发问题，相对可以处理更多的请求；</li><li>消耗更少的系统资源（不需要<code>线程调度开销</code>）</li><li>适用于<code>长连接</code>的情况（多线程模式<code>长连接</code>容易造成<code>线程过多</code>，造成<code>频繁调度</code>）</li></ul></li><li><code>阻塞IO</code>+ <code>多线程</code>：实现简单，可以不依赖系统调用，适合 CPU 密集型场景<ul><li>每个线程，都需要时间和空间；</li><li>线程数量增长时，线程调度开销指数增长</li></ul></li></ul><h3 id="Nginx-最大连接数"><a href="#Nginx-最大连接数" class="headerlink" title="Nginx 最大连接数"></a>Nginx 最大连接数</h3><p>基础背景：</p><ol><li>Nginx 是多进程模型，Worker 进程用于处理请求；</li><li>单个进程的连接数（文件描述符 fd），有上限（<code>nofile</code>）：<code>ulimit -n</code></li><li>Nginx 上配置单个 worker 进程的最大连接数：<code>worker_connections</code> 上限为 <code>nofile</code></li><li>Nginx 上配置 worker 进程的数量：<code>worker_processes</code></li></ol><p>因此，Nginx 的最大连接数：</p><ol><li>Nginx 的最大连接数：<code>Worker 进程数量</code> x <code>单个 Worker 进程的最大连接数</code></li><li>上面是 Nginx 作为通用服务器时，最大的连接数</li><li>Nginx 作为<code>反向代理</code>服务器时，能够服务的最大连接数：（<code>Worker 进程数量</code> x <code>单个 Worker 进程的最大连接数</code>）/ 2。</li><li>Nginx 反向代理时，会建立 <code>Client 的连接</code>和<code>后端 Web Server 的连接</code>，占用 2 个连接</li></ol><p>思考：</p><blockquote><ol><li>每打开一个 socket 占用一个 fd</li><li>为什么，<code>一个进程</code>能够打开的 fd 数量有限制？</li></ol></blockquote><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="HTTP-请求和响应"><a href="#HTTP-请求和响应" class="headerlink" title="HTTP 请求和响应"></a>HTTP 请求和响应</h3><ul><li>HTTP 请求：<ol><li>请求行：<code>method</code>、<code>uri</code>、<code>http version</code></li><li>请求头</li><li>请求体</li></ol></li><li>HTTP 响应：<ol><li>响应行：<code>http version</code>、<code>status code</code></li><li>响应头</li><li>响应体</li></ol></li></ul><h3 id="IO-模型"><a href="#IO-模型" class="headerlink" title="IO 模型"></a>IO 模型</h3><p>场景：</p><blockquote><p>处理多个请求时，可以采用：<code>IO 多路复用</code> 或者 <code>阻塞 IO</code> +<code>多线程</code></p><ol><li><strong>IO 多路服用</strong>：<code>一个</code> <code>线程</code>，跟踪多个 socket 状态，哪个<code>就绪</code>，就读写哪个；</li><li><strong>阻塞 IO</strong> + <strong>多线程</strong>：每一个请求，新建一个服务线程</li></ol></blockquote><p>思考：<code>IO 多路复用</code> 和 <code>多线程</code> 的适用场景？</p><ul><li><code>IO 多路复用</code>：单个连接的请求处理速度没有优势<ul><li><strong>大并发量</strong>：只使用一个线程，处理大量的并发请求，降低<strong>上下文环境</strong>切换损耗，也不需要考虑并发问题，相对可以处理更多的请求；</li><li>消耗更少的系统资源（不需要<code>线程调度开销</code>）</li><li>适用于<code>长连接</code>的情况（多线程模式<code>长连接</code>容易造成<code>线程过多</code>，造成<code>频繁调度</code>）</li></ul></li><li><code>阻塞IO</code>+ <code>多线程</code>：实现简单，可以不依赖系统调用。<ul><li>每个线程，都需要时间和空间；</li><li>线程数量增长时，线程调度开销指数增长</li></ul></li></ul><h4 id="select-poll-和-epoll-比较"><a href="#select-poll-和-epoll-比较" class="headerlink" title="select/poll 和 epoll 比较"></a>select/poll 和 epoll 比较</h4><p>详细内容，参考：</p><ul><li><a href="https://link.juejin.cn/?target=http://www.cnblogs.com/wiessharling/p/4106295.html">select poll epoll三者之间的比较</a></li></ul><p>select/poll 系统调用：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// select 系统调用</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">select</span><span class="params">(<span class="keyword">int</span> maxfdp,fd_set *readfds,</span></span></span><br><span class="line"><span class="params"><span class="function">           fd_set *writefds,</span></span></span><br><span class="line"><span class="params"><span class="function">           fd_set *errorfds,</span></span></span><br><span class="line"><span class="params"><span class="function">           struct timeval *timeout)</span></span>;</span><br><span class="line"><span class="comment">// poll 系统调用</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">poll</span><span class="params">(struct pollfd fds[], <span class="keyword">nfds_t</span> nfds, <span class="keyword">int</span> timeout)</span></span></span><br></pre></td></tr></table></figure><p><strong>select</strong>：</p><ul><li>查询 fd_set 中，是否有<code>就绪</code>的 <code>fd</code>，可以设定一个<code>超时时间</code>，当有 fd (File descripter) 就绪或超时返回；</li><li>fd_set 是一个<code>位集合</code>，大小是在<code>编译内核</code>时的常量，默认大小为 1024</li><li>特点：<ul><li><strong>连接数限制</strong>，fd_set 可表示的 fd 数量太小了；</li><li><strong>线性扫描</strong>：判断 fd 是否就绪，需要遍历一边 fd_set；</li><li><strong>数据复制</strong>：用户空间和内核空间，复制<strong>连接就绪状态</strong>信息</li></ul></li></ul><p><strong>poll</strong>：</p><ul><li><p>解决了连接数限制：</p><ul><li>poll 中将 select 中的 fd_set 替换成了一个 pollfd <code>数组</code></li><li>解决 <code>fd 数量过小</code>的问题</li></ul></li><li><p><strong>数据复制</strong>：用户空间和内核空间，复制<strong>连接就绪状态</strong>信息</p></li></ul><p><strong>epoll</strong>： event 事件驱动</p><ul><li><p>事件机制：避免线性扫描</p><ul><li>为每个 fd，<code>注册</code>一个<code>监听事件</code></li><li>fd 变更为<code>就绪</code>时，将 fd 添加到<code>就绪链表</code></li></ul></li><li><p><strong>fd 数量</strong>：无限制（OS 级别的限制，单个进程能打开多少个 fd）</p></li></ul><p>select，poll，epoll：</p><ol><li><p><code>I/O多路复用</code>的机制；</p></li><li><p><code>I/O多路复用</code>就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。</p><ol><li> 监视<code>多个文件描述符</code></li></ol></li><li><p>但select，poll，epoll本质上都是<code>同步I/O</code></p><ol><li><code>用户进程</code>负责<code>读写</code>（从<code>内核空间</code>拷贝到<code>用户空间</code>），读写过程中，用户进程是阻塞的；</li><li><code>异步 IO</code>，无需用户进程负责读写，异步IO，会负责从<code>内核空间</code>拷贝到<code>用户空间</code>；</li></ol></li></ol><h3 id="Nginx-的并发处理能力"><a href="#Nginx-的并发处理能力" class="headerlink" title="Nginx 的并发处理能力"></a>Nginx 的并发处理能力</h3><p>关于 Nginx 的并发处理能力：</p><ul><li>并发连接数，一般优化后，峰值能保持在 1~3w 左右。（内存和 CPU 核心数不同，会有进一步优化空间）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;基本原理&quot;&gt;&lt;a href=&quot;#基本原理&quot; class=&quot;head</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Nginx" scheme="http://posts.hufeifei.cn/tags/Nginx/"/>
    
    <category term="Multiplex" scheme="http://posts.hufeifei.cn/tags/Multiplex/"/>
    
  </entry>
  
  <entry>
    <title>Google论文、开源与云计算</title>
    <link href="http://posts.hufeifei.cn/backend/google-paper-opensource/"/>
    <id>http://posts.hufeifei.cn/backend/google-paper-opensource/</id>
    <published>2021-10-29T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="1-Google论文与开源"><a href="#1-Google论文与开源" class="headerlink" title="1.Google论文与开源"></a>1.Google论文与开源</h1><p>自1998年成立，至今Google已走过20个年头。在这20年里，Google不断地发表一些对于自己来说已经过时甚至不再使用的技术的论文，但是发表之后总会有类似系统被业界实现出来，也足以说明Google的技术至少领先业界数年。在Amazon不断引领全球云计算浪潮开发出一系列面向普罗大众的云产品的同时；Google也在不断引领构建着满足互联网时代海量数据的存储计算和查询分析需求的软硬件基础设施。</p><p>本文对Google在这20年中发表的论文进行了一个简单的总结和整理，主要选择了分布式系统和并行计算领域相关的论文，其中内容涉及数据中心/计算/存储/网络/数据库/调度/大数据处理等多个方向。通过这样的一个总结，一方面可以一窥Google强大的软硬件基础设施，另一方面也可以为不同领域的开发人员提供一个学习的参考。可以通过这些文章去了解上层应用的架构设计和实现，进而可以更好的理解和服务于上层应用。同时这些系统中所采用的架构/算法/设计/权衡，本身也可以为我们的系统设计和实现提供重要的参考。</p><p>通过Google论文可以了解到系统整体的架构，通过对应开源系统可以在代码层面进行学习。具体如下图(浅蓝色部分为Google论文/黄色为开源系统)：</p><p><img src="https://pica.zhimg.com/80/v2-89ebe0b3972ada20f8ef8c79b0229ab8_720w.png" alt="img"></p><h1 id="2-Google论文简介"><a href="#2-Google论文简介" class="headerlink" title="2.Google论文简介"></a>2.Google论文简介</h1><p>下面来简要介绍下”那些年我们追过的Google论文”，由于篇幅有限主要讲下每篇论文的主要思路，另外可能还会介绍下论文作者及论文本身的一些八卦。深入阅读的话，可以直接根据下面的链接查看原文，另外很多文章网上已经有中文译文，也可以作为阅读参考。</p><h2 id="2-1-起源"><a href="#2-1-起源" class="headerlink" title="2.1 起源"></a>2.1 起源</h2><ol><li><p><a href="http://zoo.cs.yale.edu/classes/cs426/2012/bib/brin98theanatomy.pdf">The anatomy of a large-scale hypertextual Web search engine</a>(1998).Google创始人Sergey Brin和Larry Page于1998年发表的奠定Google搜索引擎理论基础的原始论文。在上图中我们把它放到了最底层，在这篇论文里他们描述了最初构建的Google搜索引擎基础架构，可以说所有其他文章都是以此文为起点。此文对于搜索引擎的基本架构，尤其是Google使用的PageRank算法进行了描述，可以作为了解搜索引擎的入门文章。</p></li><li><p><a href="http://research.google.com/archive/googlecluster-ieee.pdf">WEB SEARCH FOR A PLANET: THE GOOGLE CLUSTER ARCHITECTURE</a>(IEEE Micro03).描述Google集群架构最早的一篇文章，同时也应该是最被忽略的一篇文章，此文不像GFS MapReduce Bigtable那几篇文章为人所熟知，但是其重要性丝毫不亚于那几篇。这篇文章体现了Google在硬件方面的一个革命性的选择：在数据中心中使用廉价的PC硬件取代高端服务器。这一选择的出发点主要基于性价比，实际的需求是源于互联网数据规模之大已经不能用传统方法解决，但是这个选择导致了上层的软件也要针对性地进行重新的设计和调整。由于硬件可靠性的降低及数量的上升，意味着要在软件层面实现可靠性，需要采用多个副本，需要更加自动化的集群管理和监控。也是从这个时候开始，Google开始着眼于自己设计服务器以及数据中心相关的其他硬件，逐步从托管数据中心向自建数据中心转变。而Google之后实现的各种分布式系统，都可以看做是基于这一硬件选择做出的软件层面的设计权衡。</p><p>再看下本文的作者：Luiz André Barroso/Jeff Dean/Urs Hölzle，除了Jeff Dean，其他两位也都是Google基础设施领域非常重要的人物。Urs Holzle是Google的第8号员工，最早的技术副总裁，一直在Google负责基础设施部门，Jeff Dean和Luiz Barroso等很多人都是他招进Google的，包括当前Google云平台的掌门人Diane Greene(VMWare联合创始人)据说也是在他的游说下才最终决定掌管GCP。Luiz Barroso跟Jeff Dean在加入Google以前都是在DEC工作，在DEC的时候他参与了多核处理器方面的工作，是Google最早的硬件工程师，在构建Google面向互联网时代的数据中心硬件基础设施中做了很多工作。到了2009年，Luiz André Barroso和Urs Hölzle写了一本书，书名就叫&lt;&lt;<a href="http://research.google.com/pubs/pub41606.html">The Datacenter as a Computer</a>&gt;&gt;，对这些工作(数据中心里的服务器/网络/供电/制冷/能效/成本/故障处理和修复等)做了更详细的介绍。</p></li><li><p><a href="https://research.google.com/archive/gfs-sosp2003.pdf">The Google File System</a>(SOSP03).Google在分布式系统领域发表的最早的一篇论文。关于GFS相信很多人都有所了解，此处不再赘言。今天Google内部已经进化到第二代GFS：Colossus，而关于Colossus目前为止还没有相关的论文，网上只有一些零散介绍：<a href="https://www.systutorials.com/3202/colossus-successor-to-google-file-system-gfs/">Colossus</a>。简要介绍下本文第一作者Sanjay Ghemawat，在加入Google之前他也是在DEC工作，主要从事Java编译器和Profiling相关工作。同时在DEC时代他与Jeff Dean就有很多合作，而他加入Google也是Jeff Dean先加入后推荐他加入的，此后的很多工作都是他和Jeff Dean一块完成的，像后来的MapReduce/BigTable/Spanner/TensorFlow，在做完Spanner之后，Jeff Dean和Sanjay开始转向构建AI领域的大规模分布式系统。2012年，Jeff Dean和Sanjay共同获得了ACM-Infosys Foundation Award。此外Google的一些开源项目像LevelDB/GPerftools/TCMalloc等，都可以看到Sanjay的身影。</p></li><li><p><a href="https://research.google.com/archive/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a>(OSDI04).该文作者是Jeff Dean和Sanjay Ghemawat，受Lisp语言中的Map Reduce原语启发，在大规模分布式系统中提供类似的操作原语。在框架层面屏蔽底层分布式系统实现，让用户只需要关注如何编写自己的Mapper和Reducer实现，从而大大简化分布式编程。时至今日MapReduce已经成为大规模数据处理中广泛应用的一种编程模型，虽然之后有很多新的编程模型不断被实现出来，但是在很多场景MapReduce依然发挥着不可替代的作用。</p><p>而自2004年提出之后，中间也出现过很多关于MapReduce的争论，最著名的应该是2008年1月8号David J. DeWitt和Michael Stonebraker发表的一篇文章&lt;&lt; <a href="http://databasecolumn.vertica.com/database-innovation/mapreduce-a-major-step-backwards/">MapReduce: A major step backwards</a>&gt;&gt;，该文发表后引起了广泛的争论。首先介绍下这两位都是数据库领域的著名科学家，David J. DeWitt，ACM Fellow，2008年以前一直在大学里搞研究，在并行数据库领域建树颇多，之后去了微软在威斯康辛的Jim Gray系统实验室。Michael Stonebraker(2014图灵奖得主)，名头要更大一些，在1992 年提出对象关系数据库。在加州伯克利分校计算机教授达25年，在此期间他创作了Ingres, Illustra, Cohera, StreamBase Systems和Vertica等系统。其中Ingres是很多现代RDBMS的基础，比如Sybase、Microsoft SQL Server、NonStop SQL、Informix 和许多其他的系统。Stonebraker曾担任过Informix的CEO，自己还经常出来创个业，每次还都成功了。关于这个争论，Jeff Dean和Sanjay Ghemawat在2010年1月份的&lt;<communication of="" the="" ACM="">&gt;上发表了这篇&lt;&lt;<a href="http://duanple.blog.163.com/blog/static/7097176720119711038980/">MapReduce-A Flexible Data Processing Tool</a>&gt;&gt;进行回应，同一期上还刊了Michael Stonebraker等人的&lt;&lt;<a href="http://duanple.blog.163.com/blog/static/7097176720119720494/">MapReduce and Parallel DBMSs-Friends or Foes </a>&gt;&gt;。</communication></p></li><li><p><a href="http://research.google.com/archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a>(OSDI06).Bigtable基于GFS构建，提供了结构化数据的可扩展分布式存储。自Bigtable论文发表之后，很快开源的HBase被实现出来，此后更是与Amazon的Dynamo一块引领了NoSQL系统的潮流，之后各种NoSQL系统如雨后春笋般出现在各大互联网公司及开源领域。此外在tablet-server中采用的LSM-Tree存储结构，使得这种在1996年就被提出的模型被重新认识，并广泛应用于各种新的存储系统实现中，成为与传统关系数据库中的B树并驾齐驱的两大模型。</p><p>如果说MapReduce代表着新的分布式计算模型的开端的话，Bigtable则代表着新的分布式存储系统的开端。自此之后在分布式计算存储领域，Google不断地推陈出新，发表了很多新的计算和存储系统，如上图中所示。在继续介绍这些新的计算存储系统之前，我们回到图的底层，关注下基础设施方面的一些系统。</p></li></ol><h2 id="2-2-基础设施"><a href="#2-2-基础设施" class="headerlink" title="2.2 基础设施"></a>2.2 基础设施</h2><ol start="6"><li><p><a href="http://research.google.com/archive/chubby-osdi06.pdf">The Chubby lock service for loosely-coupled distributed systems</a>(OSDI06).以文件系统接口形式提供的分布式锁服务，帮助开发者简化分布式系统中的同步和协调工作，比如进行Leader选举。除此之外，这篇文章一个很大的贡献应该是将Paxos应用于工业实践，并极大地促进了Paxos的流行，从这个时候开始Paxos逐渐被更多地工业界人士所熟知并应用在自己的分布式系统中。此后Google发表的其他论文中也不止一次地提到Paxos，像MegaStore/Spanner/Mesa都有提及。此文作者Mike Burrows加入Google之前也是在DEC工作，在DEC的时候他还是AltaVista搜索引擎的主要设计者。</p></li><li><p><a href="https://research.google.com/pubs/pub43438.html">Borg</a>(Eurosys15) <a href="https://ai.google/research/pubs/pub41684">Omega</a>(Eurosys13) <a href="https://kubernetes.io/">Kubernetes</a>.Borg是Google内部的集群资源管理系统，大概诞生在2003-2004年，在Borg之前Google通过两个系统Babysitter和Global Work Queue来分别管理它的在线服务和离线作业，而Borg实现了两者的统一管理。直到15年Google才公布了Borg论文，在此之前对外界来说Borg一直都是很神秘的存在。而Omega主要是几个博士生在Google做的研究型项目，最终并没有实际大规模上线，其中的一些理念被应用到Borg系统中。<em>注：Borg这个名字源自于&lt;&lt;星际迷航&gt;&gt;里的博格人，博格人生活在银河系的德尔塔象限，是半有机物半机械的生化人。博格个体的身体上装配有大量人造器官及机械，大脑为人造的处理器。博格人是严格奉行集体意识的种族，从生理上完全剥夺了个体的自由意识。博格人的社会系统由“博格集合体”组成，每个集合体中的个体成员被称为“Drone”。集合体内的博格个体通过某种复杂的子空间通信网络相互连接。在博格集合体中，博格个体没有自我意识，而是通过一个被称为博格女皇（Borg Queen）的程序对整个集合体进行控制。</em></p><p>在2014年中的时候，Google启动了Kubernetes(Borg的开源版本)。2015年，Kubernetes 1.0 release，同时Google与Linux基金会共同发起了CNCF。2016年，Kubernates逐渐成为容器编排管理领域的主流。提到Kubernates，需要介绍下著名的分布式系统专家Eric Brewer，伯克利教授&amp;Google infrastructure VP，互联网服务系统早期研究者。早在1995年他就和Paul Gauthier创立了Inktomi搜索引擎(2003年被Yahoo!收购，李彦宏曾在这家公司工作)，此时距离Google创立还有3年。之后在2000年的PODC上他首次提出了CAP理论，2012年又对CAP进行了<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed">回顾</a>。2011年他加入了Google，目前在负责推动Kubernetes的发展。</p></li><li><p><a href="http://www.e-wilkes.com/john/papers/2013-EuroSys-CPI2.pdf">CPI2: CPU performance isolation for shared compute clusters</a>(Eurosys13).通过监控CPI(Cycles-Per-Instruction)指标，结合历史运行数据进行分析预测找到影响系统性能的可疑程序，限制其CPU使用或进行隔离/下线，避免影响其他关键应用。本文也从一个侧面反映出，为了实现离线在线混布Google在多方面所做的努力和探索，尤其是在资源隔离方面。具体实现中，每台机器上有一个守护进程负责采集本机上运行的各个Job的CPI数据(通过采用计数模式/采样等方法降低开销，实际CPU开销小于0.1%)，然后发送到一个中央的服务器进行聚合，由于集群可能是异构的，每个Job还会根据不同的CPU类型进行单独聚合，最后把计算出来的CPI数据的平均值和标准差作为CPI spec。结合CPI历史记录建立CPI预测模型，一旦出现采样值偏离预测值的异常情况，就会记录下来，如果异常次数超过一定阈值就启动相关性分析寻找干扰源，找到之后进行相应地处理(限制批处理作业的CPU使用/调度到单独机器上等)。讲到这里，不仅让我们联想到今天大火的AIOPS概念，而很久之前Google已经在生产系统上使用类似技术。不过在论文发表时，Google只是打开了CPI2的监控功能，实际的自动化处理还未在生产系统中打开。</p></li><li><p><a href="https://ai.google/research/pubs/pub36575">GOOGLE-WIDE PROFILING:A CONTINUOUS PROFILING </a><a href="https://ai.google/research/pubs/pub36575">INFRASTRUCTURE FOR DATA CENTERS</a>(IEEE Micro10).Google的分布式Profiling基础设施，通过收集数据中心的机器上的各种硬件事件/内核事件/调用栈/锁竞争/堆内存分配/应用性能指标等信息，通过这些信息可以为程序性能优化/Job调度提供参考。为了降低开销，采样是在两个维度上进行，首先是在整个集群的机器集合上采样同一时刻只对很少一部分机器进行profiling，然后在每台机器上再进行基于事件的采样。底层通过OProfile采集系统硬件监控指标(比如CPU周期/L1 L2 Cache Miss/分支预测失败情况等)，通过GPerfTools采集应用程序进程级的运行指标(比如堆内存分配/锁竞争/CPU开销等)。收集后的原始采样信息会保存在GFS上，但是这些信息还未与源代码关联上，而部署的binary通常都是去掉了debug和符号表信息，采用的解决方法是为每个binary还会保存一个包含debug信息的未被strip的原始binary，然后通过运行MapReduce Job完成原始采样信息与源代码的关联。为了方便用户查询，历史Profiling数据还会被加载到一个分布式数据库中。通过这些Profiling数据，除了可以帮助应用理解程序的资源消耗和性能演化历史，还可以实现数据驱动的数据中心设计/构建/运维。</p></li><li><p><a href="http://research.google.com/pubs/archive/36356.pdf">Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</a>(Google TR10).Google的分布式Tracing基础设施。Dapper最初是为了追踪在线服务系统的请求处理过程。比如在搜索系统中，用户的一个请求在系统中会经过多个子系统的处理，而且这些处理是发生在不同机器甚至是不同集群上的，当请求处理发生异常时，需要快速发现问题，并准确定位到是哪个环节出了问题，这是非常重要的，Dapper就是为了解决这样的问题。对系统行为进行跟踪必须是持续进行的，因为异常的发生是无法预料的，而且可能是难以重现的。同时跟踪需要是无所不在，遍布各处的，否则可能会遗漏某些重要的点。基于此Dapper有如下三个最重要的设计目标：低的额外开销，对应用的透明性，可扩展。同时产生的跟踪数据需要可以被快速分析，这样可以帮助用户实时获取在线服务状态。</p></li><li><p><a href="http://cseweb.ucsd.edu/~vahdat/papers/b4-sigcomm13.pdf">B4: Experience with a Globally-Deployed Software Defined WAN</a>(Sigcomm13).Google在全球有几十个数据中心，这些数据中心之间通常通过2-3条专线与其他数据中心进行连接。本文描述了Google如何通过SDN/OpenFlow对数据中心间的网络进行改造，通过对跨数据中心的流量进行智能调度，最大化数据中心网络链路的利用率。Google通过强大的网络基础设施，使得它的跨越全球的数据中心就像一个局域网，从而为后续很多系统实现跨数据中心的同步复制提供了网络层面的保障。</p></li></ol><h2 id="2-3-计算分析系统"><a href="#2-3-计算分析系统" class="headerlink" title="2.3 计算分析系统"></a>2.3 计算分析系统</h2><p>自MapReduce之后，Google又不断地开发出新的分布式计算系统，一方面是为了提供更易用的编程接口(比如新的DSL/SQL语言支持)，另一方面是为了适应不同场景(图计算/流计算/即席查询/内存计算/交互式报表等)的需求。</p><ol start="12"><li><p><a href="http://cloud.pubs.dbs.uni-leipzig.de/sites/cloud.pubs.dbs.uni-leipzig.de/files/Pike2005InterpretingthedataParallelanalysiswithSawzall.pdf">Interpreting the Data: Parallel Analysis with Sawzall</a>(Scientific Programming05).Google为了简化MapReduce程序的编写，而提出的一种新的DSL。后来Google又推出了Tenzing/Dremel等数据分析系统，到了2010年就把Sawzall给开源了，项目主页：<a href="http://code.google.com/p/szl/%E3%80%82%E8%99%BD%E7%84%B6%E4%B8%8ETenzing/Dremel%E7%9B%B8%E6%AF%94%EF%BC%8C">http://code.google.com/p/szl/。虽然与Tenzing/Dremel相比，</a> Sawzall所能做的事情还是比较有限，但是它是最早的，同时作为一种DSL毕竟还是要比直接写MapReduce job要更易用些。</p><p>本文第一作者Rob Pike，当今世界上最著名的程序员之一，&lt;&lt;Unix编程环境&gt;&gt; &lt;&lt;程序设计实践&gt;&gt;作者。70年代就加入贝尔实验室，跟随Ken Thompson&amp;DMR(二人因为发明Unix和C语言共同获得1983年图灵奖)参与开发了Unix，后来又跟Ken一块设计了UTF-8。2002年起加入Google，之后搞了Sawzall，目前跟Ken Thompson一块在Google设计开发Go语言。</p></li><li><p><a href="http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf">FlumeJava: Easy, Efficient Data-Parallel Pipelines</a>(PLDI10).由于实际的数据处理中，通常都不是单个的MapReduce Job，而是多个MapReduce Job组成的Pipeline。为了简化Pipleline的管理和编程，提出了FlumeJava框架。由框架负责MapReduce Job的提交/中间数据管理，同时还会对执行过程进行优化，用户可以方便地对Pipeline进行开发/测试/运行。另外FlumeJava没有采用新的DSL，而是以Java类库的方式提供给用户，用户只需要使用Java语言编写即可。</p></li><li><p><a href="https://kowshik.github.io/JPregel/pregel_paper.pdf">Pregel: A System for Large-Scale Graph Processing</a>(SIGMOD10).Google的图处理框架。Pregel这个名称是为了纪念欧拉，在他提出的格尼斯堡七桥问题中，那些桥所在的河就叫Pregel，而正是格尼斯堡七桥问题导致了图论的诞生。最初是为了解决PageRank计算问题，由于MapReduce并不适于这种场景，所以需要发展新的计算模型去完成这项计算任务，在这个过程中逐步提炼出一个通用的图计算框架，并用来解决更多的问题。核心思想源自BSP模型，这个就更早了，是在上世纪80年代由Leslie Valiant(2010年图灵奖得主)提出，之后在1990的Communications of the ACM 上，正式发表了题为A bridging model for parallel computation的文章。</p></li><li><p><a href="https://ai.google/research/pubs/pub36632">Dremel: Interactive Analysis of Web-Scale Datasets</a>(VLDB10).由于MapReduce的延迟太大，无法满足交互式查询的需求，Google开发了Dremel系统。Dremel主要做了三件事：将嵌套记录转换为列式存储，并提供快速的反向组装；类sql的查询语言；类搜索系统的查询执行树。通过列式存储降低io，将速度提高一个数量级，这类似于诸如Vertica这样的列存式数据库，与传统行式存储不同，它们只需要读取查询语句中真正必需的那些字段数据；通过类搜索系统的查询执行系统取代mr(MapReduce)，再提高一个数量级。它类似于Hive，应该说查询层像Hive，都具有类似于SQL的查询语言，都可以用来做数据挖掘和分析；但hive是基于mr，所以实时性要差，Dremel则由于它的查询执行引擎类似于搜索服务系统，因此非常适合于交互式的数据分析方式，具有较低的延迟，但是通常数据规模要小于mr；而与传统数据库的区别是，它具有更高的可扩展性和容错性，结构相对简单，可以支持更多的底层存储方式。其中的数据转化与存储方式，巧妙地将Protobuf格式的嵌套记录转换成了列式存储，同时还能够快速的进行重组，是其比较独特的一点。</p></li><li><p><a href="https://ai.google/research/pubs/pub37200">Tenzing A SQL Implementation On The MapReduce Framework</a>(VLDB11).Tenzing是一个建立在MapReduce之上的用于Google数据的ad hoc分析的SQL查询引擎。Tenzing提供了一个具有如下关键特征的完整SQL实现(还具有几个扩展)：异构性，高性能，可扩展性，可靠性，元数据感知，低延时，支持列式存储和结构化数据，容易扩展。Tenzing的发表算是很晚的了，与之相比Facebook在VLDB09上就发表了Hive的论文。与开源系统Hive的优势在于它跟底层所依赖的MapReduce系统都是一个公司内的产品，因此它可以对MapReduce做很多改动，以满足Tenzing某些特殊性的需求，最大化Tenzing的性能。 </p></li><li><p><a href="http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf">PowerDrill：</a><a href="http://vldb.org/pvldb/vol5/p1436_alexanderhall_vldb2012.pdf">Processing a Trillion Cells per Mouse Click</a>(VLDB12).Google推出的基于内存的列存数据库，该系统在2008年就已经在Google内部上线。与Dremel相比虽然都是面向分析场景，但是PowerDrill主要面向的是少量核心数据集上的多维分析，由于数据集相对少同时分析需求多所以可以放到内存，在把数据加载到内存分析之前会进行复杂的预处理以尽量减少内存占用。而Dremel则更加适合面向大量数据集的分析，不需要把数据加载到内存。</p><p>主要采用了如下技术进行加速和内存优化：1)导入时对数据进行分区，然后查询时根据分区进行过滤尽量避免进行全量扫描 2)底层数据采用列式存储，可以跳过不需要的列 3)采用全局/chunk两级字典对列值进行编码，一方面可以加速计算(chunk级的字典可以用来进行针对用户查询的chunk过滤，编码后的value变成了更短的int类型与原始值相比可以更快速的进行相关运算)，另一方面还可以达到数据压缩的目的，与通用压缩算法相比采用这种编码方式的优点是：读取时不需要进行解压这样的预处理，同时支持随机读取 4)编码后的数据进行压缩还可以达到1.4-2倍的压缩比，为了避免压缩带来的性能降低，采用了压缩与编码的混合策略，对数据进行分层，最热的数据是解压后的编码数据，然后稍冷的数据也还会进行压缩 5)对数据行根据partition key进行重排序，提高压缩比 6)查询分布式执行，对于同一个查询会分成多个子查询并发给多个机器执行，同时同一个子查询会发给两台机器同时执行，只要有一个返回即可，但是另一个最终也要执行完以进行数据预热。</p></li><li><p><a href="https://ai.google/research/pubs/pub41378">MillWheel: Fault-Tolerant Stream Processing at Internet Scale</a>(VLDB13).Google的流计算系统，被广泛应用于构建低延迟数据处理应用的框架。用户只需要描述好关于计算的有向图，编写每个节点的应用程序代码。系统负责管理持久化状态和连续的记录流，同时将一切置于框架提供的容错性保证之下。虽然发布的比较晚，但是其中的一些机制(比如Low Watermark)被借鉴到开源的Flink系统中。</p></li><li><p><a href="https://ai.google/research/pubs/pub42851">Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing</a>(VLDB14).Google的跨数据中心数据仓库系统，主要是为了满足广告业务的场景需求，随着广告平台的不断发展，客户对各自的广告活动的可视化提出了更高的要求。对于更具体和更细粒度的信息需求，直接导致了数据规模的急速增长。虽然Google已经把核心广告数据迁移到了Spanner+F1上，但是对于这种广告效果实时统计需求来言，由于涉及非常多的指标这些指标可能是保存在成百上千张表中，同时这些指标与用户点击日志相关通常对应着非常大的峰值访问量，超过了Spanner+F1这样的OLTP系统的处理能力。为此Google构建了Mesa从而能处理持续增长的数据量，同时它还提供了一致性和近实时查询数据的能力。具体实现方法是：将增量更新进行batch，提交者负责为增量数据分配版本号，利用Paxos对跨数据中心的版本数据库进行更新，基于MVCC机制提供一致性访问。底层通过Bigtable存储元数据，通过<a href="http://static.googleusercontent.com/media/research.google.com/en/us/university/relations/facultysummit2010/storage_architecture_and_challenges.pdf">Colossus</a>来存储数据文件，此外还利用<a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a>来对连续增量数据进行合并，而为Mesa提供增量更新的上游应用通常是一个流计算系统。可以看到Mesa系统本身结合了批量处理与实时计算，还要满足OLTP+OLAP的场景需求，同时采用了分层架构实现存储计算的分离。既像一个分布式数据库，又像一个大数据准实时处理系统。</p></li><li><p><a href="https://research.google.com/pubs/archive/45394.pdf">Shasta: Interactive Reporting At Scale</a>(SIGMOD16).Google的交互式报表系统，也主要是为了满足广告业务的场景需求，与Mesa的区别在于Shasta是构建于Mesa之上的更上层封装。主要为了解决如下挑战：1)用户查询请求的低延迟要求 2)底层事务型数据库的schema与实际展现给用户的视图不友好，报表系统的开发人员需要进行复杂的转换，一个查询视图底层可能涉及多种数据源(比如F1/Mesa/Bigtable等) 3)数据实时性需求，用户修改了广告预算后希望可以在新的报表结果中可以马上体现出来。为了解决这些问题，在F1和Mesa系统之上构建了Shasta。主要从两个层面进行解决：语言层面，在SQL之上设计了一种新的语言RVL(Relational View Language)，通过该语言提供的机制(自动聚合/子句引用/视图模板/文本替换等)可以比SQL更加方便地描述用户的查询视图，RVL编译器会把RVL语句翻译成SQL，在这个过程中还会进行查询优化；系统层面，直接利用了F1的分布式查询引擎，但是进行了一些扩展比如增加单独的UDF server让UDF的执行更加安全，为了确保实时性需要直接访问F1，但是为了降低延迟在F1之上增加了一个只读的分布式Cache层。</p></li><li><p><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45390.pdf">Goods: Organizing Google’s Datasets</a>(SIGMOD16).Google的元数据仓库Goods(Google DataSet Search)。Google内部积累了大量的数据集，而这些数据散落在各种不同的存储系统中(GFS/Bigtable/Spanner等)。面临的问题就是如何组织管理这些数据，使得公司内部工程师可以方便地找到他们需要的数据，实现数据价值的最大化。Google的做法很多方面都更像一个小型的搜索引擎，不过在这个系统里被索引的数据由网页变成了Google内部生产系统产生的各种数据，用户变成了内部的数据开发人员。整个做法看起来要费劲很多，很大程度上是因为内部系统众多但是没有一个统一的入口平台，只能采用更加自动化(不依赖人和其他系统)的做法：要爬取各个系统的日志，通过日志解析数据的元信息(这个过程中还是比较费劲的，比如为了确定数据的Schema，要把Google中央代码库里的所有protobuf定义拿过来试看哪个能匹配上)，然后把这些信息(大小/owner/访问权限/时间戳/文件格式/上下游/依赖关系/Schema/内容摘要等)保存一个中央的数据字典中(存储在Bigtable中目前已经索引了260亿条数据集信息)，提供给内部用户查询。这中间解决了如下一些问题和挑战：Schema探测/数据自动摘要/血缘分析/聚类/搜索结果ranking/过期数据管理/数据备份等。本文可以让我们一窥Google是如何管理内部数据资产的，有哪些地方可以借鉴。</p></li></ol><h2 id="2-4-存储-amp-数据库"><a href="#2-4-存储-amp-数据库" class="headerlink" title="2.4 存储&数据库"></a>2.4 存储&amp;数据库</h2><ol start="22"><li><p><a href="https://ai.google/research/pubs/pub36726">Percalator:</a><a href="https://ai.google/research/pubs/pub36726">Large-scale Incremental Processing Using Distributed Transactions and Notifications</a>(OSDI10).基于Bigtable的增量索引更新系统，Google新一代索引系统”咖啡因“实时性提升的关键。此前Google的索引构建是基于MapReduce，全量索引更新一次可能需要几天才能完成，为了提高索引更新的实时性Google构建了增量更新系统。Bigtable只支持单行的原子更新，但是一个网页的更新通常涉及到其他多个网页(网页间存在链接关系比如更新的这个网页上就有其他网页的锚文本)的更新。为了解决这个问题，Percolator在Bigtable之上通过两阶段提交实现了跨行事务。同时网页更新后还要触发一系列的处理流程，Percolator又实现了类似于数据库里面的触发器机制，当Percolator中的某个cell数据发生变化，就触发应用开发者指定的Observer程序。此外开源分布式数据库TiDB就参考了Percalator的事务模型。</p></li><li><p><a href="http://cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf">Megastore: Providing Scalable, Highly Available Storage for Interactive Services</a>(CIDR11).Google在2008年的SIGMOD上就介绍了Megastore，但是直到2011年才发表完整论文。Megastore本身基于Bigtable，在保留可扩展/高性能/低延迟/高可用等优点的前提下，引入了传统关系数据库中的很多概念比如关系数据模型/事务/索引，同时基于Paxos实现了全球化同步复制，可以说是最早的分布式数据库实现了。它本身也提供了分布式事务支持，但是论文中并没有描述相关实现细节，猜测应该跟Percalator类似。虽然此后被Spanner所替代，但是它的继任者Spanner很多特性都是受它影响。</p></li><li><p><a href="http://research.google.com/archive/spanner-osdi2012.pdf">Spanner: Google’s Globally-Distributed Database</a>(OSDI12).2009年Jeff Dean的一次分享(<a href="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf">Designs, Lessons and Advice from Building Large Distributed)</a>中首次提到Spanner，也是过了3年到了2012年才发表完整论文。做为Megastore的继任者，它主要解决了Megastore存在的几个问题：性能、查询语言支持弱、分区不灵活。另外一个重要的创新是基于原子钟和GPS硬件实现了TrueTime API，并基于这个API实现了更强的一致性保证。除此之外其他部分则与Megastore非常类似，但是在文中对其分布式事务的实现细节进行了描述。</p></li><li><p><a href="https://ai.google/research/pubs/pub41344">F1: A Distributed SQL Database That Scales</a>(VLDB13).基于Spanner实现的分布式SQL数据库，主要实现了一个分布式并行查询引擎，支持一致性索引和非阻塞的在线Schema变更。与Spanner配合替换掉了Google核心广告系统中的MySQL数据库。F1这个名字来自生物遗传学，代指杂交一代，表示它结合了传统关系数据库和NoSQL系统两者的特性。</p></li></ol><h2 id="2-5-AI"><a href="#2-5-AI" class="headerlink" title="2.5 AI"></a>2.5 AI</h2><ol start="26"><li><p><a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow: A System for Large-Scale Machine Learning</a>(OSDI16).</p></li><li><p><a href="http://www.cs.toronto.edu/~pekhimenko/courses/csc2231-f17/Papers/tpu.pdf">In-Datacenter Performance Analysis of a Tensor Processing Unit</a>(SIGARCH17).Google TPU。与往常一样，在Google公布此文的时候，新一代更强大的TPU已经开发完成。由于本文更偏重硬件，具体内容没有看。但是其中的第四作者David Patterson还是值得特别来介绍一下，因为在体系结构领域的贡献(RISC、RAID、体系结构的量化研究方法)，他和John Hennessy共同获得了2017年的图灵奖：<a href="http://www.edu.cn/ke_yan_yu_fa_zhan/zui_jin_geng_xin/201803/t20180322_1591118_2.shtml">相关新闻</a>。2016年加入Google就是去做TPU的；2018年，与他共同获得图灵奖的John Hennessy(斯坦福第十任校长、MIPS公司创始人)被任命为Google母公司Alphabet的新任主席。</p></li></ol><h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h1><p>在前面两节我们对过去20年Google在分布式系统领域的经典论文进行了系统地梳理和介绍，通过这个过程我们可以看到：</p><p>每当Google发表一篇相关论文，通常都会产生一个与之对应的开源系统。比如GFS/HDFS，MapReduce/Hadoop MapReduce，BigTable/HBase，Chubby/ZooKeeper，FlumeJava/Plume，Dapper/Zipkin等等。如果把数据中心看做一台计算机的话，在数据中心之上的各种分布式系统就像当年的Unix和C语言，Hadoop及各种开源系统就像当年的Linux，而开启这个时代的人们尤其是Jeff Dean/Sanjay Ghemawat就像当年的Ken Thompson/Dennis M. Ritche，Hadoop创始人Doug Cutting就像当年的Linus Torvalds。Ken Thompson/Dennis M. Ritche因为Unix和C方面的贡献获得1983年图灵奖，或许在将来的某一天Jeff Dean/Sanjay Ghemawat也能摘得桂冠。</p><p>观察上图，我们还可以看到随着时间的推进，Google自底向上地逐步构建出一个庞大的软硬件基础设施Stack，同时每个系统内部也在不断地自我进化。而不同的系统之间，可能是互补关系，可能是继承关系，可能是替换关系。通过对这个演化过程的观察，我们也总结出一些内在的趋势和规律。论文本身固然重要，但是这些趋势和规律也很有意义。</p><h2 id="3-1-两个维度，三个层次"><a href="#3-1-两个维度，三个层次" class="headerlink" title="3.1 两个维度，三个层次"></a>3.1 两个维度，三个层次</h2><p>如题”他山之石”，人们常说不能总是低头拉车，还要注意抬头看路。那么应该如何走出去看看，看什么呢？我们可以将其划分为两个维度(时间和空间)，三个层次(架构、细节和实现)，如下图：</p><p><img src="https://pic1.zhimg.com/80/v2-14e2ebe32a591e1204dc3340a154f191_720w.png" alt="两维度，三层次"></p><p>两个维度：时间维度上可以分为过去，现在和未来。Google的那些论文就属于未来，看看它们，那可能是未来要做的，当然慢慢地它们也会成为过去；经典的理论的东西，放到过去这个维度，它们是非常重要的，这决定了对系统理解的深度和高度；现在，就是正在做的或者符合目前实际环境可以直接借鉴的。人们有时候往往喜欢抓着未来，总是忽略了过去和现在，又或者是仅看着当前，忽略了未来和过去。空间维度上可以分为上层和底层，上层是指依赖于我们自己系统的那些应用，底层则指我们的系统本身底层所依赖的那些。有时候为了继续前进，需要跳出当前的框框，从多个维度上去学习，通过不断学习反过来进一步促进当前系统的演化。</p><p>三个层次：如果要了解其他系统，可以从三个层次去学习，先大概了解架构，然后深入到一些具体的细节问题，最后如果有时间还可以继续深入到代码级别。结合本文的第一张图来说就是：可以通过Google论文了解整体架构，然后通过开源系统相关wiki或文档可以了解到更细节的一些东西，最后结合开源系统还可以看到实际的代码实现。</p><h2 id="3-2-合久必分，分久必合"><a href="#3-2-合久必分，分久必合" class="headerlink" title="3.2 合久必分，分久必合"></a>3.2 合久必分，分久必合</h2><p>天下大事，合久必分 分久必合——三国演义</p><h3 id="3-2-1-分"><a href="#3-2-1-分" class="headerlink" title="3.2.1 分"></a>3.2.1 分</h3><p><img src="https://pic2.zhimg.com/80/v2-2c8fbb6586b8e17a371f3cb7e6fdfde1_720w.png" alt="img"></p><p>实例：</p><ol><li><p>越来越多的计算被Offload到非CPU的计算单元：Google TPU</p></li><li><p><a href="http://cs.brown.edu/~ugur/fits_all.pdf">One Size Fits All: An Idea Whose Time Has Come and Gone</a>：各种新的计算模型如Pregel MillWheel Dremel PowerDrill Mesa</p></li><li><p>计算存储分离：Mesa CFS+Spanner+F1</p></li></ol><h3 id="3-2-2-合"><a href="#3-2-2-合" class="headerlink" title="3.2.2 合"></a>3.2.2 合</h3><p><a href="http://duanple.com/wp-content/uploads/2019/08/paper4.png"><img src="https://pic1.zhimg.com/80/v2-b189503f559f5f86e0298b340ccbca17_720w.png" alt="img"></a></p><p>实例：</p><ol><li><p>分布式数据库：从MegaStore开始到后来的Spanner F1，不断弥补着NoSQL的不足。同时Spanner自身仍在不断演化，开始具备更加丰富的SQL和OLAP支持。</p></li><li><p>流处理和批处理的统一：Cloud DataFlow完成了编程接口层面的统一，而Mesa则解决了数据层面的结合。</p></li><li><p>在线离线混部：Borg。</p></li><li><p>软硬件结合：整个基础设施，就是在解决一个软件(分布式系统)如何适配新硬件(面向互联网设计的数据中心)的问题。通过上层分布式系统屏蔽底层数据中心细节，实现”Datacenter As a Computer“。</p></li></ol><h2 id="3-3-理论与实践相结合"><a href="#3-3-理论与实践相结合" class="headerlink" title="3.3 理论与实践相结合"></a>3.3 理论与实践相结合</h2><h3 id="3-3-1-”新瓶装旧酒“"><a href="#3-3-1-”新瓶装旧酒“" class="headerlink" title="3.3.1 ”新瓶装旧酒“"></a>3.3.1 ”新瓶装旧酒“</h3><p>纵观过去的20年，我们可以看到如果单纯从理论上看，Google的这些论文并没有提出新理论。它们所依赖的那些基础理论(主要来自分布式系统和关系数据库领域)，基本上都是上个世纪70/80年代就已经提出的。而Google的系统只是把这些经典理论结合自己的业务场景(互联网搜索和广告)，进行了实践并发扬广大使之成为业界潮流。看起来虽然是”新瓶装旧酒”，但是却不能小觑这一点，因为旧酒在新瓶里可能会产生新的化学反应，进而创造出新的完全不同的“酒”。如果忽略了它，当新”酒“成为新浪潮之时，就再也无法站立在浪潮之巅。</p><h3 id="3-3-2-两个阶段"><a href="#3-3-2-两个阶段" class="headerlink" title="3.3.2 两个阶段"></a>3.3.2 两个阶段</h3><p>如果从理论与实践的这个角度来看，我们可以把过去的20年分成两个阶段：前十年主要解决的是可扩展性问题，理论主要源自分布式系统领域；后十年在解决了可扩展问题后，开始考虑易用性问题，提供更加方便的编程接口和一致性模型，这个阶段更多地是借鉴传统关系数据库领域的一些做法。再回到当下，从AI的再度流行中我们依然可以看到其所依赖的理论基础，依然是在上个世纪就已经提出的，而今天在互联网时代大规模的数据和计算能力这个背景下，重新焕发了生命。在解决完可扩展易用性问题后，使得可以对大规模数据进行方便地存储计算和查询之后，下一个十年人们开始关注如何进一步挖掘数据，如何借助这些数据去完成以前未完成的构想，这个过程中仍在不断学习应用前人的经典理论。</p><h3 id="3-3-3-实践联系理论"><a href="#3-3-3-实践联系理论" class="headerlink" title="3.3.3 实践联系理论"></a>3.3.3 实践联系理论</h3><p>从另一个方面来说，如果要真正理解这些论文，除了论文本身内容之外，也还需要去了解传统的分布式系统和关系数据库理论。比如Spanner那篇论文，如果只看论文本身，没有关系数据库和分布式系统理论基础的话估计很难看懂。有时候可能还需要多看看论文的参考文献，之后再看才会理解一些。很多研究领域的大牛们，经常会调侃做工程的家伙们，他们说”这些家伙看着就像生活在5,60年代的老家伙“，为什么呢，因为这些家伙们总是用一些很丑陋的方法去解决一个科学家们早在几十年前就给出了完美解决方案的问题，但是这些家伙看起来对此一无所知。当然了，做工程的也会挖苦下那些研究家们老是指指点点，从来不肯俯下身子来解决实际问题。但是实际上，如果你是做工程的，那就应该多看看研究家们的成果，其实很多问题的确是人家n多年前就已经提出并很好解决了的。如果是做研究的，那就多接触下工程实践，理解下现实需求，弥补下理论与实践的差距。</p><h3 id="3-3-4-分布式理论实践"><a href="#3-3-4-分布式理论实践" class="headerlink" title="3.3.4 分布式理论实践"></a>3.3.4 分布式理论实践</h3><p>具体到分布式系统领域，我们可以发现正是通过与实践相结合，理论才逐渐赢得科学界和工业界的重视。在此之前，分布式理论研究一直处于非常尴尬的状态，与实践的隔阂尤其严重，很多研究工作局限在研究领域，严重脱离现实世界。关于这一点从图灵奖的颁发上可以看出来，自1966年图灵奖首次颁发以来，直到2013年Lamport获奖之前，可以说还没有一个人因为在分布式系统领域的贡献而获得图灵奖。虽然有些获奖者的研究领域也涉及到分布式系统，但是他们获奖更多是因为在其他领域的贡献。而反观程序设计语言/算法/关系数据库等领域均有多人获奖，同时这些领域的研究成果早已被广泛应用在工业界，通过实践证明了其价值。可以说正是因为互联网的兴起，在Google等公司的分布式系统实践下，分布式理论逐渐被广泛应用到各个实际系统中，这也是 Lamport能够获得图灵奖的重要原因。</p><h1 id="4-云计算的起源与发展"><a href="#4-云计算的起源与发展" class="headerlink" title="4.云计算的起源与发展"></a>4.云计算的起源与发展</h1><p>本节我们将跳出Google论文的范畴，以更广泛的视角看一下今天的云计算。下面更多的是描述一些历史，进行一些”考古”，希望这个过程可以带来更多的启发和思考。</p><h2 id="4-1-从Google论文说起"><a href="#4-1-从Google论文说起" class="headerlink" title="4.1 从Google论文说起"></a>4.1 从Google论文说起</h2><h3 id="4-1-1-“冰山一角”"><a href="#4-1-1-“冰山一角”" class="headerlink" title="4.1.1 “冰山一角”"></a>4.1.1 “冰山一角”</h3><p>首先还是回到第一张图，我们把图缩小一下，并重点关注图的顶部。</p><p><a href="http://duanple.com/wp-content/uploads/2019/08/paper.jpg"><img src="https://pic3.zhimg.com/80/v2-89ebe0b3972ada20f8ef8c79b0229ab8_720w.png" alt="img"></a></p><p>可以看到，在Google强大的软硬件基础设施之上，在其云平台上暴露给外部用户使用的则寥寥无几。这个场景就像我们看到了一座冰山，露在水面上的只有那一角。即便是已经开放给外部用户的Cloud Bigtable是2015年才发布的，此时距离Bigtable论文发表已经过了快10年。Cloud Spanner是2017年，也已经是论文发表5年之后。虽然在2008年就推出了GAE，但是也一直不温不火。</p><p>将Google的这些系统与AWS的各种云产品对比一下，可以发现两者的出发点类似都是为了实现”Datacenter As a Computer“，但是目标用户不同。Google这些系统面向的是内部的搜索广告业务，而AWS则致力于让外部客户也能实现”Datacenter As a Computer“。就好比一个是面向大企业客户的国有大银行，一个是面向小微客户的普惠金融。从技术-&gt;产品-&gt;商品-&gt;服务的角度来看，Google在技术上做到了独步天下，但是要提供给外部客户后面的短板仍然需要补足。</p><p>早在2011年，Google员工Amazon前员工Steve Yegge在G+上发表了一篇文章对Google和Amazon进行了有趣的对比：<a href="https://blog.csdn.net/maray/article/details/19553301">Stevey’s Google Platforms Rant</a> ，<a href="https://news.cnblogs.com/n/120344/">中文版</a>。其中非常重要的一点就是Amazon对于服务及服务化的重视。</p><p>2015年Sundar Pichai成为Google新任CEO。进行了一系列调整，找来了VMware的联合创始人Diane Greene领导谷歌的企业及云业务，相关新闻：<a href="http://www.sohu.com/a/67216913_118794">谷歌公有云GCP轰隆崛起？</a>，可以看到Google正在做出很多改变，开始将云计算作为公司重要战略。同时开源了很多技术如Kubernetes和TensorFlow，试图通过容器、CloudNative和AI等新兴领域实现弯道超车。</p><h3 id="4-1-2-为啥要发论文"><a href="#4-1-2-为啥要发论文" class="headerlink" title="4.1.2 为啥要发论文"></a>4.1.2 为啥要发论文</h3><p>还有一个有趣的对比，可以看到在过去20年Google发表了非常多的论文来介绍它的内部系统，但是反观Amazon，对于它的云产品内部实现可以说介绍的非常少，相关论文只有寥寥几篇。</p><p>对于Google来说，发表论文主要是为了增加个人和公司的业界影响力，便于赢得声誉吸引人才。当然Google内部同样有非常严格的保密机制，禁止员工向外界透露内部系统信息，除非获得了授权。通过前面的一些论文也可以看到，从系统做出来上线算，真正论文发表通常是5年之后的事情了，而发表的时候内部已经有下一代系统了。按照中国古话说”富贵不还乡，如锦衣夜行“，内部再牛逼别人看不到就没有存在感。</p><p>反观Amazon，则没有这个苦恼，因为它云平台上的所有系统都是对外开放的，外面的人可以切实地感受到它的存在，大部分情况下都不需要通过论文来提升存在感。</p><h2 id="4-2-”5朵云“的起源"><a href="#4-2-”5朵云“的起源" class="headerlink" title="4.2 ”5朵云“的起源"></a>4.2 ”5朵云“的起源</h2><p>IBM的CEO Thomas J. Watson在1943年说过这样一段话：”I think there is a world market for maybe five computers,” 后来在Cloud Computing概念提出后，逐步演变成5朵云的说法。</p><h2 id="4-3-AWS"><a href="#4-3-AWS" class="headerlink" title="4.3 AWS"></a>4.3 AWS</h2><p>关于售卖计算能力给外部客户的想法最早源自2003年Benjamin Black和Chris Pinkham写的一篇报告中，这个想法引起了Jeff Bezos的兴趣。之后2004年就开干了，当时大家一致觉得Pinkham最适合去干这件事，但是他那个时候正想着回到他的家乡南非，于是Amazon就让他在南非开了新的办公室，在那里他们创建了EC2团队并开发出了EC2。Benjamin Black 在一篇文章(<a href="http://blog.b3k.us/2009/01/25/ec2-origins.html">EC2 Origins</a>)中介绍了这段有趣的历史。</p><p>2006年AWS正式上线了EC2和S3，自此拉开了云计算的序幕。其后续整个发展的详细历程可以参考：<a href="https://en.wikipedia.org/wiki/Timeline_of_Amazon_Web_Services">Timeline of Amazon Web Services</a>。</p><p>此外还有一个比较有意思的问题：<a href="https://www.zhihu.com/question/20058413/answer/325838352">为什么 AWS 云计算服务是亚马逊先做出来，而不是 Google ？</a>其中有偶然也有必然，简要总结一下就是”天时、地利、人和“。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.gcppodcast.com/post/episode-46-borg-and-k8s-with-john-wilkes/">https://www.gcppodcast.com/post/episode-46-borg-and-k8s-with-john-wilkes/</a></p><p><a href="https://blog.risingstack.com/the-history-of-kubernetes/">https://blog.risingstack.com/the-history-of-kubernetes/</a></p><p><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf">Borg, Omega, and Kubernetes</a></p><p><a href="http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/">http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/</a></p><p><a href="https://en.wikipedia.org/wiki/Eric_Brewer_(scientist)">https://en.wikipedia.org/wiki/Eric_Brewer_(scientist)</a></p><p><a href="https://www.zhihu.com/question/35736161">如何看待谷歌工程师透露谷歌有20亿行代码，相当于写40遍Windows？</a></p><p><a href="https://www.oschina.net/translate/google-borg-twitter-mesos">Return of the Borg: How Twitter Rebuilt Google’s Secret Weapon</a></p><p><a href="http://www.infoq.com/cn/news/2014/08/google-data-warehouse-mesa">http://www.infoq.com/cn/news/2014/08/google-data-warehouse-mesa</a></p><p><a href="https://en.wikipedia.org/wiki/Amazon_Web_Services">https://en.wikipedia.org/wiki/Amazon_Web_Services</a></p><p><a href="https://en.wikipedia.org/wiki/Thomas_J._Watson">https://en.wikipedia.org/wiki/Thomas_J._Watson</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;1-Google论文与开源&quot;&gt;&lt;a href=&quot;#1-Google</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Google" scheme="http://posts.hufeifei.cn/tags/Google/"/>
    
    <category term="OpenSource" scheme="http://posts.hufeifei.cn/tags/OpenSource/"/>
    
    <category term="Cloud" scheme="http://posts.hufeifei.cn/tags/Cloud/"/>
    
  </entry>
  
  <entry>
    <title>换零钱问题</title>
    <link href="http://posts.hufeifei.cn/algorithm/change-money/"/>
    <id>http://posts.hufeifei.cn/algorithm/change-money/</id>
    <published>2021-10-27T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="换零钱问题"><a href="#换零钱问题" class="headerlink" title="换零钱问题"></a>换零钱问题</h2><p>在《SICP》第一章 1.22节，刚介绍完递归和迭代，给了一道相当有难度的例题：统计换零钱的方法</p><blockquote><p>给定半美元、25美分、10美分、5美分、1美分 5种硬币，将 1 美元换成硬币，有多少种硬币组合？<br>给定任意数量的现金 和 任意组合的硬币种类，计算换零钱所有方式的种数。</p></blockquote><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>书上给出的解题思想为递归分治： 将总数为 a 的现金换成 n 种硬币组合的数目等于两部分之和：</p><ul><li>第一部分为现金 <code>a</code> 换成除第一种硬币外的所有其他硬币种类的数目</li><li>第二部分为现金数 <code>a-d</code> 换成所有种类硬币的不同方式数目，其中 <code>d</code> 为第一种硬币的币值。</li></ul><p>怎么来理解呢，这里是将换零钱的方法分为两组，第一组是都没有使用第一种硬币，第二组是都用了第一咱硬币。显然，换成零钱的全部方式就等于完全不用第一种硬币的方式的数目，加上用了第一种硬币的方式的数目。可以看出，分成两组后，这两个问题新问题相对于原问题的范围缩小了，第一组现金没有变，但可选的硬币种类少了一种；第二组硬币种类没有变，但是现金减少了第一种硬币的币值。<br>这样，就可以将给定金额的换零钱方式问题，通过递归化简为对 更少现金 和 更少种类硬币的同一问题，而求解这样的递归子问题，我们只需确定好问题的边界在哪里，就可以完成运算。边界问题如下：</p><ul><li>当现金数 <code>a</code> 为 0 时，应该算作是有 1 种换零钱的方法</li><li>当现金数 <code>a</code> 小于 0 时，应该算作是有 0 种换零钱的方法</li><li>当换零钱可选的硬币种类为 0 时，应该算作是有 0 种换零钱的方法</li></ul><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>光看这些解释还不够直观，我们来用 10 美分来演算一下。</p><p>10 美分能够用到的硬币种类只有<code>[10, 5, 1]</code>三种。我们用 <code>10$[10, 5, 1]</code>这种记法来标记问题。先按币值为10的硬币来分类，将问题化为：</p><ol><li>第一组，不使用 10 美分的硬币来表示，只用5美分和1美分来表示 即 <code>10$[5, 1]</code></li><li>第二组，使用了 10 美分的硬币，剩下的金额 0 使用全部的硬币种类表示，即 <code>0$[10, 5, 1]</code></li></ol><p>根据上面的规则，当现金数为0 时，应该算作是有 1 种换零钱的方法，所以第二组的值为 1。我们只用求解第一组的 <code>10$[5, 1]</code>, 然后加上第二组的 1 就是问题 <code>10$[10, 5, 1]</code> 的结果。继续化简问题 <code>10$[5, 1]</code>，按币值为 5 的硬币来分解:</p><ul><li>1-1. 第一组，不使用 5 美分的硬币，只用 1 美分的币值来表示 10 美分，即 <code>10$[1]</code></li><li>1-2. 第二组，使用了一个 5 美分，剩下的 5 美分仍用<code>[5, 1]</code> 两种来表示，即 <code>5$[5, 1]</code></li></ul><p>其中第一组的结果为 1 ，继续求解第二组问题 <code>5$[5, 1]</code>，仍按 5 美分来分类，可以分为：</p><ul><li>1-2-1. 第一组，不使用 5 美分的硬币，只用 1 美分的币值来表示 5 美分，即 <code>5$[1]</code></li><li>1-2-2. 第二组，使用一个 5 美分的硬币，剩下的金额 0 使用全部的硬币种类表示 <code>0$[5, 1]</code></li></ul><p>综上所述，10 美分金额的硬币换的结果为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>$[<span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>] = <span class="number">10</span>$[<span class="number">5</span>, <span class="number">1</span>] + <span class="number">0</span>$[<span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">              = <span class="number">10</span>$[<span class="number">1</span>] + <span class="number">5</span>$[<span class="number">5</span>, <span class="number">1</span>] + <span class="number">0</span>$[<span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">              = <span class="number">10</span>$[<span class="number">1</span>] + <span class="number">5</span>$[<span class="number">1</span>] + <span class="number">0</span>$[<span class="number">5</span>, <span class="number">1</span>] + <span class="number">0</span>$[<span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>按边界规则，可以直观看出，当金额为 0 或 可选币种只有 1 种时，组合数都为 1 。<br>所以 <code>10$[10, 5, 1]</code> 的最终结果为 4 。这其中的思想就是将一个复杂的问题归约为简单的子问题求解。</p><h2 id="scheme代码"><a href="#scheme代码" class="headerlink" title="scheme代码"></a>scheme代码</h2><p>书上 1.22节给出的示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lang planet neil/sicp</span></span><br><span class="line">(define (count-change amount)</span><br><span class="line">  (cc amount <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">(define (cc amount kinds-of-coins)</span><br><span class="line">  (cond ((= amount <span class="number">0</span>) <span class="number">1</span>)</span><br><span class="line">        ((<span class="keyword">or</span> (&lt; amount <span class="number">0</span>) (= kinds-of-coins <span class="number">0</span>)) <span class="number">0</span>)</span><br><span class="line">        (<span class="keyword">else</span> (+ (cc amount (- kinds-of-coins <span class="number">1</span>))</span><br><span class="line">                 (cc (- amount</span><br><span class="line">                        (first-denomination kinds-of-coins))</span><br><span class="line">                     kinds-of-coins)))))</span><br><span class="line"></span><br><span class="line">(define (first-denomination kinds-of-coins)</span><br><span class="line">  (cond ((= kinds-of-coins <span class="number">1</span>) <span class="number">1</span>)</span><br><span class="line">        ((= kinds-of-coins <span class="number">2</span>) <span class="number">5</span>)</span><br><span class="line">        ((= kinds-of-coins <span class="number">3</span>) <span class="number">10</span>)</span><br><span class="line">        ((= kinds-of-coins <span class="number">4</span>) <span class="number">25</span>)</span><br><span class="line">        ((= kinds-of-coins <span class="number">5</span>) <span class="number">50</span>)))</span><br><span class="line"></span><br><span class="line">(count-change <span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>由于第一章刚刚介绍完 Lisp 的判断语句，没有学习数据结构，需要用判断语句来实现硬币种类，所以上面的示例相对麻烦一些。<br>在学习完第二章的数据结构链表后，可以用表来表示硬币的种类和币值，这样可以轻易更换兑换币种。将上面的程序再重写一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lang planet neil/sicp</span></span><br><span class="line"></span><br><span class="line">(define us-coins (<span class="built_in">list</span> <span class="number">50</span> <span class="number">25</span> <span class="number">10</span> <span class="number">5</span> <span class="number">1</span>))</span><br><span class="line">(define uk-coins (<span class="built_in">list</span> <span class="number">100</span> <span class="number">50</span> <span class="number">20</span> <span class="number">10</span> <span class="number">5</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">(define (new-cc amount coin-values)</span><br><span class="line">  (cond ((= amount <span class="number">0</span>) <span class="number">1</span>)</span><br><span class="line">        (( <span class="keyword">or</span> (&lt; amount <span class="number">0</span>) (null? coin-values)) <span class="number">0</span>)</span><br><span class="line">        (<span class="keyword">else</span></span><br><span class="line">         (+ (new-cc amount</span><br><span class="line">                    (cdr coin-values))</span><br><span class="line">            (new-cc (- amount</span><br><span class="line">                       (car coin-values))</span><br><span class="line">                    coin-values)))))</span><br><span class="line"></span><br><span class="line">(new-cc <span class="number">100</span> us-coins)</span><br><span class="line">(new-cc <span class="number">100</span> uk-coins)  </span><br></pre></td></tr></table></figure><p>上面的两种方案都是用递归来化简问题，其中用 <code>car</code>、 <code>cdr</code>、 <code>null?</code> 来实现对列表的操作。</p><h2 id="Python代码"><a href="#Python代码" class="headerlink" title="Python代码"></a>Python代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- coding = utf-8 --</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_change</span>(<span class="params">amount, coins_list</span>):</span></span><br><span class="line">    <span class="keyword">if</span> amount == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> amount &lt; <span class="number">0</span> <span class="keyword">or</span> <span class="keyword">not</span> coins_list:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        part1 = count_change(amount, coins_list[<span class="number">1</span>:])</span><br><span class="line">        part2 = count_change(amount-coins_list[<span class="number">0</span>], coins_list[:])</span><br><span class="line">    <span class="keyword">return</span> part1 + part2</span><br><span class="line"></span><br><span class="line">us_coins = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">25</span>, <span class="number">50</span>]</span><br><span class="line"><span class="built_in">print</span>(count_change(<span class="number">100</span>, us_coins)</span><br></pre></td></tr></table></figure><p>上面代码使用递归方法实现，逻辑非常清楚直观。缺点也比较明显，一是Python 有递归深度限制，当数据金额特别大时可能报错。二是空间复用度高。</p><p>凡是递归能解决的问题都可以用迭代解决，具体到这个题目，可以用动态规划的思路来解决。 下面是网上大神的动态规划代码，特别优雅，只有6行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- coding = utf-8 --</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_change_iterate</span>(<span class="params">count, coins_list</span>):</span></span><br><span class="line">    result=[<span class="number">1</span>] + [<span class="number">0</span>] * (count) <span class="comment"># 设置结果集</span></span><br><span class="line">    <span class="keyword">for</span> coin_value <span class="keyword">in</span> coins_list: <span class="comment"># 控制变量，硬币种类从第一种开始，逐一添加</span></span><br><span class="line">        <span class="keyword">for</span> amount <span class="keyword">in</span> <span class="built_in">range</span>(coin_value, count+<span class="number">1</span>):  </span><br><span class="line">            <span class="comment"># result[amount] 存储未添加当前种类硬币币值时 金额 amount 全部兑换的方式</span></span><br><span class="line">            <span class="comment"># result[amount - coin_value] 为包括当前币值时 金额 amount - coin_value可以兑换的方式</span></span><br><span class="line">            <span class="comment"># 两者相加，则为将 result[amount] 更新为 包括当前币值金额 amount可以兑换的方式</span></span><br><span class="line">            result[amount] += result[amount - coin_value]</span><br><span class="line">    <span class="keyword">return</span> result[count]</span><br><span class="line"></span><br><span class="line">us_coins = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">25</span>, <span class="number">50</span>]</span><br><span class="line"><span class="built_in">print</span>(count_change_iterate(<span class="number">100</span>, us_coins)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过对换零钱问题的分析和求解，让我们更深入的理分治算法的思想，就是将复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h2 id=&quot;换零钱问题&quot;&gt;&lt;a href=&quot;#换零钱问题&quot; class=&quot;he</summary>
      
    
    
    
    <category term="算法" scheme="http://posts.hufeifei.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="algorithm" scheme="http://posts.hufeifei.cn/tags/algorithm/"/>
    
    <category term="递归" scheme="http://posts.hufeifei.cn/tags/%E9%80%92%E5%BD%92/"/>
    
    <category term="动态规划" scheme="http://posts.hufeifei.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Docker 核心技术与实现原理</title>
    <link href="http://posts.hufeifei.cn/backend/docker-core-technologies/"/>
    <id>http://posts.hufeifei.cn/backend/docker-core-technologies/</id>
    <published>2021-10-20T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Docker 已经成为了很多公司的标配，也不再是一个只能在开发阶段使用的玩具了。作为在生产环境中广泛应用的产品，Docker 有着非常成熟的社区以及大量的使用者，代码库中的内容也变得非常庞大。</p><p><img src="https://img.draveness.me/2017-11-30-docker-logo.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-logo"></p><p>同样，由于项目的发展、功能的拆分以及各种奇怪的改名 <a href="https://github.com/moby/moby/pull/32691">PR</a>，让我们再次理解 Docker 的整体架构变得更加困难。</p><p>虽然 Docker 目前的组件较多，并且实现也非常复杂，但是本文不想过多的介绍 Docker 具体的实现细节，我们更想谈一谈 Docker 这种虚拟化技术的出现有哪些核心技术的支撑。</p><p><img src="https://img.draveness.me/2017-11-30-docker-core-techs.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-core-techs"></p><p>首先，Docker 的出现一定是因为目前的后端在开发和运维阶段确实需要一种虚拟化技术解决开发环境和生产环境环境一致的问题，通过 Docker 我们可以将程序运行的环境也纳入到版本控制中，排除因为环境造成不同运行结果的可能。但是上述需求虽然推动了虚拟化技术的产生，但是如果没有合适的底层技术支撑，那么我们仍然得不到一个完美的产品。本文剩下的内容会介绍几种 Docker 使用的核心技术，如果我们了解它们的使用方法和原理，就能清楚 Docker 的实现原理。</p><h1 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h1><p>命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到<strong>完全隔离</strong>，就像运行在多台不同的机器上一样。</p><p><img src="https://img.draveness.me/2017-11-30-multiple-servers-on-linux.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="multiple-servers-on-linux"></p><p>在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 Docker 其实就通过 Linux 的 Namespaces 对不同的容器实现了隔离。</p><p>Linux 的命名空间机制提供了以下七种不同的命名空间，包括 <code>CLONE_NEWCGROUP</code>、<code>CLONE_NEWIPC</code>、<code>CLONE_NEWNET</code>、<code>CLONE_NEWNS</code>、<code>CLONE_NEWPID</code>、<code>CLONE_NEWUSER</code> 和 <code>CLONE_NEWUTS</code>，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。</p><h1 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h1><p>进程是 Linux 以及现在操作系统中非常重要的概念，它表示一个正在执行的程序，也是在现代分时系统中的一个任务单元。在每一个 *nix 的操作系统上，我们都能够通过 <code>ps</code> 命令打印出当前操作系统中正在执行的进程，比如在 Ubuntu 上，使用该命令就能得到以下的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps -ef</span></span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 Apr08 ?        00:00:09 /sbin/init</span><br><span class="line">root         2     0  0 Apr08 ?        00:00:00 [kthreadd]</span><br><span class="line">root         3     2  0 Apr08 ?        00:00:05 [ksoftirqd/0]</span><br><span class="line">root         5     2  0 Apr08 ?        00:00:00 [kworker/0:0H]</span><br><span class="line">root         7     2  0 Apr08 ?        00:07:10 [rcu_sched]</span><br><span class="line">root        39     2  0 Apr08 ?        00:00:00 [migration/0]</span><br><span class="line">root        40     2  0 Apr08 ?        00:01:54 [watchdog/0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>当前机器上有很多的进程正在执行，在上述进程中有两个非常特殊，一个是 <code>pid</code> 为 1 的 <code>/sbin/init</code> 进程，另一个是 <code>pid</code> 为 2 的 <code>kthreadd</code> 进程，这两个进程都是被 Linux 中的上帝进程 <code>idle</code> 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 <code>getty</code> 的注册进程，而后者负责管理和调度其他的内核进程。</p><p><img src="https://img.draveness.me/2017-11-30-linux-processes.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="linux-processes"></p><p>如果我们在当前的 Linux 操作系统下运行一个新的 Docker 容器，并通过 <code>exec</code> 进入其内部的 <code>bash</code> 并打印其中的全部进程，我们会得到以下的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@iZ255w13cy6Z:~# docker run -it -d ubuntu</span><br><span class="line">b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79</span><br><span class="line">root@iZ255w13cy6Z:~# docker exec -it b809a2eb3630 /bin/bash</span><br><span class="line">root@b809a2eb3630:/# ps -ef</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 15:42 pts/0    00:00:00 /bin/bash</span><br><span class="line">root         9     0  0 15:42 pts/1    00:00:00 /bin/bash</span><br><span class="line">root        17     9  0 15:43 pts/1    00:00:00 ps -ef</span><br></pre></td></tr></table></figure><p>在新的容器内部执行 <code>ps</code> 命令打印出了非常干净的进程列表，只有包含当前 <code>ps -ef</code> 在内的三个进程，在宿主机器上的几十个进程都已经消失不见了。</p><p>当前的 Docker 容器成功将容器内的进程与宿主机器中的进程隔离，如果我们在宿主机器上打印当前的全部进程时，会得到下面三条与 Docker 相关的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root     29407     1  0 Nov16 ?        00:08:38 /usr/bin/dockerd --raw-logs</span><br><span class="line">root      1554 29407  0 Nov19 ?        00:03:28 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc</span><br><span class="line">root      5006  1554  0 08:38 ?        00:00:00 docker-containerd-shim b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 /var/run/docker/libcontainerd/b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 docker-runc</span><br></pre></td></tr></table></figure><p>在当前的宿主机器上，可能就存在由上述的不同进程构成的进程树：</p><p><img src="https://img.draveness.me/2017-11-30-docker-process-group.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-process-group"></p><p>这就是在使用 <code>clone(2)</code> 创建新进程时传入 <code>CLONE_NEWPID</code> 实现的，也就是使用 Linux 的命名空间实现进程的隔离，Docker 容器内部的任意进程都对宿主机器的进程一无所知。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">containerRouter.postContainersStart</span><br><span class="line">└── daemon.ContainerStart</span><br><span class="line">    └── daemon.createSpec</span><br><span class="line">        └── setNamespaces</span><br><span class="line">            └── setNamespace</span><br></pre></td></tr></table></figure><p>Docker 的容器就是使用上述技术实现与宿主机器的进程隔离，当我们每次运行 <code>docker run</code> 或者 <code>docker start</code> 时，都会在下面的方法中创建一个用于设置进程间隔离的 Spec：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(daemon *Daemon)</span> <span class="title">createSpec</span><span class="params">(c *container.Container)</span> <span class="params">(*specs.Spec, error)</span></span> &#123;</span><br><span class="line">s := oci.DefaultSpec()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">if</span> err := setNamespaces(daemon, &amp;s, c); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;linux spec namespaces: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &amp;s, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>setNamespaces</code> 方法中不仅会设置进程相关的命名空间，还会设置与用户、网络、IPC 以及 UTS 相关的命名空间：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">setNamespaces</span><span class="params">(daemon *Daemon, s *specs.Spec, c *container.Container)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="comment">// user</span></span><br><span class="line"><span class="comment">// network</span></span><br><span class="line"><span class="comment">// ipc</span></span><br><span class="line"><span class="comment">// uts</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// pid</span></span><br><span class="line"><span class="keyword">if</span> c.HostConfig.PidMode.IsContainer() &#123;</span><br><span class="line">ns := specs.LinuxNamespace&#123;Type: <span class="string">&quot;pid&quot;</span>&#125;</span><br><span class="line">pc, err := daemon.getPidContainer(c)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">ns.Path = fmt.Sprintf(<span class="string">&quot;/proc/%d/ns/pid&quot;</span>, pc.State.GetPID())</span><br><span class="line">setNamespace(s, ns)</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> c.HostConfig.PidMode.IsHost() &#123;</span><br><span class="line">oci.RemoveNamespace(s, specs.LinuxNamespaceType(<span class="string">&quot;pid&quot;</span>))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ns := specs.LinuxNamespace&#123;Type: <span class="string">&quot;pid&quot;</span>&#125;</span><br><span class="line">setNamespace(s, ns)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有命名空间相关的设置 <code>Spec</code> 最后都会作为 <code>Create</code> 函数的入参在创建新的容器时进行设置：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">daemon.containerd.Create(context.Background(), container.ID, spec, createOptions)</span><br></pre></td></tr></table></figure><p>所有与命名空间的相关的设置都是在上述的两个函数中完成的，Docker 通过命名空间成功完成了与宿主机进程和网络的隔离。</p><h1 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h1><p>如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却又没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。</p><p>每一个使用 <code>docker run</code> 启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。</p><p><img src="https://img.draveness.me/2017-11-30-docker-network.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-network"></p><p>在这一部分，我们将介绍 Docker 默认的网络设置模式：网桥模式。在这种模式下，除了分配隔离的网络命名空间之外，Docker 还会为所有的容器设置 IP 地址。当 Docker 服务器在主机上启动之后会创建新的虚拟网桥 docker0，随后在该主机上启动的全部服务在默认情况下都与该网桥相连。</p><p><img src="https://img.draveness.me/2017-11-30-docker-network-topology.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-network-topology"></p><p>在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">docker08000.0242a6654980noveth3e84d4f</span><br><span class="line">            veth9953b75</span><br></pre></td></tr></table></figure><p>docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L</span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain DOCKER (2 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">RETURN     all  --  anywhere             anywhere</span><br></pre></td></tr></table></figure><p>我们在当前的机器上使用 <code>docker run -d -p 6379:6379 redis</code> 命令启动了一个新的 Redis 容器，在这之后我们再查看当前 <code>iptables</code> 的 NAT 配置就会看到在 <code>DOCKER</code> 的链中出现了一条新的规则：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DNAT       tcp  --  anywhere             anywhere             tcp dpt:6379 to:192.168.0.4:6379</span><br></pre></td></tr></table></figure><p>上述规则会将从任意源发送到当前机器 6379 端口的 TCP 包转发到 192.168.0.4:6379 所在的地址上。</p><p>这个地址其实也是 Docker 为 Redis 服务分配的 IP 地址，如果我们在当前机器上直接 ping 这个 IP 地址就会发现它是可以访问到的：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ping 192.168.0.4</span><br><span class="line">PING 192.168.0.4 (192.168.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=0.069 ms</span><br><span class="line">64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=0.043 ms</span><br><span class="line">^C</span><br><span class="line">--- 192.168.0.4 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.043/0.056/0.069/0.013 ms</span><br></pre></td></tr></table></figure><p>从上述的一系列现象，我们就可以推测出 Docker 是如何将容器的内部的端口暴露出来并对数据包进行转发的了；当有 Docker 的容器需要将服务暴露给宿主机器，就会为容器分配一个 IP 地址，同时向 iptables 中追加一条新的规则。</p><p><img src="https://img.draveness.me/2017-11-30-docker-network-forward.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-network-forward"></p><p>当我们使用 <code>redis-cli</code> 在宿主机器的命令行中访问 127.0.0.1:6379 的地址时，经过 iptables 的 NAT PREROUTING 将 ip 地址定向到了 192.168.0.4，重定向过的数据包就可以通过 iptables 中的 FILTER 配置，最终在 NAT POSTROUTING 阶段将 ip 地址伪装成 127.0.0.1，到这里虽然从外面看起来我们请求的是 127.0.0.1:6379，但是实际上请求的已经是 Docker 容器暴露出的端口了。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ redis-cli -h 127.0.0.1 -p 6379 ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure><p>Docker 通过 Linux 的命名空间实现了网络的隔离，又通过 iptables 进行数据包转发，让 Docker 容器能够优雅地为宿主机器或者其他容器提供服务。</p><h2 id="libnetwork"><a href="#libnetwork" class="headerlink" title="libnetwork"></a>libnetwork</h2><p>整个网络部分的功能都是通过 Docker 拆分出来的 libnetwork 实现的，它提供了一个连接不同容器的实现，同时也能够为应用给出一个能够提供一致的编程接口和网络层抽象的<strong>容器网络模型</strong>。</p><blockquote><p>The goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications.</p></blockquote><p>libnetwork 中最重要的概念，容器网络模型由以下的几个主要组件组成，分别是 Sandbox、Endpoint 和 Network：</p><p><img src="https://img.draveness.me/2017-11-30-container-network-model.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="container-network-model"></p><p>在容器网络模型中，每一个容器内部都包含一个 Sandbox，其中存储着当前容器的网络栈配置，包括容器的接口、路由表和 DNS 设置，Linux 使用网络命名空间实现这个 Sandbox，每一个 Sandbox 中都可能会有一个或多个 Endpoint，在 Linux 上就是一个虚拟的网卡 veth，Sandbox 通过 Endpoint 加入到对应的网络中，这里的网络可能就是我们在上面提到的 Linux 网桥或者 VLAN。</p><blockquote><p>想要获得更多与 libnetwork 或者容器网络模型相关的信息，可以阅读 <a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">Design · libnetwork</a> 了解更多信息，当然也可以阅读源代码了解不同 OS 对容器网络模型的不同实现。</p></blockquote><h1 id="挂载点"><a href="#挂载点" class="headerlink" title="挂载点"></a>挂载点</h1><p>虽然我们已经通过 Linux 的命名空间解决了进程和网络隔离的问题，在 Docker 进程中我们已经没有办法访问宿主机器上的其他进程并且限制了网络的访问，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。</p><p>在新的进程中创建隔离的挂载点命名空间需要在 <code>clone</code> 函数中传入 <code>CLONE_NEWNS</code>，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数<strong>子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统</strong>。</p><p>如果一个容器需要启动，那么它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中。</p><p><img src="https://img.draveness.me/2017-11-30-libcontainer-filesystem.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="libcontainer-filesystem"></p><p>想要正常启动一个容器就需要在 rootfs 中挂载以上的几个特定的目录，除了上述的几个目录需要挂载之外我们还需要建立一些符号链接保证系统 IO 不会出现问题。</p><p><img src="https://img.draveness.me/2017-11-30-libcontainer-symlinks-and-io.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="libcontainer-symlinks-and-io"></p><p>为了保证当前的容器进程没有办法访问宿主机器上其他目录，我们在这里还需要通过 libcontainer 提供的 <code>pivot_root</code> 或者 <code>chroot</code> 函数改变进程能够访问个文件目录的根节点。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pivor_root</span></span><br><span class="line">put_old = mkdir(...);</span><br><span class="line">pivot_root(rootfs, put_old);</span><br><span class="line">chdir(<span class="string">&quot;/&quot;</span>);</span><br><span class="line">unmount(put_old, MS_DETACH);</span><br><span class="line">rmdir(put_old);</span><br><span class="line"></span><br><span class="line"><span class="comment">// chroot</span></span><br><span class="line">mount(rootfs, <span class="string">&quot;/&quot;</span>, <span class="literal">NULL</span>, MS_MOVE, <span class="literal">NULL</span>);</span><br><span class="line">chroot(<span class="string">&quot;.&quot;</span>);</span><br><span class="line">chdir(<span class="string">&quot;/&quot;</span>);</span><br></pre></td></tr></table></figure><p>到这里我们就将容器需要的目录挂载到了容器中，同时也禁止当前的容器进程访问宿主机器上的其他目录，保证了不同文件系统的隔离。</p><blockquote><p>这一部分的内容是作者在 libcontainer 中的 <a href="https://github.com/opencontainers/runc/blob/master/libcontainer/SPEC.md">SPEC.md</a> 文件中找到的，其中包含了 Docker 使用的文件系统的说明，对于 Docker 是否真的使用 <code>chroot</code> 来确保当前的进程无法访问宿主机器的目录，作者其实也<strong>没有确切的答案</strong>，一是 Docker 项目的代码太多庞大，不知道该从何入手，作者尝试通过 Google 查找相关的结果，但是既找到了无人回答的 <a href="https://forums.docker.com/t/does-the-docker-engine-use-chroot/25429">问题</a>，也得到了与 SPEC 中的描述有冲突的 <a href="https://www.quora.com/Do-Docker-containers-use-a-chroot-environment">答案</a> ，如果各位读者有明确的答案可以在博客下面留言，非常感谢。</p></blockquote><h2 id="chroot"><a href="#chroot" class="headerlink" title="chroot"></a>chroot</h2><p>在这里不得不简单介绍一下 <code>chroot</code>（change root），在 Linux 系统中，系统默认的目录就都是以 <code>/</code> 也就是根目录开头的，<code>chroot</code> 的使用能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。</p><blockquote><p>与 chroot 的相关内容部分来自 <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html">理解 chroot</a> 一文，各位读者可以阅读这篇文章获得更详细的信息。</p></blockquote><h1 id="CGroups"><a href="#CGroups" class="headerlink" title="CGroups"></a>CGroups</h1><p>我们通过 Linux 的命名空间为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是命名空间并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存，如果在同一台机器上运行了多个对彼此以及宿主机器一无所知的『容器』，这些容器却共同占用了宿主机器的物理资源。</p><p><img src="https://img.draveness.me/2017-11-30-docker-shared-resources.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-shared-resources"></p><p>如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题，而 Control Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。</p><p>每一个 CGroup 都是一组被相同的标准和参数限制的进程，不同的 CGroup 之间是有层级关系的，也就是说它们之间可以从父类继承一些用于限制资源使用的标准和参数。</p><p><img src="https://img.draveness.me/2017-11-30-cgroups-inheritance.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="cgroups-inheritance"></p><p>Linux 的 CGroup 能够为一组进程分配资源，也就是我们在上面提到的 CPU、内存、网络带宽等资源，通过对资源的分配，CGroup 能够提供以下的几种功能：</p><p><img src="https://img.draveness.me/2017-11-30-groups-features.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="groups-features"></p><blockquote><p>在 CGroup 中，所有的任务就是一个系统的一个进程，而 CGroup 就是一组按照某种标准划分的进程，在 CGroup 这种机制中，所有的资源控制都是以 CGroup 作为单位实现的，每一个进程都可以随时加入一个 CGroup 也可以随时退出一个 CGroup。</p><p>– <a href="https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html">CGroup 介绍、应用实例及原理描述</a></p></blockquote><p>Linux 使用文件系统来实现 CGroup，我们可以直接使用下面的命令查看当前的 CGroup 中有哪些子系统：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ lssubsys -m</span><br><span class="line">cpuset /sys/fs/cgroup/cpuset</span><br><span class="line">cpu /sys/fs/cgroup/cpu</span><br><span class="line">cpuacct /sys/fs/cgroup/cpuacct</span><br><span class="line">memory /sys/fs/cgroup/memory</span><br><span class="line">devices /sys/fs/cgroup/devices</span><br><span class="line">freezer /sys/fs/cgroup/freezer</span><br><span class="line">blkio /sys/fs/cgroup/blkio</span><br><span class="line">perf_event /sys/fs/cgroup/perf_event</span><br><span class="line">hugetlb /sys/fs/cgroup/hugetlb</span><br></pre></td></tr></table></figure><p>大多数 Linux 的发行版都有着非常相似的子系统，而之所以将上面的 cpuset、cpu 等东西称作子系统，是因为它们能够为对应的控制组分配资源并限制资源的使用。</p><p>如果我们想要创建一个新的 cgroup 只需要在想要分配或者限制资源的子系统下面创建一个新的文件夹，然后这个文件夹下就会自动出现很多的内容，如果你在 Linux 上安装了 Docker，你就会发现所有子系统的目录下都有一个名为 docker 的文件夹：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls cpu</span></span><br><span class="line">cgroup.clone_children  </span><br><span class="line">...</span><br><span class="line">cpu.stat  </span><br><span class="line">docker  </span><br><span class="line">notify_on_release </span><br><span class="line">release_agent </span><br><span class="line">tasks</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls cpu/docker/</span></span><br><span class="line">9c3057f1291b53fd54a3d12023d2644efe6a7db6ddf330436ae73ac92d401cf1 </span><br><span class="line">cgroup.clone_children  </span><br><span class="line">...</span><br><span class="line">cpu.stat  </span><br><span class="line">notify_on_release </span><br><span class="line">release_agent </span><br><span class="line">tasks</span><br></pre></td></tr></table></figure><p><code>9c3057xxx</code> 其实就是我们运行的一个 Docker 容器，启动这个容器时，Docker 会为这个容器创建一个与容器标识符相同的 CGroup，在当前的主机上 CGroup 就会有以下的层级关系：</p><p><img src="https://img.draveness.me/2017-11-30-linux-cgroups.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="linux-cgroups"></p><p>每一个 CGroup 下面都有一个 <code>tasks</code> 文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，<code>cpu.cfs_quota_us</code> 文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。</p><p>如果系统管理员想要控制 Docker 某个容器的资源使用率就可以在 <code>docker</code> 这个父控制组下面找到对应的子控制组并且改变它们对应文件的内容，当然我们也可以直接在程序运行时就使用参数，让 Docker 进程去改变相应文件中的内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -it -d --cpu-quota=50000 busybox</span></span><br><span class="line">53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> 53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">cgroup.clone_children  cgroup.event_control  cgroup.procs  cpu.cfs_period_us  cpu.cfs_quota_us  cpu.shares  cpu.stat  notify_on_release  tasks</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat cpu.cfs_quota_us</span></span><br><span class="line">50000</span><br></pre></td></tr></table></figure><p>当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除，Docker 在使用 CGroup 时其实也只是做了一些创建文件夹改变文件内容的文件操作，不过 CGroup 的使用也确实解决了我们限制子容器资源占用的问题，系统管理员能够为多个容器合理的分配资源并且不会出现多个容器互相抢占资源的问题。</p><h1 id="UnionFS"><a href="#UnionFS" class="headerlink" title="UnionFS"></a>UnionFS</h1><p>Linux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。</p><p>镜像到底是什么，它又是如何组成和组织的是作者使用 Docker 以来的一段时间内一直比较让作者感到困惑的问题，我们可以使用 <code>docker run</code> 非常轻松地从远程下载 Docker 的镜像并在本地运行。</p><p>Docker 镜像其实本质就是一个压缩包，我们可以使用下面的命令将一个 Docker 镜像中的文件导出：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">export</span> $(docker create busybox) | tar -C rootfs -xvf -</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">bin  dev  etc  home proc root sys  tmp  usr  var</span><br></pre></td></tr></table></figure><p>你可以看到这个 busybox 镜像中的目录结构与 Linux 操作系统的根目录中的内容并没有太多的区别，可以说 <strong>Docker 镜像就是一个文件</strong>。</p><h1 id="存储驱动"><a href="#存储驱动" class="headerlink" title="存储驱动"></a>存储驱动</h1><p>Docker 使用了一系列不同的存储驱动管理镜像内的文件系统并运行容器，这些存储驱动与 Docker 卷（volume）有些不同，存储引擎管理着能够在多个容器之间共享的存储。</p><p>想要理解 Docker 使用的存储驱动，我们首先需要理解 Docker 是如何构建并且存储镜像的，也需要明白 Docker 的镜像是如何被每一个容器所使用的；Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层：</p><figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">15.04</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> make /app</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> python /app/app.py</span></span><br></pre></td></tr></table></figure><p>容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像：</p><p><img src="https://img.draveness.me/2017-11-30-docker-container-layer.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-container-laye"></p><p>当镜像被 <code>docker run</code> 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。</p><p>容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。</p><p><img src="https://img.draveness.me/2017-12-06-docker-images-and-container.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-images-and-container"></p><h1 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h1><p>UnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统『联合』到同一个挂载点的文件系统服务。而 AUFS 即 Advanced UnionFS 其实就是 UnionFS 的升级版，它能够提供更优秀的性能和效率。</p><p>AUFS 作为联合文件系统，它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，这些文件夹在 AUFS 中称作分支，整个『联合』的过程被称为<em>联合挂载（Union Mount）</em>：</p><p><img src="https://img.draveness.me/2017-11-30-docker-aufs.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-aufs"></p><p>每一个镜像层或者容器层都是 <code>/var/lib/docker/</code> 目录下的一个子文件夹；在 Docker 中，所有镜像层和容器层的内容都存储在 <code>/var/lib/docker/aufs/diff/</code> 目录中：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls /var/lib/docker/aufs/diff/00adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c       93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d8</span></span><br><span class="line">00adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c-init  93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d8-init</span><br><span class="line">019a8283e2ff6fca8d0a07884c78b41662979f848190f0658813bb6a9a464a90       93b06191602b7934fafc984fbacae02911b579769d0debd89cf2a032e7f35cfa</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>而 <code>/var/lib/docker/aufs/layers/</code> 中存储着镜像层的元数据，每一个文件都保存着镜像层的元数据，最后的 <code>/var/lib/docker/aufs/mnt/</code> 包含镜像或者容器层的挂载点，最终会被 Docker 通过联合的方式进行组装。</p><p><img src="https://img.draveness.me/2017-11-30-docker-filesystems.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-filesystems"></p><p>上面的这张图片非常好的展示了组装的过程，每一个镜像层都是建立在另一个镜像层之上的，同时所有的镜像层都是只读的，只有每个容器最顶层的容器层才可以被用户直接读写，所有的容器都建立在一些底层服务（Kernel）上，包括命名空间、控制组、rootfs 等等，这种容器的组装方式提供了非常大的灵活性，只读的镜像层通过共享也能够减少磁盘的占用。</p><h1 id="其他存储驱动"><a href="#其他存储驱动" class="headerlink" title="其他存储驱动"></a>其他存储驱动</h1><p>AUFS 只是 Docker 使用的存储驱动的一种，除了 AUFS 之外，Docker 还支持了不同的存储驱动，包括 <code>aufs</code>、<code>devicemapper</code>、<code>overlay2</code>、<code>zfs</code> 和 <code>vfs</code> 等等，在最新的 Docker 中，<code>overlay2</code> 取代了 <code>aufs</code> 成为了推荐的存储驱动，但是在没有 <code>overlay2</code> 驱动的机器上仍然会使用 <code>aufs</code> 作为 Docker 的默认驱动。</p><p><img src="https://img.draveness.me/2017-11-30-docker-storage-driver.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="docker-storage-driver"></p><p>不同的存储驱动在存储镜像和容器文件时也有着完全不同的实现，有兴趣的读者可以在 Docker 的官方文档 <a href="https://docs.docker.com/engine/userguide/storagedriver/selectadriver/">Select a storage driver</a> 中找到相应的内容。</p><p>想要查看当前系统的 Docker 上使用了哪种存储驱动只需要使用以下的命令就能得到相对应的信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker info | grep Storage</span></span><br><span class="line">Storage Driver: aufs</span><br></pre></td></tr></table></figure><p>作者的这台 Ubuntu 上由于没有 <code>overlay2</code> 存储驱动，所以使用 <code>aufs</code> 作为 Docker 的默认存储驱动。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Docker 目前已经成为了非常主流的技术，已经在很多成熟公司的生产环境中使用，但是 Docker 的核心技术其实已经有很多年的历史了，Linux 命名空间、控制组和 UnionFS 三大技术支撑了目前 Docker 的实现，也是 Docker 能够出现的最重要原因。</p><p>作者在学习 Docker 实现原理的过程中查阅了非常多的资料，从中也学习到了很多与 Linux 操作系统相关的知识，不过由于 Docker 目前的代码库实在是太过庞大，想要从源代码的角度完全理解 Docker 实现的细节已经是非常困难的了，但是如果各位读者真的对其实现细节感兴趣，可以从 <a href="https://github.com/docker/docker-ce">Docker CE</a> 的源代码开始了解 Docker 的原理。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.safaribooksonline.com/library/view/using-docker/9781491915752/ch04.html">Chapter 4. Docker Fundamentals · Using Docker by Adrian Mount</a></li><li><a href="https://washraf.gitbooks.io/the-docker-ecosystem/content">TECHNIQUES BEHIND DOCKER</a></li><li><a href="https://docs.docker.com/engine/docker-overview/#the-underlying-technology">Docker overview</a></li><li><a href="https://lwn.net/Articles/312641/">Unifying filesystems with union mounts</a></li><li><a href="https://coolshell.cn/articles/17061.html">DOCKER 基础技术：AUFS</a></li><li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/">RESOURCE MANAGEMENT GUIDE</a></li><li><a href="http://www.linuxjournal.com/article/7714">Kernel Korner - Unionfs: Bringing Filesystems Together</a></li><li><a href="https://lwn.net/Articles/325369/">Union file systems: Implementations, part I</a></li><li><a href="https://blog.docker.com/2016/05/docker-unikernels-open-source/">IMPROVING DOCKER WITH UNIKERNELS: INTRODUCING HYPERKIT, VPNKIT AND DATAKIT</a></li><li><a href="https://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespaces">Separation Anxiety: A Tutorial for Isolating Your System with Linux Namespaces</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html">理解 chroot</a></li><li><a href="http://www.yolinux.com/TUTORIALS/LinuxTutorialInitProcess.html">Linux Init Process / PC Boot Procedure</a></li><li><a href="http://www.infoq.com/cn/articles/docker-network-and-pipework-open-source-explanation-practice#">Docker 网络详解及 pipework 源码解读与实践</a></li><li><a href="https://docs.docker.com/engine/userguide/networking/default_network/container-communication/#communication-between-containers">Understand container communication</a></li><li><a href="https://github.com/docker/labs/blob/master/networking/concepts/05-bridge-networks.md">Docker Bridge Network Driver Architecture</a></li><li><a href="http://www.thegeekstuff.com/2011/01/iptables-fundamentals/">Linux Firewall Tutorial: IPTables Tables, Chains, Rules Fundamentals</a></li><li><a href="http://www.iptables.info/en/structure-of-iptables.html">Traversing of tables and chains</a></li><li><a href="http://dockone.io/article/1255">Docker 网络部分执行流分析（Libnetwork 源码解读）</a></li><li><a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">Libnetwork Design</a></li><li><a href="http://www.infoq.com/cn/articles/analysis-of-docker-file-system-aufs-and-devicemapper">剖析 Docker 文件系统：Aufs与Devicemapper</a></li><li><a href="https://stackoverflow.com/questions/22889241/linux-understanding-the-mount-namespace-clone-clone-newns-flag">Linux - understanding the mount namespace &amp; clone CLONE_NEWNS flag</a></li><li><a href="http://www.infoq.com/cn/articles/docker-kernel-knowledge-namespace-resource-isolation">Docker 背后的内核知识 —— Namespace 资源隔离</a></li><li><a href="https://linuxcontainers.org/">Infrastructure for container projects</a></li><li><a href="https://github.com/opencontainers/runc/blob/master/libcontainer/SPEC.md">Spec · libcontainer</a></li><li><a href="https://coolshell.cn/articles/17010.html">DOCKER 基础技术：LINUX NAMESPACE（上）</a></li><li><a href="https://coolshell.cn/articles/17049.html">DOCKER 基础技术：LINUX CGROUP</a></li><li><a href="https://yq.aliyun.com/articles/65034">《自己动手写Docker》书摘之三： Linux UnionFS</a></li><li><a href="http://www.programering.com/a/MDMzAjMwATk.html">Introduction to Docker</a></li><li><a href="https://docs.docker.com/v1.9/engine/userguide/storagedriver/imagesandcontainers/">Understand images, containers, and storage drivers</a></li><li><a href="https://docs.docker.com/engine/userguide/storagedriver/aufs-driver/#configure-docker-with-the-aufs-storage-driver">Use the AUFS storage driver</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Do</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Docker" scheme="http://posts.hufeifei.cn/tags/Docker/"/>
    
    <category term="Namespace" scheme="http://posts.hufeifei.cn/tags/Namespace/"/>
    
    <category term="cGroups" scheme="http://posts.hufeifei.cn/tags/cGroups/"/>
    
    <category term="unionfs" scheme="http://posts.hufeifei.cn/tags/unionfs/"/>
    
    <category term="aufs" scheme="http://posts.hufeifei.cn/tags/aufs/"/>
    
  </entry>
  
  <entry>
    <title>谈谈 Kubernetes 的问题和局限性</title>
    <link href="http://posts.hufeifei.cn/backend/kuberentes-limitations/"/>
    <id>http://posts.hufeifei.cn/backend/kuberentes-limitations/</id>
    <published>2021-10-20T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>2014 年发布的 Kubernetes 在今天俨然已成为容器编排领域的事实标准，相信谈到 Kubernetes 的开发者都会一再复述上述现象。如下图所示，今天的大多数个人或者团队都会选择 Kubernetes 管理容器，而也有 75% 的人会在生产环境中使用 Kubernetes<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Kubernetes and Container Security and Adoption Trends](https://www.stackrox.com/kubernetes-adoption-security-and-market-share-for-containers/ )">[1]</span></a></sup>。</p><p><img src="https://img.draveness.me/kube-in-prod-2021-04-17-16186676828761.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="kube-in-prod"></p><p>在这种全民学习和使用 Kubernetes 的大背景下，我们也应该非常清晰地知道 Kubernetes 有哪些局限性。虽然 Kubernetes 能够解决容器编排领域的大多数问题，但是仍然有一些场景是它很难处理、甚至无法处理的，只有对这些潜在的风险有清晰的认识，才能更好地驾驭这项技术，这篇文章将从集群管理和应用场景两个部分谈谈 Kubernetes 社区目前的发展和一些局限性。</p><h1 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h1><p>集群是一组能够在一起协同工作的计算机，我们可以将集群中的所有计算机看成一个整体，所有资源调度系统都是以集群为维度进行管理的，集群中的所有机器构成了资源池，这个巨大的资源池会为待运行的容器提供资源执行计算任务，这里简单谈一谈 Kubernetes 集群管理面对的几个复杂问题。</p><h2 id="水平扩展性"><a href="#水平扩展性" class="headerlink" title="水平扩展性"></a>水平扩展性</h2><p>集群大小是我们在评估资源管理系统时需要关注的重要指标之一，然而 Kubernetes 能够管理的集群规模远远小于业界的其他资源管理系统。集群大小为什么重要呢，我们先来看另一个同样重要的指标 — 资源利用率，很多工程师可能没有在公有云平台上申请过资源，这些资源都相当昂贵，在 AWS 上申请一个与主机差不多配置的虚拟机实例（8 CPU、16 GB）每个月大概需要 150 美金，约为 1000 人民币<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[AWS Pricing Calculator](https://calculator.aws/#/createCalculator/EC2 )">[2]</span></a></sup>。</p><p><img src="https://img.draveness.me/aws-instance-pricing-2021-04-17-16186676828787.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="AWS EC2 价格"></p><p>大多数的集群都会使用 48 CPU 或者 64 CPU 的物理机或者虚拟机作为集群中的节点，如果我们的集群中需要包含 5,000 个节点，那么这些节点每个月大概要 8,000,000 美元，约为 50,000,000 人民币，在这样的集群中<strong>提升 1% 的资源利用率就相当于每个月节省了 500,000 的成本</strong>。</p><p>多数在线任务的资源利用率都很低，更大的集群意味着能够运行更多的工作负载，而多种高峰和低谷期不同的负载部署在一起可以实现超售，这样能够显著地提高集群的资源利用率，如果单个集群的节点数足够多，我们在部署不同类型的任务时会有更合理的组合，可以完美错开不同服务的高峰期。</p><p>Kubernetes 社区对外宣传的是单个集群最多支持 5,000 节点，Pod 总数不超过 150,000，容器总数不超过 300,000 以及单节点 Pod 数量不超过 100 个<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Considerations for large clusters](https://kubernetes.io/docs/setup/best-practices/cluster-large/ )">[3]</span></a></sup>，与几万节点的 Apache Mesos 集群、50,000 节点的微软 YARN 集群<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[How Microsoft drives exabyte analytics on the world’s largest YARN cluster](https://azure.microsoft.com/en-us/blog/how-microsoft-drives-exabyte-analytics-on-the-world-s-largest-yarn-cluster/ )">[4]</span></a></sup>相比，Kubernetes 的集群规模整整差了一个数量级。虽然阿里云的工程师也通过优化 Kubernetes 的各个组件实现了 5 位数的集群规模，但是与其他的资源管理方式相比却有比较大的差距<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[备战双 11！蚂蚁金服万级规模 K8s 集群管理系统如何设计？](https://www.sofastack.tech/blog/ant-financial-managing-large-scale-kubernetes-clusters/ )">[5]</span></a></sup>。</p><p><img src="https://img.draveness.me/Apache-Mesos-vs-Hadoop-YARN-2021-04-17-16186676828795.jpg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Apache Mesos 与 Hadoop YARN"></p><p>需要注意的是 Kubernetes 社区虽然对外宣称单集群可以支持 5,000 节点，同时社区也有各种各样的集成测试保证每个改动都不会影响它的伸缩性<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[sig-scalability-kubemark dashboard](https://testgrid.k8s.io/sig-scalability-kubemark#kubemark-5000 )">[6]</span></a></sup>，但是 Kubernetes 真的非常复杂，我们没有办法保证你使用的每个功能在扩容的过程中都不出问题。而在生产环境中，我们甚至可能在集群扩容到 1000 ~ 1500 节点时遇到瓶颈。</p><p>每个稍具规模的大公司都想要实现更大规模的 Kubernetes 集群，但是这不是一个改几行代码就能解决的简单问题，它可能需要我们限制 Kubernetes 中一些功能的使用，在扩容的过程中，etcd、API 服务器、调度器以及控制器都有可能出现问题。社区中已经有一些开发者注意到了其中的一些问题，例如在节点上增加缓存降低 API 服务器的负载<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Node-local API cache #84248](https://github.com/kubernetes/kubernetes/issues/84248 )">[7]</span></a></sup>，但是要推动类似的改变还是很困难的，有志之士可以尝试在社区推动类似的项目。</p><h2 id="多集群管理"><a href="#多集群管理" class="headerlink" title="多集群管理"></a>多集群管理</h2><p>单个集群的容量再大也无法解决企业面对的问题，哪怕有一天 Kubernetes 集群可以达到 50,000 节点的规模，我们仍然需要管理多个集群，多集群管理也是 Kubernetes 社区目前正在探索的方向，社区中的多集群兴趣小组（SIG Multi-Cluster）目前就在完成相关的工作<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Multicluster Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-multicluster )">[8]</span></a></sup>。在作者看来，Kubernetes 的多集群会带来资源不平衡、跨集群访问困难以及提高运维和管理成本三大问题，我们在这里谈一谈目前在开源社区和业界几种可供参考和选择的解决方案。</p><h3 id="kubefed"><a href="#kubefed" class="headerlink" title="kubefed"></a>kubefed</h3><p>首先要介绍的是 <a href="https://github.com/kubernetes-sigs/kubefed">kubefed</a>，该项目是 Kubernetes 社区给出的解决方案，它同时提供了跨集群的资源和网络管理的功能，社区的多集群兴趣小组（SIG Multi-Cluster）负责了该项目的开发工作：</p><p><img src="https://img.draveness.me/kubernetes-federation-2021-04-17-16186676828802.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Kubernetes 联邦"></p><p>kubefed 通过一个中心化的联邦控制面板管理多集群中的元数据，上层的控制面板会为管理器群中的资源创建对应的联邦对象，例如：<code>FederatedDeployment</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">FederatedDeployment</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">overrides:</span></span><br><span class="line">  <span class="comment"># Apply overrides to cluster1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">clusterName:</span> <span class="string">cluster1</span></span><br><span class="line">      <span class="attr">clusterOverrides:</span></span><br><span class="line">        <span class="comment"># Set the replicas field to 5</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;/spec/replicas&quot;</span></span><br><span class="line">          <span class="attr">value:</span> <span class="number">5</span></span><br><span class="line">        <span class="comment"># Set the image of the first container</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;/spec/template/spec/containers/0/image&quot;</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;nginx:1.17.0-alpine&quot;</span></span><br><span class="line">        <span class="comment"># Ensure the annotation &quot;foo: bar&quot; exists</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;/metadata/annotations&quot;</span></span><br><span class="line">          <span class="attr">op:</span> <span class="string">&quot;add&quot;</span></span><br><span class="line">          <span class="attr">value:</span></span><br><span class="line">            <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line">        <span class="comment"># Ensure an annotation with key &quot;foo&quot; does not exist</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;/metadata/annotations/foo&quot;</span></span><br><span class="line">          <span class="attr">op:</span> <span class="string">&quot;remove&quot;</span></span><br><span class="line">        <span class="comment"># Adds an argument `-q` at index 0 of the args list</span></span><br><span class="line">        <span class="comment"># this will obviously shift the existing arguments, if any</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;/spec/template/spec/containers/0/args/0&quot;</span></span><br><span class="line">          <span class="attr">op:</span> <span class="string">&quot;add&quot;</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;-q&quot;</span></span><br></pre></td></tr></table></figure><p>上层的控制面板会根据联邦对象 <code>FederatedDeployment</code> 的规格文件生成对应的 <code>Deployment</code> 并推送到下层的集群，下层集群可以正常根据 <code>Deployment</code> 中的定义创建特定数量的副本。</p><p><img src="https://img.draveness.me/kubernetes-federated-resources-2021-04-17-16186676828808.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="从联邦对象到普通对象"></p><p><code>FederatedDeployment</code> 只是一种最简单的分发策略，在生产环境中我们希望通过联邦的集群实现容灾等复杂功能，这时可以利用 <code>ReplicaSchedulingPreference</code> 在不同集群中实现更加智能的分发策略：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.kubefed.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSchedulingPreference</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-deployment</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">test-ns</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">targetKind:</span> <span class="string">FederatedDeployment</span></span><br><span class="line">  <span class="attr">totalReplicas:</span> <span class="number">9</span></span><br><span class="line">  <span class="attr">clusters:</span></span><br><span class="line">    <span class="attr">A:</span></span><br><span class="line">      <span class="attr">minReplicas:</span> <span class="number">4</span></span><br><span class="line">      <span class="attr">maxReplicas:</span> <span class="number">6</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">B:</span></span><br><span class="line">      <span class="attr">minReplicas:</span> <span class="number">4</span></span><br><span class="line">      <span class="attr">maxReplicas:</span> <span class="number">8</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><p>上述调度的策略可以实现工作负载在不同集群之间的权重，在集群资源不足甚至出现问题时将实例迁移到其他集群，这样既能够提高服务部署的灵活性和可用性，基础架构工程师也可以更好地平衡多个集群的负载。</p><p>我们可以认为 kubefed 的主要作用是将多个松散的集群组成强耦合的联邦集群，并提供更加高级的网络和部署功能，这样我们可以更容易地解决集群之间资源不平衡和连通性的一些问题，然而该项目的关注点不包含集群生命周期的管理，</p><h3 id="集群接口"><a href="#集群接口" class="headerlink" title="集群接口"></a>集群接口</h3><p><a href="https://cluster-api.sigs.k8s.io/">Cluster API</a> 也是 Kubernetes 社区中与多集群管理相关的项目，该项目由集群生命周期小组（SIG Cluster-Lifecycle）负责开发，其主要目标是通过声明式的 API 简化多集群的准备、更新和运维工作，你在该项目的 <a href="https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/scope-and-objectives.md">设计提案</a> 中能够找到它的职责范围<sup id="fnref:9"><a href="#fn:9" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Cluster API Scope and Objectives](https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/scope-and-objectives.md )">[9]</span></a></sup>。</p><p><img src="https://img.draveness.me/k8s-cluster-api-concepts-2021-04-17-16186676828819.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Cluster API 概念"></p><p>在该项目中最重要的资源就是 <code>Machine</code>，它表示一个 Kubernetes 集群中的节点。当该资源被创建时，特定提供商的控制器会根据机器的定义初始化并将新的节点注册到集群中，在该资源被更新或者删除时，也会执行操作达到用户的状态。</p><p>这种策略与阿里的多集群管理的方式有一些相似，它们都使用声明式的 API 定义机器和集群的状态，然后使用 Kubernetes 原生的 Operator 模型在更高一层的集群中管理下层集群，这能够极大降低集群的运维成本并提高集群的运行效率<sup id="fnref:10"><a href="#fn:10" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Demystifying Kubernetes as a service – How Alibaba cloud manages 10,000s of Kubernetes clusters](https://www.cncf.io/blog/2019/12/12/demystifying-kubernetes-as-a-service-how-does-alibaba-cloud-manage-10000s-of-kubernetes-clusters/ )">[10]</span></a></sup>，不过类似的项目都没有考虑跨集群的资源管理和网络管理。</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>我们在这一节将谈谈 Kubernetes 中一些有趣的应用场景，其中包括应用分发方式的现状、批处理调度任务以及硬多租户在集群中的支持，这些是社区中比较关注的问题，也是 Kubernetes 目前的盲点。</p><h2 id="应用分发"><a href="#应用分发" class="headerlink" title="应用分发"></a>应用分发</h2><p>Kubernetes 主项目提供了几种部署应用的最基本方式，分别是 <code>Deployment</code>、<code>StatefulSet</code> 和 <code>DaemonSet</code>，这些资源分别适用于无状态服务、有状态服务和节点上的守护进程，这些资源能够提供最基本的策略，但是它们无法处理更加复杂的应用。</p><p><img src="https://img.draveness.me/kubernetes-basic-resources-2021-04-17-16186676828825.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Kubernetes 应用管理"></p><p>随着 CRD 的引入，目前社区的应用管理小组（SIG Apps）基本不会向 Kubernetes 主仓库引入较大的改动，大多数的改动都是在现有资源上进行的修补，很多常见的场景，例如只运行一次的 DaemonSet<sup id="fnref:11"><a href="#fn:11" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Run job on each node once to help with setup #64623](https://github.com/kubernetes/kubernetes/issues/64623 )">[11]</span></a></sup> 以及金丝雀和蓝绿部署等功能，现在的资源也存在很多问题，例如 StatefulSet 在初始化容器中卡住无法回滚和更新<sup id="fnref:12"><a href="#fn:12" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[StatefulSet does not upgrade to a newer version of manifests #78007](https://github.com/kubernetes/kubernetes/issues/78007 )">[12]</span></a></sup>。</p><p>我们可以理解社区不想在 Kubernetes 中维护更多的基本资源，通过几个基本的资源可以覆盖 90% 的场景，剩下的各种复杂场景可以让其他社区通过 CRD 的方式实现。不过作者认为如果社区能够在上游实现更多高质量的组件，这对于整个生态都是很有价值并且很重要的工作，需要注意的是假如各位读者想要在 Kubernetes 项目中成为贡献者，SIG Apps 可能不是一个很好的选择。</p><h2 id="批处理调度"><a href="#批处理调度" class="headerlink" title="批处理调度"></a>批处理调度</h2><p>机器学习、批处理任务和流式任务等工作负载的运行从 Kubernetes 诞生第一天起到今天都不是它的强项，大多数的公司都会使用 Kubernetes 运行在线服务处理用户请求，用 Yarn 管理的集群运行批处理的负载。</p><p><img src="https://img.draveness.me/hadoop-yarn-2021-04-17-16186676828831.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="hadoop yarn"></p><p>在线任务和离线任务往往是两种截然不同的作业，大多数的在线任务都是无状态的服务，它们可以在不同机器上进行迁移，彼此很难有极强的依赖关系；但是很多离线任务的拓扑结构都很复杂，有些任务需要多个作业一同执行，而有些任务需要按照依赖关系先后执行，这种复杂的调度场景在 Kubernetes 中比较难以处理。</p><p>在 Kubernetes 调度器引入调度框架之前，所有的 Pod 在调度器看来是没有任何关联的，不过有了调度框架，我们可以在调度系统中实现更加复杂的调度策略，例如保证一组 Pod 同时调度的 PodGroup<sup id="fnref:13"><a href="#fn:13" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Coscheduling based on PodGroup CRD](https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/42-podgroup-coscheduling )">[13]</span></a></sup>，这对于 Spark 和 TensorFlow 任务非常有用。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PodGroup CRD spec</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.sigs.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodGroup</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">scheduleTimeoutSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">minMember:</span> <span class="number">3</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Add a label `pod-group.scheduling.sigs.k8s.io` to mark the pod belongs to a group</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">  <span class="attr">pod-group.scheduling.sigs.k8s.io:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p>Volcano 也是在 Kubernetes 上构建的批处理任务管理系统<sup id="fnref:14"><a href="#fn:14" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Volcano · A Kubernetes Native Batch System](https://github.com/volcano-sh/volcano)">[14]</span></a></sup>，它能够处理机器学习、深度学习以及其他大数据应用，可以支持包括 TensorFlow、Spark、PyTorch 和 MPI 在内的多个框架。</p><p><img src="https://img.draveness.me/volcano-logo-2021-04-17-16186676828837.png" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="Volcano"></p><p>虽然 Kubernetes 能够运行一些批处理任务，但是距离在这个领域上取代 Yarn 等老牌资源管理系统上还有非常大的差距，相信在较长的一段时间内，大多数公司都会同时维护 Kubernetes 和 Yarn 两种技术栈，分别管理和运行不同类型的工作负载。</p><h2 id="硬多租户"><a href="#硬多租户" class="headerlink" title="硬多租户"></a>硬多租户</h2><p>多租户是指同一个软件实例可以为不同的用户组提供服务，Kubernetes 的多租户是指多个用户或者用户组使用同一个 Kubernetes 集群，今天的 Kubernetes 还很难做到硬多租户支持，也就是同一个集群的多个租户不会相互影响，也感知不到彼此的存在。</p><p>硬多租户在 Kubernetes 中是一个很重要、也很困难的课题，合租公寓就是一个典型的多租户场景，多个租客共享房屋内的基础设施，硬多租户要求多个访客之间不会相互影响，你可以想象这有多么困难，Kubernetes 社区甚至有一个工作小组专门讨论和研究相关的问题<sup id="fnref:15"><a href="#fn:15" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="[Kubernetes Working Group for Multi-Tenancy](https://github.com/kubernetes-sigs/multi-tenancy)">[15]</span></a></sup>，然而虽然感兴趣的工程师很多，但是成果却非常有限。</p><p><img src="https://img.draveness.me/multi-tenant-2021-04-17-16186676828844.jpg" rel="external noreferrer nofollow noopener" referrerpolicy="no-referrer" alt="多租户"></p><p>尽管 Kubernetes 使用命名空间来划分虚拟机群，然而这也很难实现真正的多租户。多租户的支持到底有哪些作用呢，这里简单列几个多租户带来的好处：</p><ul><li>Kubernetes 带来的额外部署成本对于小集群来说非常高昂，稳定的 Kubernetes 集群一般都需要至少三个运行 etcd 的主节点，如果大多数的集群都是小集群，这些额外的机器会带来很高的额外开销；</li><li>Kubernetes 中运行的容器可能需要共享物理机和虚拟机，一些开发者可能在公司内部遇到过自己的服务被其他业务影响，因为主机上容器可能隔离了 CPU 和内存资源，但是没有隔离 I/O、网络 和 CPU 缓存等资源，这些资源的隔离是相对困难的；</li></ul><p>如果 Kubernetes 能够实现硬多租户，这不仅对云服务商和小集群的使用者来说都是个福音，它还能够隔离不同容器之间的影响并防止潜在安全问题的发生，不过这在现阶段还是比较难实现的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>每个技术都有自己的生命周期，越底层的技术生命周期会越长，而越上层的技术生命周期也就越短，虽然 Kubernetes 是当今容器界的扛把子，但是未来的事情没有人可以说的准。我们要时刻清楚手中工具的优点和缺点，花一些时间学习 Kubernetes 中设计的精髓，不过如果在未来的某一天 Kubernetes 也成为了过去，我们也应该感到喜悦，因为会有更好的工具取代它。</p><hr><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.stackrox.com/kubernetes-adoption-security-and-market-share-for-containers/">Kubernetes and Container Security and Adoption Trends</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://calculator.aws/#/createCalculator/EC2">AWS Pricing Calculator</a><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/">Considerations for large clusters</a><a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://azure.microsoft.com/en-us/blog/how-microsoft-drives-exabyte-analytics-on-the-world-s-largest-yarn-cluster/">How Microsoft drives exabyte analytics on the world’s largest YARN cluster</a><a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.sofastack.tech/blog/ant-financial-managing-large-scale-kubernetes-clusters/">备战双 11！蚂蚁金服万级规模 K8s 集群管理系统如何设计？</a><a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://testgrid.k8s.io/sig-scalability-kubemark#kubemark-5000">sig-scalability-kubemark dashboard</a><a href="#fnref:6" rev="footnote"> ↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes/kubernetes/issues/84248">Node-local API cache #84248</a><a href="#fnref:7" rev="footnote"> ↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes/community/tree/master/sig-multicluster">Multicluster Special Interest Group</a><a href="#fnref:8" rev="footnote"> ↩</a></span></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes-sigs/cluster-api/blob/master/docs/scope-and-objectives.md">Cluster API Scope and Objectives</a><a href="#fnref:9" rev="footnote"> ↩</a></span></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.cncf.io/blog/2019/12/12/demystifying-kubernetes-as-a-service-how-does-alibaba-cloud-manage-10000s-of-kubernetes-clusters/">Demystifying Kubernetes as a service – How Alibaba cloud manages 10,000s of Kubernetes clusters</a><a href="#fnref:10" rev="footnote"> ↩</a></span></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes/kubernetes/issues/64623">Run job on each node once to help with setup #64623</a><a href="#fnref:11" rev="footnote"> ↩</a></span></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">12.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes/kubernetes/issues/78007">StatefulSet does not upgrade to a newer version of manifests #78007</a><a href="#fnref:12" rev="footnote"> ↩</a></span></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">13.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/42-podgroup-coscheduling">Coscheduling based on PodGroup CRD</a><a href="#fnref:13" rev="footnote"> ↩</a></span></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">14.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/volcano-sh/volcano">Volcano · A Kubernetes Native Batch System</a><a href="#fnref:14" rev="footnote"> ↩</a></span></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">15.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Working Group for Multi-Tenancy</a><a href="#fnref:15" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;2014 年发布的 Kubernetes 在今天俨然已成为容器编排领域的事实</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Kubernetes" scheme="http://posts.hufeifei.cn/tags/Kubernetes/"/>
    
    <category term="云原生" scheme="http://posts.hufeifei.cn/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
    <category term="集群管理" scheme="http://posts.hufeifei.cn/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    <category term="容器编排" scheme="http://posts.hufeifei.cn/tags/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"/>
    
    <category term="数据中心" scheme="http://posts.hufeifei.cn/tags/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>Dockerfile最佳实践</title>
    <link href="http://posts.hufeifei.cn/backend/dockerfile-best-practice/"/>
    <id>http://posts.hufeifei.cn/backend/dockerfile-best-practice/</id>
    <published>2021-10-14T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>译者按:</strong> Dockerfile的语法非常简单，然而如何加快镜像构建速度，如何减少Docker镜像的大小却不是那么直观，需要积累实践经验。这篇博客可以帮助你快速掌握编写Dockerfile的技巧。</p><ul><li>原文: <a href="https://rock-it.pl/how-to-write-excellent-dockerfiles/">How to write excellent Dockerfiles</a></li><li>译者: <a href="https://www.fundebug.com/">Fundebug</a></li></ul><p><strong>为了保证可读性，本文采用意译而非直译。另外，本文版权归原作者所有，翻译仅用于学习</strong>。</p><p>我已经使用Docker有一段时间了，其中编写Dockerfile是非常重要的一部分工作。在这篇博客中，我打算分享一些建议，帮助大家编写更好的Dockerfile。</p><p><strong>目标:</strong></p><ul><li>更快的构建速度</li><li>更小的Docker镜像大小</li><li>更少的Docker镜像层</li><li>充分利用镜像缓存</li><li>增加Dockerfile可读性</li><li>让Docker容器使用起来更简单</li></ul><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>示例 Dockerfile 犯了几乎所有的错(当然我是故意的)。接下来，我会一步步优化它。假设我们需要使用 Docker 运行一个 Node.js 应用，下面就是它的 Dockerfile(CMD 指令太复杂了，所以我简化了，它是错误的，仅供参考)。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get upgrade -y</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get install -y nodejs ssh mysql</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># this should start three processes, mysql and ssh</span></span><br><span class="line"><span class="comment"># in the background and node app in foreground</span></span><br><span class="line"><span class="comment"># isn&#x27;t it beautifully terrible? &lt;3</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> mysql &amp; sshd &amp; npm start</span></span><br></pre></td></tr></table></figure><p>构建镜像:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t wtf .</span><br></pre></td></tr></table></figure><h1 id="1-编写-dockerignore-文件"><a href="#1-编写-dockerignore-文件" class="headerlink" title="1. 编写.dockerignore 文件"></a>1. 编写.dockerignore 文件</h1><p>构建镜像时，Docker 需要先准备<code>context</code> ，将所有需要的文件收集到进程中。默认的<code>context</code>包含 Dockerfile 目录中的所有文件，但是实际上，<strong>我们并不需要.git 目录，node_modules 目录等内容</strong>。 <code>.dockerignore</code> 的作用和语法类似于 <code>.gitignore</code>，可以忽略一些不需要的文件，这样可以有效加快镜像构建时间，同时减少 Docker 镜像的大小。示例如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.git/</span><br><span class="line">node_modules/</span><br></pre></td></tr></table></figure><h1 id="2-容器只运行单个应用"><a href="#2-容器只运行单个应用" class="headerlink" title="2. 容器只运行单个应用"></a>2. 容器只运行单个应用</h1><p>从技术角度讲，你可以在 Docker 容器中运行多个进程。你可以将数据库，前端，后端，ssh，supervisor 都运行在同一个 Docker 容器中。但是，这会让你非常痛苦:</p><ul><li>非常长的构建时间(修改前端之后，整个后端也需要重新构建)</li><li>非常大的镜像大小</li><li>多个应用的日志难以处理(不能直接使用 stdout，否则多个应用的日志会混合到一起)</li><li>横向扩展时非常浪费资源(不同的应用需要运行的容器数并不相同)</li><li>僵尸进程问题 - 你需要选择合适的 init 进程</li></ul><p>因此，我建议大家为每个应用构建单独的 Docker 镜像，然后使用 <a href="https://docs.docker.com/compose/">Docker Compose</a> 运行多个 Docker 容器。</p><p>现在，我从 Dockerfile 中删除一些不需要的安装包，另外，SSH 可以用<a href="https://docs.docker.com/engine/reference/commandline/exec/">docker exec</a>替代。示例如下：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get upgrade -y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># we should remove ssh and mysql, and use</span></span><br><span class="line"><span class="comment"># separate container for database</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get install -y nodejs  <span class="comment"># ssh mysql</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><h1 id="3-将多个-RUN-指令合并为一个"><a href="#3-将多个-RUN-指令合并为一个" class="headerlink" title="3. 将多个 RUN 指令合并为一个"></a>3. 将多个 RUN 指令合并为一个</h1><p>Docker 镜像是分层的，下面这些知识点非常重要:</p><ul><li>Dockerfile 中的每个指令都会创建一个新的镜像层。</li><li>镜像层将被缓存和复用</li><li>当 Dockerfile 的指令修改了，复制的文件变化了，或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效</li><li>某一层的镜像缓存失效之后，它之后的镜像层缓存都会失效</li><li>镜像层是不可变的，如果我们再某一层中添加一个文件，然后在下一层中删除它，则镜像中依然会包含该文件(只是这个文件在 Docker 容器中不可见了)。</li></ul><p>Docker 镜像类似于洋葱。它们都有很多层。为了修改内层，则需要将外面的层都删掉。记住这一点的话，其他内容就很好理解了。</p><p>现在，我们<strong>将所有的<a href="https://docs.docker.com/engine/reference/builder/#run">RUN</a>指令合并为一个</strong>。同时把<code>apt-get upgrade</code>删除，因为它会使得镜像构建非常不确定(我们只需要依赖基础镜像的更新就好了)</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apt-get install -y nodejs \</span></span><br><span class="line"><span class="bash">    &amp;&amp; <span class="built_in">cd</span> /app \</span></span><br><span class="line"><span class="bash">    &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><p>记住一点，我们只能将变化频率一样的指令合并在一起。将 node.js 安装与 npm 模块安装放在一起的话，则每次修改源代码，都需要重新安装 node.js，这显然不合适。因此，正确的写法是这样的:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update &amp;&amp; apt-get install -y nodejs</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><h1 id="4-基础镜像的标签不要用-latest"><a href="#4-基础镜像的标签不要用-latest" class="headerlink" title="4. 基础镜像的标签不要用 latest"></a>4. 基础镜像的标签不要用 latest</h1><p>当镜像没有指定标签时，将默认使用<code>latest</code> 标签。因此， <code>FROM ubuntu</code> 指令等同于<code>FROM ubuntu:latest</code>。当时，当镜像更新时，latest 标签会指向不同的镜像，这时构建镜像有可能失败。如果你的确需要使用最新版的基础镜像，可以使用 latest 标签，否则的话，最好指定确定的镜像标签。</p><p>示例 Dockerfile 应该使用<code>16.04</code>作为标签。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">16.04</span>  <span class="comment"># it&#x27;s that easy!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update &amp;&amp; apt-get install -y nodejs</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><h1 id="5-每个-RUN-指令后删除多余文件"><a href="#5-每个-RUN-指令后删除多余文件" class="headerlink" title="5. 每个 RUN 指令后删除多余文件"></a>5. 每个 RUN 指令后删除多余文件</h1><p>假设我们更新了 apt-get 源，下载，解压并安装了一些软件包，它们都保存在<code>/var/lib/apt/lists/</code>目录中。但是，运行应用时 Docker 镜像中并不需要这些文件。我们最好将它们删除，因为它会使 Docker 镜像变大。</p><p>示例 Dockerfile 中，我们可以删除<code>/var/lib/apt/lists/</code>目录中的文件(它们是由 apt-get update 生成的)。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">16.04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apt-get install -y nodejs \</span></span><br><span class="line"><span class="bash">    <span class="comment"># added lines</span></span></span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><h1 id="6-选择合适的基础镜像-alpine-版本最好"><a href="#6-选择合适的基础镜像-alpine-版本最好" class="headerlink" title="6. 选择合适的基础镜像(alpine 版本最好)"></a>6. 选择合适的基础镜像(alpine 版本最好)</h1><p>在示例中，我们选择了<code>ubuntu</code>作为基础镜像。但是我们只需要运行 node 程序，有必要使用一个通用的基础镜像吗？<code>node</code>镜像应该是更好的选择。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="comment"># we don&#x27;t need to install node</span></span><br><span class="line"><span class="comment"># anymore and use apt-get</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><p>更好的选择是 alpine 版本的<code>node</code>镜像。alpine 是一个极小化的 Linux 发行版，只有 4MB，这让它非常适合作为基础镜像。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">cd</span> /app &amp;&amp; npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> npm start</span></span><br></pre></td></tr></table></figure><p><a href="https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management">apk</a>是 Alpine 的包管理工具。它与<code>apt-get</code>有些不同，但是非常容易上手。另外，它还有一些非常有用的特性，比如<code>no-cache</code>和 <code>--virtual</code>选项，它们都可以帮助我们减少镜像的大小。</p><h1 id="7-设置-WORKDIR-和-CMD"><a href="#7-设置-WORKDIR-和-CMD" class="headerlink" title="7. 设置 WORKDIR 和 CMD"></a>7. 设置 WORKDIR 和 CMD</h1><p><a href="https://docs.docker.com/engine/reference/builder/#workdir">WORKDIR</a>指令可以设置默认目录，也就是运行<code>RUN</code> / <code>CMD</code> / <code>ENTRYPOINT</code>指令的地方。</p><p><a href="https://docs.docker.com/engine/reference/builder/#cmd">CMD</a>指令可以设置容器创建是执行的默认命令。另外，你应该讲命令写在一个数组中，数组中每个元素为命令的每个单词(参考<a href="https://docs.docker.com/engine/reference/builder/#cmd">官方文档</a>)。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;npm&quot;</span>, <span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><h1 id="8-使用-ENTRYPOINT-可选"><a href="#8-使用-ENTRYPOINT-可选" class="headerlink" title="8. 使用 ENTRYPOINT (可选)"></a>8. 使用 ENTRYPOINT (可选)</h1><p><a href="https://docs.docker.com/engine/reference/builder/#entrypoint">ENTRYPOINT</a>指令并不是必须的，因为它会增加复杂度。<code>ENTRYPOINT</code>是一个脚本，它会默认执行，并且将指定的命令错误其参数。它通常用于构建可执行的 Docker 镜像。entrypoint.sh 如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env sh</span></span><br><span class="line"><span class="comment"># $0 is a script name,</span></span><br><span class="line"><span class="comment"># $1, $2, $3 etc are passed arguments</span></span><br><span class="line"><span class="comment"># $1 is our command</span></span><br><span class="line">CMD=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">&quot;<span class="variable">$CMD</span>&quot;</span> <span class="keyword">in</span></span><br><span class="line">  <span class="string">&quot;dev&quot;</span> )</span><br><span class="line">    npm install</span><br><span class="line">    <span class="built_in">export</span> NODE_ENV=development</span><br><span class="line">    <span class="built_in">exec</span> npm run dev</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;start&quot;</span> )</span><br><span class="line">    <span class="comment"># we can modify files here, using ENV variables passed in</span></span><br><span class="line">    <span class="comment"># &quot;docker create&quot; command. It can&#x27;t be done during build process.</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;db: <span class="variable">$DATABASE_ADDRESS</span>&quot;</span> &gt;&gt; /app/config.yml</span><br><span class="line">    <span class="built_in">export</span> NODE_ENV=production</span><br><span class="line">    <span class="built_in">exec</span> npm start</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">   * )</span><br><span class="line">    <span class="comment"># Run custom command. Thanks to this line we can still use</span></span><br><span class="line">    <span class="comment"># &quot;docker run our_image /bin/bash&quot; and it will work</span></span><br><span class="line">    <span class="built_in">exec</span> <span class="variable">$CMD</span> <span class="variable">$&#123;@:2&#125;</span></span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>示例 Dockerfile:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>可以使用如下命令运行该镜像:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行开发版本</span></span><br><span class="line">docker run our-app dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行生产版本</span></span><br><span class="line">docker run our-app start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行bash</span></span><br><span class="line">docker run -it our-app /bin/bash</span><br></pre></td></tr></table></figure><h1 id="9-在-entrypoint-脚本中使用-exec"><a href="#9-在-entrypoint-脚本中使用-exec" class="headerlink" title="9. 在 entrypoint 脚本中使用 exec"></a>9. 在 entrypoint 脚本中使用 exec</h1><p>在前文的 entrypoint 脚本中，我使用了<code>exec</code>命令运行 node 应用。不使用<code>exec</code>的话，我们则不能顺利地关闭容器，因为 SIGTERM 信号会被 bash 脚本进程吞没。<code>exec</code>命令启动的进程可以取代脚本进程，因此所有的信号都会正常工作。</p><h1 id="10-COPY-与-ADD-优先使用前者"><a href="#10-COPY-与-ADD-优先使用前者" class="headerlink" title="10. COPY 与 ADD 优先使用前者"></a>10. COPY 与 ADD 优先使用前者</h1><p><a href="https://docs.docker.com/engine/reference/builder/#copy">COPY</a>指令非常简单，仅用于将文件拷贝到镜像中。<a href="https://docs.docker.com/engine/reference/builder/#add">ADD</a>相对来讲复杂一些，可以用于下载远程文件以及解压压缩包(参考<a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#add-or-copy">官方文档</a>)。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><h1 id="11-合理调整-COPY-与-RUN-的顺序"><a href="#11-合理调整-COPY-与-RUN-的顺序" class="headerlink" title="11. 合理调整 COPY 与 RUN 的顺序"></a>11. 合理调整 COPY 与 RUN 的顺序</h1><p>我们应该<strong>把变化最少的部分放在 Dockerfile 的前面</strong>，这样可以充分利用镜像缓存。</p><p>示例中，源代码会经常变化，则每次构建镜像时都需要重新安装 NPM 模块，这显然不是我们希望看到的。因此我们可以先拷贝<code>package.json</code>，然后安装 NPM 模块，最后才拷贝其余的源代码。这样的话，即使源代码变化，也不需要重新安装 NPM 模块。</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json /app</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /app</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><h1 id="12-设置默认的环境变量，映射端口和数据卷"><a href="#12-设置默认的环境变量，映射端口和数据卷" class="headerlink" title="12. 设置默认的环境变量，映射端口和数据卷"></a>12. 设置默认的环境变量，映射端口和数据卷</h1><p>运行 Docker 容器时很可能需要一些环境变量。在 Dockerfile 设置默认的环境变量是一种很好的方式。另外，我们应该在 Dockerfile 中设置映射端口和数据卷。示例如下:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> PROJECT_DIR=/app</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> MEDIA_DIR=/media \</span><br><span class="line">    NODE_ENV=production \</span><br><span class="line">    APP_PORT=<span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> <span class="variable">$MEDIA_DIR</span></span></span><br><span class="line"><span class="keyword">EXPOSE</span> $APP_PORT</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><p><a href="https://docs.docker.com/engine/reference/builder/#env">ENV</a>指令指定的环境变量在容器中可以使用。如果你只是需要指定构建镜像时的变量，你可以使用<a href="https://docs.docker.com/engine/reference/builder/#arg">ARG</a>指令。</p><h1 id="13-使用-LABEL-设置镜像元数据"><a href="#13-使用-LABEL-设置镜像元数据" class="headerlink" title="13. 使用 LABEL 设置镜像元数据"></a>13. 使用 LABEL 设置镜像元数据</h1><p>使用<a href="https://docs.docker.com/engine/reference/builder/#label">LABEL</a>指令，可以为镜像设置元数据，例如<strong>镜像创建者</strong>或者<strong>镜像说明</strong>。旧版的 Dockerfile 语法使用<a href="https://docs.docker.com/engine/reference/builder/#maintainer-deprecated">MAINTAINER</a>指令指定镜像创建者，但是它已经被弃用了。有时，一些外部程序需要用到镜像的元数据，例如<a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>需要用到<code>com.nvidia.volumes.needed</code>。示例如下:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer <span class="string">&quot;jakub.skalecki@example.com&quot;</span></span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="14-添加-HEALTHCHECK"><a href="#14-添加-HEALTHCHECK" class="headerlink" title="14. 添加 HEALTHCHECK"></a>14. 添加 HEALTHCHECK</h1><p>运行容器时，可以指定<code>--restart always</code>选项。这样的话，容器崩溃时，Docker 守护进程(docker daemon)会重启容器。对于需要长时间运行的容器，这个选项非常有用。但是，如果容器的确在运行，但是不可(陷入死循环，配置错误)用怎么办？使用<a href="https://docs.docker.com/engine/reference/builder/#healthcheck">HEALTHCHECK</a>指令可以让 Docker 周期性的检查容器的健康状况。我们只需要指定一个命令，如果一切正常的话返回 0，否则返回 1。对 HEALTHCHECK 感兴趣的话，可以参考<a href="https://blog.newrelic.com/2016/08/24/docker-health-check-instruction/">这篇博客</a>。示例如下:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:<span class="number">7</span>-alpine</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer <span class="string">&quot;jakub.skalecki@example.com&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> PROJECT_DIR=/app</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package.json <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . <span class="variable">$PROJECT_DIR</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> MEDIA_DIR=/media \</span><br><span class="line">    NODE_ENV=production \</span><br><span class="line">    APP_PORT=<span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> <span class="variable">$MEDIA_DIR</span></span></span><br><span class="line"><span class="keyword">EXPOSE</span> $APP_PORT</span><br><span class="line"><span class="keyword">HEALTHCHECK</span><span class="bash"> CMD curl --fail http://localhost:<span class="variable">$APP_PORT</span> || <span class="built_in">exit</span> 1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>当请求失败时，<code>curl --fail</code> 命令返回非 0 状态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;&lt;strong&gt;译者按:&lt;/strong&gt; Dockerfile的语法非常简</summary>
      
    
    
    
    <category term="后端" scheme="http://posts.hufeifei.cn/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
    <category term="Docker" scheme="http://posts.hufeifei.cn/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>深度干货｜云原生分布式数据库 PolarDB-X 的技术演进</title>
    <link href="http://posts.hufeifei.cn/db/introduction-PolarDB-X/"/>
    <id>http://posts.hufeifei.cn/db/introduction-PolarDB-X/</id>
    <published>2021-10-12T00:00:00.000Z</published>
    <updated>2024-08-14T08:34:47.985Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="一、PolarDB-X是什么"><a href="#一、PolarDB-X是什么" class="headerlink" title="一、PolarDB-X是什么"></a>一、PolarDB-X是什么</h1><p>PolarDB-X最早起源于阿里集团2009年提出用分布式架构替代传统商业数据库，阿里研发了TDDL分库分表中间件。2014年阿里集团开始全面上云，将TDDL升级成DRDS分布式数据库服务，实现了在线扩缩容以及数据拆分等能力。2018年后，国内分布式数据库技术进入一个百家争鸣的局面，阿里在这方面也做了很多探索，经过对X-DB、PolarDB等技术整合，诞生了PolarDB-X。</p><p><img src="https://pic4.zhimg.com/80/v2-9dd6ffbc163fb08f2ccb4b9666fa8b07_720w.jpg" alt="img"></p><p>PolarDB-X结合了Sharding On MySQL、NewSQL、Cloud Native DB几种数据库理念的精华，具有云原生分布式的特性，底层使用了PolarDB云原生数据库的技术，上层用到了很多分布式技术。</p><h1 id="二、PolarDB-X-技术架构"><a href="#二、PolarDB-X-技术架构" class="headerlink" title="二、PolarDB-X 技术架构"></a>二、PolarDB-X 技术架构</h1><p>PolarDB-X采用经典的两层架构，分计算层和存储层。计算层用的PolarDB-X，可以独立水平扩展、扩缩容，各种能力完备。在整个系统里，一条SQL经过自研的解析器、优化器，得到分布式的执行计划；然后发送到存储节点执行；在中间的网络传输层，使用了定制的RPC协议，效率远高于传统的JDBC协议；之后执行计划会发送到PolarDB-X的执行引擎里去做具体的计算。</p><p><img src="https://pic4.zhimg.com/80/v2-b861edfe489782894b26d6b2408644b7_720w.jpg" alt="img"></p><p>PolarDB-X目前具有高可用、高可扩展、极致弹性等几个特性，高兼容、HTAP、开放生态，在MySQL生态里是一款具有竞争力的产品。</p><h1 id="三、PolarDB-X的几个关键技术"><a href="#三、PolarDB-X的几个关键技术" class="headerlink" title="三、PolarDB-X的几个关键技术"></a>三、PolarDB-X的几个关键技术</h1><p><strong>（一）分布式事务，如何实现ACID？</strong></p><p><img src="https://pic3.zhimg.com/80/v2-c1ce78dd9ba44f6e0139e7d77db6bc4a_720w.jpg" alt="img"></p><p>如果分布式数据库要支持金融转账场景，就必须支持分布式事务，才能保证一致性，不会发生数据丢失等异常。纵观业界技术，可以归成以下几类，第一类是基于MySQL的XA技术，实现两阶段提交；缺点是不能保证全局一致，不能保证全局快照。第二类是TSO技术做全局分配，实现给全局的事务定序，从而实现分布式快照。第三是HLC技术，也存在一定的局限性。第四类是在PG里比较多使用的GTM技术。这几项技术目前没有一个能完美解决所有场景，都需要在性能、可用性、扩展性方面去做权衡。PolarDB-X认为TSO是比较契合公有云以及混合云的技术。</p><p>PolarDB-X基于TSO技术实现全局分布式事务。第一个问题是如何去做全局时钟，也就是TSO。TSO会给分布式事务做定序，按照时间戳的顺序去做排序。第二个问题是如何基于MySQL的InnoDB做分布式事务。PolarDB-X对InnoDB的事务系统做了深度改造，从原本的ReadView的事务机制改造成基于时间戳的事务系统。有了基于时间戳的事务系统之后，结合TSO技术，就可以实现全局一致的分布式事务。除此之外，事务里还有很多的技术难点，如何处理长写事务以及做全局的垃圾回收。</p><p>用TSO技术有一个必须要解决的问题——通常会增加几十微秒到几百微秒的RT。因此，PolarDB-X实现了一阶段提交、2PC的异步提交等优化，能够尽量克服TSO带来的性能损失。</p><p>实现上述性能优化之后，经过与业界产品在sysbench和TPCC等测试集做了性能对比，PolarDB-X的性能相对来说非常有竞争力。</p><p><strong>（二）透明分布式，如何优化易用性？</strong></p><p>透明分布式主要解决的问题是分布式数据库的使用门槛。很多分布式数据库技术听起来很好，但用户却认为很难用。比如用户常常困扰，为什么某些场景的性能会不如一个单机系统，或者某些功能不具备，或者问题难以排查？从我们对服务用户的经验来看，用户在使用分布式数据库过程中通常会遇到以下几个门槛，即如何选择拆分键、如何优化分布式事务、如何优化慢查询。因此，我们研发了透明分布式的项目，试图降低用户使用分布式数据库的门槛。</p><p>第一，如何做Sharding。每个产品都有不同的解决方案，PolarDB-X结合了MySQL分区表语法，从语法上完全兼容MySQL列表，使用二级分区覆盖到用户的各种Workload。这背后是基于一致性哈希算法，实现分区级的动态分裂，大大降低扩缩容的代价。以Range分区为例，一开始可能是4千到5千这个数据范围，当这个Range的数据变多之后，它可以分裂成多个Range，迁移到多个机器上，避免数据过于集中。将这些技术融入PolarDB-X中，能够有效解决热点数据等问题。</p><p>第二，PolarDB-X做的跟其他产品有差异化的技术，是TableGroup。它解决的问题是Join下推，这是阿里的业务场景中非常常见。如果不能做Join的下推，做分布式Join的性能会比较差。在PolarDB-X中，多个表按一个分区方式做Partition，它们就会放置于同一个TableGroup，因此就可以实现Join下推。当然对应的，一个TableGroup中的分区分裂、迁移，都需要以PartitionGroup为单位了。</p><p><img src="https://pic2.zhimg.com/80/v2-b01acb14972a42d59d6921233dbdeb2d_720w.jpg" alt="img"></p><p>第三，扩缩容离不开的一个问题，就是Online DDL。例如PolarDB-X支持单表、拆分表、分区表，当用户对表类型进行修改，把分区键从买家ID改成卖家ID的时候，背后就是用Online DDL的技术。PolarDB-X支持多种的Online DDL，包括拆分键修改、创建索引、加减列等等，这些操作都可以在线上直接执行，对用户业务影响非常小。</p><p>PolarDB-X的透明分布式提供了分区表、全局索引、Online DDL等技术，使得用户的业务能够以很低的成本接入到分布式数据库中，并且后续随着业务的发展，数据库还可以做通过Scale-Up或者Scale-Out的方式提高性能。</p><p><strong>（三）HTAP技术，如何提高分析能力</strong></p><p>所谓HTAP，在PolarDB-X的理解中，即能否在线上数据库中执行复杂查询。它的价值有两方面，一方面是能够降低用户的使用成本、运维成本，另一方面，就是实时的分析，能够从实时数据获得实时洞察。做HTAP面对的技术挑战有几方面，分别是负载隔离、计算能力、存储能力。</p><p>对应到PolarDB-X的架构，会通过只读节点做负载隔离，简单查询发到读写节点，复杂查询发到只读节点执行，因此这两种负载能够得到较好的隔离，不会相互影响。这中间的智能路由是通过优化器的代价估算去实现，代价高的判定为AP查询，代价低的判定TP查询。除此之外，这种架构还需要解决的一个问题是一致性快照，PolarDB-X通过TSO技术，实现了只读节点的分布式事务。</p><p>接下来的问题是如何提升计算能力和存储能力。</p><p>提高计算能力主要通过MPP并行计算、向量化计算等方式。此前PolarDB-X主要面向TP场景，做算子下推，以及通过分区裁剪尽量查询更少的分片，优化TP场景的性能。而面对AP场景，需要的技术则很不一样。具体来说，PolarDB-X提供了原生的MPP支持，能够充分发挥多个节点的资源进行计算。为此，优化器里中增加了MPP优化阶段，在单机执行计划之后，中间加入Exchange，变成分布式的执行计划，实现多机并行。具体到执行器，也会有两种执行模式，一种是本地单机执行，另一种是MPP分布式执行。</p><p><img src="https://pic2.zhimg.com/80/v2-48dd52b6138c5230f86d57d2ac80d055_720w.jpg" alt="img"></p><p>具体来看，在MPP并行计算中，PolarDB-X做了两层的并行，第一层是节点之间的并行，第二层是计算节点内部的运行。分为两层的好处在于能够减少调度开销，减少数据传输的开销。除此之外，PolarDB-X还做了内存池化、流水线化、向量化等精细化的技术，通过向量化提高执行器的执行效率，通过流水线化增加并行度减少数据物化。这些技术使得PolarDB-X在执行复杂SQL查询时具有较高的效率。</p><p><img src="https://pic2.zhimg.com/80/v2-2cadc7962f21628776640e950892b46d_720w.jpg" alt="img"></p><p>除此之外，就是提高存储方面的性能。从技术角度看，单独做一个行存、列存都不难，难的是做一个能够实时更新的列存。PolarDB-X采用的方案是在写入节点用行存，在只读节点用列存，中间通过redo做异步复制，实现列存的实时更新。基于这样的架构，就可以实现行列混存，行存承担高并发写入，列存承担复杂查询。结合MPP、行列混存、向量化等技术，PolarDB-X实现了TPC-H场景的5-10倍的性能提升。这一成果也即将在公有云上线，敬请期待。</p><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p>PolarDB-X能够高度兼容单机MySQL，从SQL兼容到事务兼容到生态兼容。在此基础上，通过透明分布式的技术降低用户使用门槛，使得用户可以快速上手，适配各种用户业务，并通过弹性扩缩容的能力，适应用户的业务变化。而HTAP技术，将形成差异化的竞争力，使得用户能够从在线数据中获得实时洞察。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;h1 id=&quot;一、PolarDB-X是什么&quot;&gt;&lt;a href=&quot;#一、Polar</summary>
      
    
    
    
    <category term="数据库" scheme="http://posts.hufeifei.cn/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
    <category term="MySQL" scheme="http://posts.hufeifei.cn/tags/MySQL/"/>
    
    <category term="PolarDB" scheme="http://posts.hufeifei.cn/tags/PolarDB/"/>
    
    <category term="Sharding" scheme="http://posts.hufeifei.cn/tags/Sharding/"/>
    
    <category term="NewSQL" scheme="http://posts.hufeifei.cn/tags/NewSQL/"/>
    
    <category term="Cloud Native" scheme="http://posts.hufeifei.cn/tags/Cloud-Native/"/>
    
    <category term="Distributed" scheme="http://posts.hufeifei.cn/tags/Distributed/"/>
    
  </entry>
  
</feed>
